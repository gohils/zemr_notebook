{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970e2aa6-aa38-43b5-9a6e-f2b275a34896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res3: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res3: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs rm -r /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f22e137-35a3-40fb-b6cc-0f9132116cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:729)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:447)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:454)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:556)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:53)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:53)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:53)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:53)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:538)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:801)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:817)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:881)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:674)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:729)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:447)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:454)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:556)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:53)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:53)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:53)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:53)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:538)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:801)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:817)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:881)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:674)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this lakehouse script covers many essential features of a lakehouse data reporting solution, including:\n",
    "# 1.\tSchema Management: It starts by dropping any existing schemas and creating new ones, ensuring that the environment is clean and ready for data ingestion.\n",
    "# 2.\tData Ingestion: The script handles data ingestion for both initial (Day 1) and subsequent (Day 2) data loads. It creates DataFrames and writes them to Delta Lake in the bronze layer.\n",
    "# 3.\tTransformation and Deduplication: It includes transformations and deduplication steps in the silver layer. For instance, it joins dimension tables with fact tables, removes duplicates, and handles customer segmentation.\n",
    "# 4.\tIncremental Updates: It demonstrates incremental updates by appending new data (Day 2) to existing tables in the bronze, silver, and gold layers. This shows how the script can handle ongoing data ingestion and processing.\n",
    "# 5.\tLayered Approach: The script follows the common lakehouse architecture with three layers:\n",
    "# o\tBronze Layer: Raw data ingestion.\n",
    "# o\tSilver Layer: Data transformation and cleaning.\n",
    "# o\tGold Layer: Aggregated and consolidated data.\n",
    "# 6.\tPartitioning: It uses partitioning in the bronze layer to optimize performance, such as partitioning by state and order_date.\n",
    "# 7.\tData Storage: It writes the transformed data to Delta tables, ensuring efficient data storage and retrieval.\n",
    "\n",
    "# # Ensure proper schema registration for all layers\n",
    "spark.sql(\"DROP SCHEMA  IF EXISTS bronze CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS silver CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS gold CASCADE\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, date_add, lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LakehousePipeline\").getOrCreate()\n",
    "\n",
    "# Define the ingestion date\n",
    "ingestion_date = current_date()  # Base date for initial load\n",
    "ingestion_date_day2 = date_add(ingestion_date, 1)  # Simulate Day 2 ingestion date\n",
    "\n",
    "# Example data for dimension tables (Day 1)\n",
    "raw_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\", \"Segment A\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\", \"Segment B\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\", \"Segment A\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\", \"Segment C\")\n",
    "]\n",
    "\n",
    "raw_customer_segment_data = [\n",
    "    (\"Segment A\", \"High Value Customers\"),\n",
    "    (\"Segment B\", \"Medium Value Customers\"),\n",
    "    (\"Segment C\", \"Low Value Customers\")\n",
    "]\n",
    "\n",
    "# Example data for fact table (Day 1)\n",
    "raw_sales_data = [\n",
    "    (101, 1, 1, \"2024-03-01\", 2, 100),\n",
    "    (102, 2, 2, \"2024-03-02\", 1, 50),\n",
    "    (103, 3, 3, \"2024-03-02\", 3, 200),\n",
    "    (104, 1, 1, \"2024-03-03\", 1, 50),\n",
    "    (105, 2, 2, \"2024-03-04\", 2, 150)\n",
    "]\n",
    "\n",
    "# Create schemas if not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "# Create DataFrames for separate tables in the bronze layer with ingestion_date (Day 1)\n",
    "raw_customer_df = spark.createDataFrame(raw_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_segment\"]) \\\n",
    "    .withColumn(\"ingestion_date\", lit(ingestion_date))\n",
    "\n",
    "raw_customer_segment_df = spark.createDataFrame(raw_customer_segment_data, [\"customer_segment\", \"segment_description\"]) \\\n",
    "    .withColumn(\"ingestion_date\", lit(ingestion_date))\n",
    "\n",
    "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", lit(ingestion_date))\n",
    "\n",
    "# Bronze Layer: Write each table separately to Delta Lake\n",
    "raw_customer_df.write.format(\"delta\").mode(\"append\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "\n",
    "raw_customer_segment_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/raw_customer_segment\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer_segment USING DELTA LOCATION '/tmp/raw_customer_segment'\")\n",
    "\n",
    "raw_sales_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").save(\"/tmp/raw_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n",
    "\n",
    "# Silver Layer: Join and transform data, then write to Delta Lake\n",
    "silver_customer_df = spark.read.format(\"delta\").table(\"bronze.raw_customer\").alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_segment\").alias(\"seg\"), col(\"cust.customer_segment\") == col(\"seg.customer_segment\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_segment\"), col(\"seg.segment_description\").alias(\"customer_segment_description\"),\n",
    "            col(\"cust.ingestion_date\")) \\\n",
    "    .dropDuplicates([\"customer_id\"])  # Deduplication step\n",
    "\n",
    "silver_customer_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_dim_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"bronze.raw_sales\") \\\n",
    "    .join(silver_customer_df, \"customer_id\") \\\n",
    "    .drop(silver_customer_df[\"ingestion_date\"]) \\\n",
    "    .dropDuplicates([\"order_id\"])  # Deduplication step\n",
    "\n",
    "# Split fact_sales into three tables based on customer segment\n",
    "silver_sales_high_value_df = silver_sales_df.filter(col(\"customer_segment_description\") == \"High Value Customers\")\n",
    "silver_sales_medium_value_df = silver_sales_df.filter(col(\"customer_segment_description\") == \"Medium Value Customers\")\n",
    "silver_sales_low_value_df = silver_sales_df.filter(col(\"customer_segment_description\") == \"Low Value Customers\")\n",
    "\n",
    "# Write each segment to a separate Delta table\n",
    "silver_sales_high_value_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_high_value\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales_high_value USING DELTA LOCATION '/tmp/silver_fact_sales_high_value'\")\n",
    "\n",
    "silver_sales_medium_value_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_medium_value\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales_medium_value USING DELTA LOCATION '/tmp/silver_fact_sales_medium_value'\")\n",
    "\n",
    "silver_sales_low_value_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_low_value\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales_low_value USING DELTA LOCATION '/tmp/silver_fact_sales_low_value'\")\n",
    "\n",
    "# Gold Layer: Load Silver Layer data and perform union, then write to Delta Lake\n",
    "silver_sales_high_value_df = spark.read.format(\"delta\").table(\"silver.fact_sales_high_value\")\n",
    "silver_sales_medium_value_df = spark.read.format(\"delta\").table(\"silver.fact_sales_medium_value\")\n",
    "silver_sales_low_value_df = spark.read.format(\"delta\").table(\"silver.fact_sales_low_value\")\n",
    "\n",
    "# Perform union of the three segment fact tables\n",
    "gold_fact_sales_df = silver_sales_high_value_df.unionByName(silver_sales_medium_value_df).unionByName(silver_sales_low_value_df)\n",
    "\n",
    "# Write the combined fact table to the gold layer\n",
    "gold_fact_sales_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/gold_fact_sales_summary\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS gold.fact_sales_summary USING DELTA LOCATION '/tmp/gold_fact_sales_summary'\")\n",
    "\n",
    "# Day 2 Example Data\n",
    "day2_customer_data = [\n",
    "    (5, \"Alice Green\", \"Seattle\", \"WA\", \"USA\", \"Segment A\"),\n",
    "    (6, \"Bob White\", \"Miami\", \"FL\", \"USA\", \"Segment B\")\n",
    "]\n",
    "\n",
    "day2_sales_data = [\n",
    "    (106, 5, 1, \"2024-03-05\", 2, 120),\n",
    "    (107, 6, 2, \"2024-03-06\", 1, 80)\n",
    "]\n",
    "\n",
    "# Create DataFrames for Day 2 data with ingestion_date\n",
    "day2_customer_df = spark.createDataFrame(day2_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_segment\"]) \\\n",
    "    .withColumn(\"ingestion_date\", lit(ingestion_date_day2))\n",
    "\n",
    "day2_sales_df = spark.createDataFrame(day2_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", lit(ingestion_date_day2))\n",
    "\n",
    "# Bronze Layer: Append Day 2 data to Delta Lake\n",
    "day2_customer_df.write.format(\"delta\").mode(\"append\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "day2_sales_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").save(\"/tmp/raw_sales\")\n",
    "\n",
    "# Silver Layer: Append Day 2 data to Silver Tables\n",
    "silver_customer_df_day2 = day2_customer_df.alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_segment\").alias(\"seg\"), col(\"cust.customer_segment\") == col(\"seg.customer_segment\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_segment\"), col(\"seg.segment_description\").alias(\"customer_segment_description\"),\n",
    "            col(\"cust.ingestion_date\")) \\\n",
    "    .dropDuplicates([\"customer_id\"])  # Deduplication step\n",
    "\n",
    "silver_customer_df_day2.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_dim_customer\")\n",
    "\n",
    "silver_sales_df_day2 = day2_sales_df \\\n",
    "    .join(silver_customer_df_day2, \"customer_id\") \\\n",
    "    .drop(silver_customer_df_day2[\"ingestion_date\"]) \\\n",
    "    .dropDuplicates([\"order_id\"])  # Deduplication step\n",
    "\n",
    "# Split Day 2 fact_sales into three tables based on customer segment\n",
    "silver_sales_high_value_df_day2 = silver_sales_df_day2.filter(col(\"customer_segment_description\") == \"High Value Customers\")\n",
    "silver_sales_medium_value_df_day2 = silver_sales_df_day2.filter(col(\"customer_segment_description\") == \"Medium Value Customers\")\n",
    "silver_sales_low_value_df_day2 = silver_sales_df_day2.filter(col(\"customer_segment_description\") == \"Low Value Customers\")\n",
    "\n",
    "# Append Day 2 data to the Silver Fact Tables\n",
    "silver_sales_high_value_df_day2.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_high_value\")\n",
    "silver_sales_medium_value_df_day2.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_medium_value\")\n",
    "silver_sales_low_value_df_day2.write.format(\"delta\").mode(\"append\").save(\"/tmp/silver_fact_sales_low_value\")\n",
    "\n",
    "# Gold Layer: Append Day 2 data to Gold Table\n",
    "gold_fact_sales_df_day2 = silver_sales_high_value_df_day2.unionByName(silver_sales_medium_value_df_day2).unionByName(silver_sales_low_value_df_day2)\n",
    "gold_fact_sales_df_day2.write.format(\"delta\").mode(\"append\").save(\"/tmp/gold_fact_sales_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545a36b0-4463-4e0e-8abe-0659d7be7992",
     "showTitle": true,
     "title": "Day 1 Script for Dimensional Modeling Delta lakehouse Data Load"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# # Ensure proper schema registration for all layers\n",
    "spark.sql(\"DROP SCHEMA  IF EXISTS bronze CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS silver CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS gold CASCADE\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DimensionalModeling\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 0)  # The date of the load\n",
    "\n",
    "# Example data for dimension tables\n",
    "raw_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\", \"Segment A\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\", \"Segment B\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\", \"Segment A\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\", \"Segment C\")\n",
    "]\n",
    "\n",
    "raw_customer_group_data = [\n",
    "    (\"Segment A\", \"High Value Customers\"),\n",
    "    (\"Segment B\", \"Medium Value Customers\"),\n",
    "    (\"Segment C\", \"Low Value Customers\")\n",
    "]\n",
    "\n",
    "raw_product_data = [\n",
    "    (1, \"Product A\", \"Category X\", \"Group 1\"),\n",
    "    (2, \"Product B\", \"Category Y\", \"Group 2\"),\n",
    "    (3, \"Product C\", \"Category Z\", \"Group 3\")\n",
    "]\n",
    "\n",
    "raw_product_group_data = [\n",
    "    (\"Group 1\", \"Electronics\"),\n",
    "    (\"Group 2\", \"Clothing\"),\n",
    "    (\"Group 3\", \"Home & Kitchen\")\n",
    "]\n",
    "\n",
    "raw_date_data = [\n",
    "    (\"2024-03-01\", \"2024-03-01\", 2024, 3),\n",
    "    (\"2024-03-02\", \"2024-03-02\", 2024, 3),\n",
    "    (\"2024-03-03\", \"2024-03-03\", 2024, 3),\n",
    "    (\"2024-03-04\", \"2024-03-04\", 2024, 3)\n",
    "]\n",
    "\n",
    "# Example data for fact table\n",
    "raw_sales_data = [\n",
    "    (101, 1, 1, \"2024-03-01\", 2, 100),\n",
    "    (102, 2, 2, \"2024-03-02\", 1, 50),\n",
    "    (103, 3, 3, \"2024-03-02\", 3, 200),\n",
    "    (104, 1, 1, \"2024-03-03\", 1, 50),\n",
    "    (105, 2, 2, \"2024-03-04\", 2, 150)\n",
    "]\n",
    "\n",
    "# Ensure proper schema registration for all layers\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "# Create DataFrames for separate tables in the bronze layer with ingestion_date\n",
    "raw_customer_df = spark.createDataFrame(raw_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_customer_group_df = spark.createDataFrame(raw_customer_group_data, [\"customer_group\", \"group_description\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_product_df = spark.createDataFrame(raw_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_product_group_df = spark.createDataFrame(raw_product_group_data, [\"product_group\", \"group_description\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_date_df = spark.createDataFrame(raw_date_data, [\"order_date\", \"full_date\", \"year\", \"month\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Write each table separately to Delta Lake without joining\n",
    "raw_customer_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "\n",
    "raw_customer_group_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_customer_group\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer_group USING DELTA LOCATION '/tmp/raw_customer_group'\")\n",
    "\n",
    "raw_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_product USING DELTA LOCATION '/tmp/raw_product'\")\n",
    "\n",
    "raw_product_group_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product_group\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_product_group USING DELTA LOCATION '/tmp/raw_product_group'\")\n",
    "\n",
    "raw_date_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\", \"month\").option(\"zorder\", \"order_date\").save(\"/tmp/raw_date\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_date USING DELTA LOCATION '/tmp/raw_date'\")\n",
    "\n",
    "raw_sales_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_date\").option(\"zorder\", \"customer_id\").save(\"/tmp/raw_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n",
    "\n",
    "# Silver Layer: Join and transform data, then write to Delta Lake\n",
    "silver_customer_df = spark.read.format(\"delta\").table(\"bronze.raw_customer\").alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_group\").alias(\"grp\"), col(\"cust.customer_group\") == col(\"grp.customer_group\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_group\"), col(\"grp.group_description\").alias(\"customer_group_description\"),\n",
    "            col(\"cust.ingestion_date\"))\n",
    "\n",
    "silver_customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "\n",
    "silver_product_df = spark.read.format(\"delta\").table(\"bronze.raw_product\").alias(\"prod\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_product_group\").alias(\"grp\"), col(\"prod.product_group\") == col(\"grp.product_group\"), \"left\") \\\n",
    "    .select(col(\"prod.product_id\"), col(\"prod.product_name\"), col(\"prod.product_category\"),\n",
    "            col(\"grp.group_description\").alias(\"product_group_description\"),\n",
    "            col(\"prod.ingestion_date\"))\n",
    "\n",
    "silver_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_product USING DELTA LOCATION '/tmp/silver_dim_product'\")\n",
    "\n",
    "# Date and Sales Data - No changes needed for Silver Layer\n",
    "silver_date_df = spark.read.format(\"delta\").table(\"bronze.raw_date\")\n",
    "silver_date_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_date\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_date USING DELTA LOCATION '/tmp/silver_dim_date'\")\n",
    "\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"bronze.raw_sales\")\n",
    "silver_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales USING DELTA LOCATION '/tmp/silver_fact_sales'\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297dd865-06c4-4480-8c8c-e44ceb178b72",
     "showTitle": true,
     "title": "Day 1 Script for gold layer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Gold Layer: Aggregate and enrich data, then write to Delta Lake\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoldFactTableWithTimeDimension\").getOrCreate()\n",
    "\n",
    "# Load Silver Layer data\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"silver.fact_sales\")\n",
    "dim_customer_df = spark.read.format(\"delta\").table(\"silver.dim_customer\")\n",
    "dim_product_df = spark.read.format(\"delta\").table(\"silver.dim_product\")\n",
    "dim_date_df = spark.read.format(\"delta\").table(\"silver.dim_date\")\n",
    "\n",
    "# Join sales with customer, product, and date dimensions\n",
    "fact_sales_with_dimensions_df = silver_sales_df \\\n",
    "    .join(dim_customer_df, \"customer_id\", \"left\") \\\n",
    "    .join(dim_product_df, \"product_id\", \"left\") \\\n",
    "    .join(dim_date_df, silver_sales_df[\"order_date\"] == dim_date_df[\"order_date\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "        silver_sales_df[\"order_date\"].alias(\"order_date\"),  # Explicit reference to fact_sales order_date\n",
    "        col(\"quantity\"),\n",
    "        col(\"amount\"),\n",
    "        col(\"customer_group_description\"),\n",
    "        col(\"product_group_description\"),\n",
    "        col(\"product_category\"),\n",
    "        col(\"state\"),\n",
    "        col(\"year\"),\n",
    "        col(\"month\"),\n",
    "        silver_sales_df[\"ingestion_date\"].alias(\"ingestion_date\")  # Explicit reference to fact_sales ingestion_date\n",
    "    )\n",
    "\n",
    "# Write the combined fact table to the gold layer\n",
    "fact_sales_with_dimensions_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS gold.fact_sales_summary  USING DELTA LOCATION '/tmp/gold_fact_sales'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7892e751-a7a9-4656-a9c8-c827ad111c10",
     "showTitle": true,
     "title": "Next Day Script for Daily Delta Data Load Appending"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DailyDeltaLoad\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 1)  # The date of the load\n",
    "\n",
    "# New data for the next day\n",
    "next_day_customer_data = [\n",
    "    (5, \"Alice Williams\", \"Houston\", \"TX\", \"USA\", \"Group B\"),\n",
    "    (6, \"Bob Davis\", \"Seattle\", \"WA\", \"USA\", \"Group A\")\n",
    "]\n",
    "\n",
    "next_day_product_data = [\n",
    "    (4, \"Product D\", \"Category W\", \"Group 2\")\n",
    "]\n",
    "\n",
    "next_day_sales_data = [\n",
    "    (106, 5, 4, \"2024-03-05\", 1, 75),\n",
    "    (107, 6, 4, \"2024-03-05\", 2, 100)\n",
    "]\n",
    "\n",
    "# Create DataFrames for next day's data\n",
    "next_day_customer_df = spark.createDataFrame(next_day_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_product_df = spark.createDataFrame(next_day_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_sales_df = spark.createDataFrame(next_day_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Append new data to existing Delta Lake tables\n",
    "next_day_customer_df.write.format(\"delta\").mode(\"append\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "next_day_product_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/raw_product\")\n",
    "next_day_sales_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").save(\"/tmp/raw_sales\")\n",
    "\n",
    "# Silver Layer: Update Silver tables\n",
    "# Join new customer data with existing data\n",
    "silver_customer_df = spark.read.format(\"delta\").table(\"bronze.raw_customer\").alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_group\").alias(\"grp\"), col(\"cust.customer_group\") == col(\"grp.customer_group\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_group\"), col(\"grp.group_description\").alias(\"customer_group_description\"),\n",
    "            col(\"cust.ingestion_date\"))\n",
    "\n",
    "silver_customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_customer\")\n",
    "\n",
    "# Join new product data with existing data\n",
    "silver_product_df = spark.read.format(\"delta\").table(\"bronze.raw_product\").alias(\"prod\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_product_group\").alias(\"grp\"), col(\"prod.product_group\") == col(\"grp.product_group\"), \"left\") \\\n",
    "    .select(col(\"prod.product_id\"), col(\"prod.product_name\"), col(\"prod.product_category\"),\n",
    "            col(\"grp.group_description\").alias(\"product_group_description\"),\n",
    "            col(\"prod.ingestion_date\"))\n",
    "\n",
    "silver_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_product\")\n",
    "\n",
    "# Date and Sales Data - No changes needed for Silver Layer\n",
    "silver_date_df = spark.read.format(\"delta\").table(\"bronze.raw_date\")\n",
    "silver_date_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_date\")\n",
    "\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"bronze.raw_sales\")\n",
    "silver_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_fact_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d22d15-d53f-4140-9de3-a4c2db88f490",
     "showTitle": true,
     "title": "Next Day Script for Daily Delta Data Load with full load of customer, product and sales"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DailyDeltaLoadFull\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 1)  # The date of the load\n",
    "\n",
    "# New data for the next day (replacing previous day’s data)\n",
    "next_day_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\", \"Group A\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\", \"Group B\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\", \"Group A\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\", \"Group C\"),\n",
    "    (5, \"Alice Williams\", \"Houston\", \"TX\", \"USA\", \"Group B\"),\n",
    "    (6, \"Bob Davis\", \"Seattle\", \"WA\", \"USA\", \"Group A\")\n",
    "]\n",
    "\n",
    "next_day_product_data = [\n",
    "    (1, \"Product A\", \"Category X\", \"Group 1\"),\n",
    "    (2, \"Product B\", \"Category Y\", \"Group 2\"),\n",
    "    (3, \"Product C\", \"Category Z\", \"Group 3\"),\n",
    "    (4, \"Product D\", \"Category W\", \"Group 2\")\n",
    "]\n",
    "\n",
    "next_day_sales_data = [\n",
    "    (101, 1, 1, \"2024-03-01\", 2, 100),\n",
    "    (102, 2, 2, \"2024-03-02\", 1, 50),\n",
    "    (103, 3, 3, \"2024-03-02\", 3, 200),\n",
    "    (104, 1, 1, \"2024-03-03\", 1, 50),\n",
    "    (105, 2, 2, \"2024-03-04\", 2, 150),\n",
    "    (106, 5, 4, \"2024-03-05\", 1, 75),\n",
    "    (107, 6, 4, \"2024-03-05\", 2, 100)\n",
    "]\n",
    "\n",
    "# Create DataFrames for the full load of data\n",
    "next_day_customer_df = spark.createDataFrame(next_day_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_product_df = spark.createDataFrame(next_day_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_sales_df = spark.createDataFrame(next_day_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Overwrite the entire tables with new data\n",
    "next_day_customer_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "\n",
    "next_day_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_product USING DELTA LOCATION '/tmp/raw_product'\")\n",
    "\n",
    "next_day_sales_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_date\").save(\"/tmp/raw_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n",
    "\n",
    "# Silver Layer: Recompute Silver tables based on the new Bronze data\n",
    "# Join customer data with customer group data\n",
    "silver_customer_df = spark.read.format(\"delta\").table(\"bronze.raw_customer\").alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_group\").alias(\"grp\"), col(\"cust.customer_group\") == col(\"grp.customer_group\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_group\"), col(\"grp.group_description\").alias(\"customer_group_description\"),\n",
    "            col(\"cust.ingestion_date\"))\n",
    "\n",
    "silver_customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "\n",
    "# Join product data with product group data\n",
    "silver_product_df = spark.read.format(\"delta\").table(\"bronze.raw_product\").alias(\"prod\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_product_group\").alias(\"grp\"), col(\"prod.product_group\") == col(\"grp.product_group\"), \"left\") \\\n",
    "    .select(col(\"prod.product_id\"), col(\"prod.product_name\"), col(\"prod.product_category\"),\n",
    "            col(\"grp.group_description\").alias(\"product_group_description\"),\n",
    "            col(\"prod.ingestion_date\"))\n",
    "\n",
    "silver_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_product USING DELTA LOCATION '/tmp/silver_dim_product'\")\n",
    "\n",
    "# Overwrite Silver date and sales tables\n",
    "silver_date_df = spark.read.format(\"delta\").table(\"bronze.raw_date\")\n",
    "silver_date_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_date\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_date USING DELTA LOCATION '/tmp/silver_dim_date'\")\n",
    "\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"bronze.raw_sales\")\n",
    "silver_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales USING DELTA LOCATION '/tmp/silver_fact_sales'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d019813-c07c-4ccf-ab0b-71323628042e",
     "showTitle": true,
     "title": "Next Day Script for Daily Delta Data Load upsert merge"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DailyDeltaLoadIncrementalMerge\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 1)  # The date of the load\n",
    "\n",
    "# New data for the next day (incremental data)\n",
    "next_day_customer_data = [\n",
    "    (5, \"Alice Williams\", \"Chicago\", \"IL\", \"USA\", \"Group C\"),\n",
    "    (6, \"Bob Davis\",  \"Los Angeles\", \"CA\", \"USA\", \"Group B\"),\n",
    "    (11, \"James Williams\", \"Houston\", \"TX\", \"USA\", \"Group B\"),\n",
    "    (12, \"John Davis\", \"Seattle\", \"WA\", \"USA\", \"Group A\")\n",
    "]\n",
    "\n",
    "next_day_product_data = [\n",
    "    (3, \"Product C\", \"Category X\", \"Group 3\"),\n",
    "    (4, \"Product D\", \"Category Z\", \"Group 2\"),\n",
    "    (101, \"Product 101\", \"Category X\", \"Group 1\"),\n",
    "    (102, \"Product 102\", \"Category Y\", \"Group 3\")\n",
    "]\n",
    "\n",
    "next_day_sales_data = [\n",
    "    (106, 5, 4, \"2024-03-05\", 1, 75),\n",
    "    (107, 6, 4, \"2024-03-05\", 2, 100),\n",
    "    (201, 11, 101, \"2024-03-06\", 1, 75),\n",
    "    (202, 12, 102, \"2024-03-06\", 2, 100)\n",
    "]\n",
    "\n",
    "# Create DataFrames for next day's data\n",
    "next_day_customer_df = spark.createDataFrame(next_day_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_product_df = spark.createDataFrame(next_day_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_sales_df = spark.createDataFrame(next_day_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Merge new data into existing Delta Lake tables\n",
    "\n",
    "# Merge customer data\n",
    "next_day_customer_df.createOrReplaceTempView(\"staging_customer\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO bronze.raw_customer AS target\n",
    "USING staging_customer AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Merge product data\n",
    "next_day_product_df.createOrReplaceTempView(\"staging_product\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO bronze.raw_product AS target\n",
    "USING staging_product AS source\n",
    "ON target.product_id = source.product_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Merge sales data\n",
    "next_day_sales_df.createOrReplaceTempView(\"staging_sales\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO bronze.raw_sales AS target\n",
    "USING staging_sales AS source\n",
    "ON target.order_id = source.order_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Silver Layer: Update Silver tables with merged data\n",
    "# Join customer data with customer group data\n",
    "silver_customer_df = spark.read.format(\"delta\").table(\"bronze.raw_customer\").alias(\"cust\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_customer_group\").alias(\"grp\"), col(\"cust.customer_group\") == col(\"grp.customer_group\"), \"left\") \\\n",
    "    .select(col(\"cust.customer_id\"), col(\"cust.customer_name\"), col(\"cust.city\"), col(\"cust.state\"), col(\"cust.country\"),\n",
    "            col(\"cust.customer_group\"), col(\"grp.group_description\").alias(\"customer_group_description\"),\n",
    "            col(\"cust.ingestion_date\"))\n",
    "\n",
    "silver_customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "\n",
    "# Join product data with product group data\n",
    "silver_product_df = spark.read.format(\"delta\").table(\"bronze.raw_product\").alias(\"prod\") \\\n",
    "    .join(spark.read.format(\"delta\").table(\"bronze.raw_product_group\").alias(\"grp\"), col(\"prod.product_group\") == col(\"grp.product_group\"), \"left\") \\\n",
    "    .select(col(\"prod.product_id\"), col(\"prod.product_name\"), col(\"prod.product_category\"),\n",
    "            col(\"grp.group_description\").alias(\"product_group_description\"),\n",
    "            col(\"prod.ingestion_date\"))\n",
    "\n",
    "silver_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_product USING DELTA LOCATION '/tmp/silver_dim_product'\")\n",
    "\n",
    "# Date and Sales Data - No changes needed for Silver Layer\n",
    "silver_date_df = spark.read.format(\"delta\").table(\"bronze.raw_date\")\n",
    "silver_date_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_date\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.dim_date USING DELTA LOCATION '/tmp/silver_dim_date'\")\n",
    "\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"bronze.raw_sales\")\n",
    "silver_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.fact_sales USING DELTA LOCATION '/tmp/silver_fact_sales'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fac5de-a73d-4733-9bb0-57444295c876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Gold Layer: Aggregate and enrich data, then write to Delta Lake\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoldFactTableWithTimeDimension\").getOrCreate()\n",
    "\n",
    "# Load Silver Layer data\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"silver.fact_sales\")\n",
    "dim_customer_df = spark.read.format(\"delta\").table(\"silver.dim_customer\")\n",
    "dim_product_df = spark.read.format(\"delta\").table(\"silver.dim_product\")\n",
    "dim_date_df = spark.read.format(\"delta\").table(\"silver.dim_date\")\n",
    "\n",
    "# Join sales with customer, product, and date dimensions\n",
    "fact_sales_with_dimensions_df = silver_sales_df \\\n",
    "    .join(dim_customer_df, \"customer_id\", \"left\") \\\n",
    "    .join(dim_product_df, \"product_id\", \"left\") \\\n",
    "    .join(dim_date_df, silver_sales_df[\"order_date\"] == dim_date_df[\"order_date\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "        silver_sales_df[\"order_date\"].alias(\"order_date\"),  # Explicit reference to fact_sales order_date\n",
    "        col(\"quantity\"),\n",
    "        col(\"amount\"),\n",
    "        col(\"customer_group_description\"),\n",
    "        col(\"product_group_description\"),\n",
    "        col(\"product_category\"),\n",
    "        col(\"state\"),\n",
    "        col(\"year\"),\n",
    "        col(\"month\"),\n",
    "        silver_sales_df[\"ingestion_date\"].alias(\"ingestion_date\")  # Explicit reference to fact_sales ingestion_date\n",
    "    )\n",
    "\n",
    "# Write the combined fact table to the gold layer\n",
    "fact_sales_with_dimensions_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS gold.fact_sales_summary  USING DELTA LOCATION '/tmp/gold_fact_sales'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db79acda-0046-4082-87c5-8dedf63481b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[47]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, date_add, current_date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoldFactTableWithTimeDimension\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 1)  # The date of the load\n",
    "\n",
    "# Load Silver Layer data\n",
    "silver_sales_df = spark.read.format(\"delta\").table(\"silver.fact_sales\").filter(col(\"ingestion_date\") == ingestion_date)\n",
    "dim_customer_df = spark.read.format(\"delta\").table(\"silver.dim_customer\")\n",
    "dim_product_df = spark.read.format(\"delta\").table(\"silver.dim_product\")\n",
    "dim_date_df = spark.read.format(\"delta\").table(\"silver.dim_date\")\n",
    "\n",
    "# Join sales with customer, product, and date dimensions\n",
    "fact_sales_with_dimensions_df = silver_sales_df \\\n",
    "    .join(dim_customer_df, \"customer_id\", \"left\") \\\n",
    "    .join(dim_product_df, \"product_id\", \"left\") \\\n",
    "    .join(dim_date_df, silver_sales_df[\"order_date\"] == dim_date_df[\"order_date\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "        silver_sales_df[\"order_date\"].alias(\"order_date\"),  # Explicit reference to fact_sales order_date\n",
    "        col(\"quantity\"),\n",
    "        col(\"amount\"),\n",
    "        col(\"customer_group_description\"),\n",
    "        col(\"product_group_description\"),\n",
    "        col(\"product_category\"),\n",
    "        col(\"state\"),\n",
    "        col(\"year\"),\n",
    "        col(\"month\"),\n",
    "        silver_sales_df[\"ingestion_date\"].alias(\"ingestion_date\")  # Explicit reference to fact_sales ingestion_date\n",
    "    )\n",
    "\n",
    "# Append the combined fact table to the gold layer\n",
    "fact_sales_with_dimensions_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/gold_fact_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS gold.fact_sales_summary USING DELTA LOCATION '/tmp/gold_fact_sales'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000df5cb-356c-46d1-ab3c-a36b39ea5659",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>product_id</th><th>order_date</th><th>quantity</th><th>amount</th><th>customer_group_description</th><th>product_group_description</th><th>product_category</th><th>state</th><th>year</th><th>month</th><th>ingestion_date</th></tr></thead><tbody><tr><td>106</td><td>5</td><td>4</td><td>2024-03-05</td><td>1</td><td>75</td><td>Low Value Customers</td><td>Clothing</td><td>Category Z</td><td>IL</td><td>null</td><td>null</td><td>2024-08-11</td></tr><tr><td>107</td><td>6</td><td>4</td><td>2024-03-05</td><td>2</td><td>100</td><td>Medium Value Customers</td><td>Clothing</td><td>Category Z</td><td>CA</td><td>null</td><td>null</td><td>2024-08-11</td></tr><tr><td>201</td><td>11</td><td>101</td><td>2024-03-06</td><td>1</td><td>75</td><td>Medium Value Customers</td><td>Electronics</td><td>Category X</td><td>TX</td><td>null</td><td>null</td><td>2024-08-11</td></tr><tr><td>202</td><td>12</td><td>102</td><td>2024-03-06</td><td>2</td><td>100</td><td>High Value Customers</td><td>Home & Kitchen</td><td>Category Y</td><td>WA</td><td>null</td><td>null</td><td>2024-08-11</td></tr><tr><td>103</td><td>3</td><td>3</td><td>2024-03-02</td><td>3</td><td>200</td><td>High Value Customers</td><td>Home & Kitchen</td><td>Category X</td><td>CA</td><td>2024</td><td>3</td><td>2024-08-11</td></tr><tr><td>102</td><td>2</td><td>2</td><td>2024-03-02</td><td>1</td><td>50</td><td>Medium Value Customers</td><td>Clothing</td><td>Category Y</td><td>CA</td><td>2024</td><td>3</td><td>2024-08-11</td></tr><tr><td>104</td><td>1</td><td>1</td><td>2024-03-03</td><td>1</td><td>50</td><td>High Value Customers</td><td>Electronics</td><td>Category X</td><td>NY</td><td>2024</td><td>3</td><td>2024-08-11</td></tr><tr><td>101</td><td>1</td><td>1</td><td>2024-03-01</td><td>2</td><td>100</td><td>High Value Customers</td><td>Electronics</td><td>Category X</td><td>NY</td><td>2024</td><td>3</td><td>2024-08-11</td></tr><tr><td>105</td><td>2</td><td>2</td><td>2024-03-04</td><td>2</td><td>150</td><td>Medium Value Customers</td><td>Clothing</td><td>Category Y</td><td>CA</td><td>2024</td><td>3</td><td>2024-08-11</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         106,
         5,
         4,
         "2024-03-05",
         1,
         75,
         "Low Value Customers",
         "Clothing",
         "Category Z",
         "IL",
         null,
         null,
         "2024-08-11"
        ],
        [
         107,
         6,
         4,
         "2024-03-05",
         2,
         100,
         "Medium Value Customers",
         "Clothing",
         "Category Z",
         "CA",
         null,
         null,
         "2024-08-11"
        ],
        [
         201,
         11,
         101,
         "2024-03-06",
         1,
         75,
         "Medium Value Customers",
         "Electronics",
         "Category X",
         "TX",
         null,
         null,
         "2024-08-11"
        ],
        [
         202,
         12,
         102,
         "2024-03-06",
         2,
         100,
         "High Value Customers",
         "Home & Kitchen",
         "Category Y",
         "WA",
         null,
         null,
         "2024-08-11"
        ],
        [
         103,
         3,
         3,
         "2024-03-02",
         3,
         200,
         "High Value Customers",
         "Home & Kitchen",
         "Category X",
         "CA",
         2024,
         3,
         "2024-08-11"
        ],
        [
         102,
         2,
         2,
         "2024-03-02",
         1,
         50,
         "Medium Value Customers",
         "Clothing",
         "Category Y",
         "CA",
         2024,
         3,
         "2024-08-11"
        ],
        [
         104,
         1,
         1,
         "2024-03-03",
         1,
         50,
         "High Value Customers",
         "Electronics",
         "Category X",
         "NY",
         2024,
         3,
         "2024-08-11"
        ],
        [
         101,
         1,
         1,
         "2024-03-01",
         2,
         100,
         "High Value Customers",
         "Electronics",
         "Category X",
         "NY",
         2024,
         3,
         "2024-08-11"
        ],
        [
         105,
         2,
         2,
         "2024-03-04",
         2,
         150,
         "Medium Value Customers",
         "Clothing",
         "Category Y",
         "CA",
         2024,
         3,
         "2024-08-11"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_group_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_group_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ingestion_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fact_sales_with_dimensions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87dc82aa-dd34-4e21-b657-360c8b5dde0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af069177-4945-4655-a9ff-bbaa57751b53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- SQL Script to update Silver Layer\n",
    "\n",
    "-- Customer Dimension\n",
    "CREATE OR REPLACE TABLE silver.tab_dim_customer\n",
    "USING DELTA\n",
    "LOCATION '/tmp/path/to/external/silver/tab_dim_customer'\n",
    "AS\n",
    "SELECT\n",
    "    cust.customer_id,\n",
    "    cust.customer_name,\n",
    "    cust.city,\n",
    "    cust.state,\n",
    "    cust.country,\n",
    "    cust.customer_group,\n",
    "    grp.group_description AS customer_group_description,\n",
    "    cust.ingestion_date\n",
    "FROM bronze.raw_customer AS cust\n",
    "LEFT JOIN bronze.raw_customer_group AS grp\n",
    "    ON cust.customer_group = grp.customer_group;\n",
    "\n",
    "-- Product Dimension\n",
    "CREATE OR REPLACE TABLE silver.tab_dim_product\n",
    "USING DELTA\n",
    "LOCATION '/tmp/path/to/external/silver/tab_dim_product'\n",
    "AS\n",
    "SELECT\n",
    "    prod.product_id,\n",
    "    prod.product_name,\n",
    "    prod.product_category,\n",
    "    grp.group_description AS product_group_description,\n",
    "    prod.ingestion_date\n",
    "FROM bronze.raw_product AS prod\n",
    "LEFT JOIN bronze.raw_product_group AS grp\n",
    "    ON prod.product_group = grp.product_group;\n",
    "\n",
    "-- Date Dimension\n",
    "CREATE OR REPLACE TABLE silver.tab_dim_date\n",
    "USING DELTA\n",
    "LOCATION '/tmp/path/to/external/silver/tab_dim_date'\n",
    "AS\n",
    "SELECT\n",
    "    order_date,\n",
    "    full_date,\n",
    "    year,\n",
    "    month\n",
    "FROM bronze.raw_date;\n",
    "\n",
    "-- Sales Fact Table\n",
    "CREATE OR REPLACE TABLE silver.tab_fact_sales\n",
    "USING DELTA\n",
    "LOCATION '/tmp/path/to/external/silver/tab_fact_sales'\n",
    "AS\n",
    "SELECT\n",
    "    sales.order_id,\n",
    "    sales.customer_id,\n",
    "    sales.product_id,\n",
    "    sales.order_date,\n",
    "    sales.quantity,\n",
    "    sales.amount,\n",
    "    sales.ingestion_date\n",
    "FROM bronze.raw_sales AS sales;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d01250-e5f2-4909-a0a4-0a6abb8edb9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2828109350322065>:13\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m     display(df)\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m---> 13\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     15\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-2828109350322065>:9\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gQ3JlYXRlIHN0YWdpbmdfc2FsZXMgdmlldyBieSBmaWx0ZXJpbmcgdGhlIEJyb256ZSB0YWJsZSBvbiBpbmdlc3Rpb25fZGF0ZQpDUkVBVEUgT1IgUkVQTEFDRSBURU1QIFZJRVcgc3RhZ2luZ19zYWxlcyBBUwpTRUxFQ1QgCiAgICBvcmRlcl9pZCwKICAgIGN1c3RvbWVyX2lkLAogICAgcHJvZHVjdF9pZCwKICAgIG9yZGVyX2RhdGUsCiAgICBxdWFudGl0eSwKICAgIGFtb3VudApGUk9NIGJyb256ZS5yYXdfc2FsZXMKV0hFUkUgaW5nZXN0aW9uX2RhdGUgPSAnJw==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[1;32m      8\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmRpbV9jdXN0b21lcgpNRVJHRSBJTlRPIHNpbHZlci5kaW1fY3VzdG9tZXIgQVMgdGFyZ2V0ClVTSU5HIHN0YWdpbmdfY3VzdG9tZXIgQVMgc291cmNlCk9OIHRhcmdldC5jdXN0b21lcl9pZCA9IHNvdXJjZS5jdXN0b21lcl9pZApXSEVOIE1BVENIRUQgVEhFTgogIFVQREFURSBTRVQgCiAgICB0YXJnZXQuY3VzdG9tZXJfbmFtZSA9IHNvdXJjZS5jdXN0b21lcl9uYW1lLCAKICAgIHRhcmdldC5jaXR5ID0gc291cmNlLmNpdHksCiAgICB0YXJnZXQuc3RhdGUgPSBzb3VyY2Uuc3RhdGUsCiAgICB0YXJnZXQuY291bnRyeSA9IHNvdXJjZS5jb3VudHJ5LAogICAgdGFyZ2V0LmN1c3RvbWVyX2dyb3VwID0gc291cmNlLmN1c3RvbWVyX2dyb3VwLAogICAgdGFyZ2V0LmluZ2VzdGlvbl9kYXRlID0gJycKV0hFTiBOT1QgTUFUQ0hFRCBUSEVOCiAgSU5TRVJUIChjdXN0b21lcl9pZCwgY3VzdG9tZXJfbmFtZSwgY2l0eSwgc3RhdGUsIGNvdW50cnksIGN1c3RvbWVyX2dyb3VwLCBpbmdlc3Rpb25fZGF0ZSkKICBWQUxVRVMgKHNvdXJjZS5jdXN0b21lcl9pZCwgc291cmNlLmN1c3RvbWVyX25hbWUsIHNvdXJjZS5jaXR5LCBzb3VyY2Uuc3RhdGUsIHNvdXJjZS5jb3VudHJ5LCBzb3VyY2UuY3VzdG9tZXJfZ3JvdXAsICcnKQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[0;32m----> 9\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmRpbV9wcm9kdWN0Ck1FUkdFIElOVE8gc2lsdmVyLmRpbV9wcm9kdWN0IEFTIHRhcmdldApVU0lORyBzdGFnaW5nX3Byb2R1Y3QgQVMgc291cmNlCk9OIHRhcmdldC5wcm9kdWN0X2lkID0gc291cmNlLnByb2R1Y3RfaWQKV0hFTiBNQVRDSEVEIFRIRU4KICBVUERBVEUgU0VUIAogICAgdGFyZ2V0LnByb2R1Y3RfbmFtZSA9IHNvdXJjZS5wcm9kdWN0X25hbWUsCiAgICB0YXJnZXQucHJvZHVjdF9jYXRlZ29yeSA9IHNvdXJjZS5wcm9kdWN0X2NhdGVnb3J5LAogICAgdGFyZ2V0LnByb2R1Y3RfZ3JvdXAgPSBzb3VyY2UucHJvZHVjdF9ncm91cCwKICAgIHRhcmdldC5pbmdlc3Rpb25fZGF0ZSA9ICcnCldIRU4gTk9UIE1BVENIRUQgVEhFTgogIElOU0VSVCAocHJvZHVjdF9pZCwgcHJvZHVjdF9uYW1lLCBwcm9kdWN0X2NhdGVnb3J5LCBwcm9kdWN0X2dyb3VwLCBpbmdlc3Rpb25fZGF0ZSkKICBWQUxVRVMgKHNvdXJjZS5wcm9kdWN0X2lkLCBzb3VyY2UucHJvZHVjdF9uYW1lLCBzb3VyY2UucHJvZHVjdF9jYXRlZ29yeSwgc291cmNlLnByb2R1Y3RfZ3JvdXAsICcnKQ==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     10\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmZhY3Rfc2FsZXMKTUVSR0UgSU5UTyBzaWx2ZXIuZmFjdF9zYWxlcyBBUyB0YXJnZXQKVVNJTkcgc3RhZ2luZ19zYWxlcyBBUyBzb3VyY2UKT04gdGFyZ2V0Lm9yZGVyX2lkID0gc291cmNlLm9yZGVyX2lkCldIRU4gTUFUQ0hFRCBUSEVOCiAgVVBEQVRFIFNFVCAKICAgIHRhcmdldC5jdXN0b21lcl9pZCA9IHNvdXJjZS5jdXN0b21lcl9pZCwKICAgIHRhcmdldC5wcm9kdWN0X2lkID0gc291cmNlLnByb2R1Y3RfaWQsCiAgICB0YXJnZXQub3JkZXJfZGF0ZSA9IHNvdXJjZS5vcmRlcl9kYXRlLAogICAgdGFyZ2V0LnF1YW50aXR5ID0gc291cmNlLnF1YW50aXR5LAogICAgdGFyZ2V0LmFtb3VudCA9IHNvdXJjZS5hbW91bnQsCiAgICB0YXJnZXQuaW5nZXN0aW9uX2RhdGUgPSAnJwpXSEVOIE5PVCBNQVRDSEVEIFRIRU4KICBJTlNFUlQgKG9yZGVyX2lkLCBjdXN0b21lcl9pZCwgcHJvZHVjdF9pZCwgb3JkZXJfZGF0ZSwgcXVhbnRpdHksIGFtb3VudCwgaW5nZXN0aW9uX2RhdGUpCiAgVkFMVUVTIChzb3VyY2Uub3JkZXJfaWQsIHNvdXJjZS5jdXN0b21lcl9pZCwgc291cmNlLnByb2R1Y3RfaWQsIHNvdXJjZS5vcmRlcl9kYXRlLCBzb3VyY2UucXVhbnRpdHksIHNvdXJjZS5hbW91bnQsICcnKQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[1;32m     11\u001B[0m display(df)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: cannot resolve target.product_group in UPDATE clause given columns target.product_id, target.product_name, target.product_category, target.product_group_description, target.ingestion_date; line 2 pos 0"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2828109350322065>:13\u001B[0m\n\u001B[1;32m     11\u001B[0m     display(df)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m---> 13\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     15\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-2828109350322065>:9\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gQ3JlYXRlIHN0YWdpbmdfc2FsZXMgdmlldyBieSBmaWx0ZXJpbmcgdGhlIEJyb256ZSB0YWJsZSBvbiBpbmdlc3Rpb25fZGF0ZQpDUkVBVEUgT1IgUkVQTEFDRSBURU1QIFZJRVcgc3RhZ2luZ19zYWxlcyBBUwpTRUxFQ1QgCiAgICBvcmRlcl9pZCwKICAgIGN1c3RvbWVyX2lkLAogICAgcHJvZHVjdF9pZCwKICAgIG9yZGVyX2RhdGUsCiAgICBxdWFudGl0eSwKICAgIGFtb3VudApGUk9NIGJyb256ZS5yYXdfc2FsZXMKV0hFUkUgaW5nZXN0aW9uX2RhdGUgPSAnJw==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m      8\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmRpbV9jdXN0b21lcgpNRVJHRSBJTlRPIHNpbHZlci5kaW1fY3VzdG9tZXIgQVMgdGFyZ2V0ClVTSU5HIHN0YWdpbmdfY3VzdG9tZXIgQVMgc291cmNlCk9OIHRhcmdldC5jdXN0b21lcl9pZCA9IHNvdXJjZS5jdXN0b21lcl9pZApXSEVOIE1BVENIRUQgVEhFTgogIFVQREFURSBTRVQgCiAgICB0YXJnZXQuY3VzdG9tZXJfbmFtZSA9IHNvdXJjZS5jdXN0b21lcl9uYW1lLCAKICAgIHRhcmdldC5jaXR5ID0gc291cmNlLmNpdHksCiAgICB0YXJnZXQuc3RhdGUgPSBzb3VyY2Uuc3RhdGUsCiAgICB0YXJnZXQuY291bnRyeSA9IHNvdXJjZS5jb3VudHJ5LAogICAgdGFyZ2V0LmN1c3RvbWVyX2dyb3VwID0gc291cmNlLmN1c3RvbWVyX2dyb3VwLAogICAgdGFyZ2V0LmluZ2VzdGlvbl9kYXRlID0gJycKV0hFTiBOT1QgTUFUQ0hFRCBUSEVOCiAgSU5TRVJUIChjdXN0b21lcl9pZCwgY3VzdG9tZXJfbmFtZSwgY2l0eSwgc3RhdGUsIGNvdW50cnksIGN1c3RvbWVyX2dyb3VwLCBpbmdlc3Rpb25fZGF0ZSkKICBWQUxVRVMgKHNvdXJjZS5jdXN0b21lcl9pZCwgc291cmNlLmN1c3RvbWVyX25hbWUsIHNvdXJjZS5jaXR5LCBzb3VyY2Uuc3RhdGUsIHNvdXJjZS5jb3VudHJ5LCBzb3VyY2UuY3VzdG9tZXJfZ3JvdXAsICcnKQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[0;32m----> 9\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmRpbV9wcm9kdWN0Ck1FUkdFIElOVE8gc2lsdmVyLmRpbV9wcm9kdWN0IEFTIHRhcmdldApVU0lORyBzdGFnaW5nX3Byb2R1Y3QgQVMgc291cmNlCk9OIHRhcmdldC5wcm9kdWN0X2lkID0gc291cmNlLnByb2R1Y3RfaWQKV0hFTiBNQVRDSEVEIFRIRU4KICBVUERBVEUgU0VUIAogICAgdGFyZ2V0LnByb2R1Y3RfbmFtZSA9IHNvdXJjZS5wcm9kdWN0X25hbWUsCiAgICB0YXJnZXQucHJvZHVjdF9jYXRlZ29yeSA9IHNvdXJjZS5wcm9kdWN0X2NhdGVnb3J5LAogICAgdGFyZ2V0LnByb2R1Y3RfZ3JvdXAgPSBzb3VyY2UucHJvZHVjdF9ncm91cCwKICAgIHRhcmdldC5pbmdlc3Rpb25fZGF0ZSA9ICcnCldIRU4gTk9UIE1BVENIRUQgVEhFTgogIElOU0VSVCAocHJvZHVjdF9pZCwgcHJvZHVjdF9uYW1lLCBwcm9kdWN0X2NhdGVnb3J5LCBwcm9kdWN0X2dyb3VwLCBpbmdlc3Rpb25fZGF0ZSkKICBWQUxVRVMgKHNvdXJjZS5wcm9kdWN0X2lkLCBzb3VyY2UucHJvZHVjdF9uYW1lLCBzb3VyY2UucHJvZHVjdF9jYXRlZ29yeSwgc291cmNlLnByb2R1Y3RfZ3JvdXAsICcnKQ==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLS0gVXBzZXJ0IGludG8gc2lsdmVyLmZhY3Rfc2FsZXMKTUVSR0UgSU5UTyBzaWx2ZXIuZmFjdF9zYWxlcyBBUyB0YXJnZXQKVVNJTkcgc3RhZ2luZ19zYWxlcyBBUyBzb3VyY2UKT04gdGFyZ2V0Lm9yZGVyX2lkID0gc291cmNlLm9yZGVyX2lkCldIRU4gTUFUQ0hFRCBUSEVOCiAgVVBEQVRFIFNFVCAKICAgIHRhcmdldC5jdXN0b21lcl9pZCA9IHNvdXJjZS5jdXN0b21lcl9pZCwKICAgIHRhcmdldC5wcm9kdWN0X2lkID0gc291cmNlLnByb2R1Y3RfaWQsCiAgICB0YXJnZXQub3JkZXJfZGF0ZSA9IHNvdXJjZS5vcmRlcl9kYXRlLAogICAgdGFyZ2V0LnF1YW50aXR5ID0gc291cmNlLnF1YW50aXR5LAogICAgdGFyZ2V0LmFtb3VudCA9IHNvdXJjZS5hbW91bnQsCiAgICB0YXJnZXQuaW5nZXN0aW9uX2RhdGUgPSAnJwpXSEVOIE5PVCBNQVRDSEVEIFRIRU4KICBJTlNFUlQgKG9yZGVyX2lkLCBjdXN0b21lcl9pZCwgcHJvZHVjdF9pZCwgb3JkZXJfZGF0ZSwgcXVhbnRpdHksIGFtb3VudCwgaW5nZXN0aW9uX2RhdGUpCiAgVkFMVUVTIChzb3VyY2Uub3JkZXJfaWQsIHNvdXJjZS5jdXN0b21lcl9pZCwgc291cmNlLnByb2R1Y3RfaWQsIHNvdXJjZS5vcmRlcl9kYXRlLCBzb3VyY2UucXVhbnRpdHksIHNvdXJjZS5hbW91bnQsICcnKQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m     11\u001B[0m display(df)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: cannot resolve target.product_group in UPDATE clause given columns target.product_id, target.product_name, target.product_category, target.product_group_description, target.ingestion_date; line 2 pos 0",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: cannot resolve target.product_group in UPDATE clause given columns target.product_id, target.product_name, target.product_category, target.product_group_description, target.ingestion_date; line 2 pos 0",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Define ingestion_date as a variable for the current load\n",
    "SET ingestion_date = date_add(current_date(), 1);\n",
    "\n",
    "-- Create staging_customer view by filtering the Bronze table on ingestion_date\n",
    "CREATE OR REPLACE TEMP VIEW staging_customer AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    customer_group\n",
    "FROM bronze.raw_customer\n",
    "WHERE ingestion_date = '${ingestion_date}';\n",
    "\n",
    "-- Create staging_product view by filtering the Bronze table on ingestion_date\n",
    "CREATE OR REPLACE TEMP VIEW staging_product AS\n",
    "SELECT \n",
    "    product_id,\n",
    "    product_name,\n",
    "    product_category,\n",
    "    product_group\n",
    "FROM bronze.raw_product\n",
    "WHERE ingestion_date = '${ingestion_date}';\n",
    "\n",
    "-- Create staging_sales view by filtering the Bronze table on ingestion_date\n",
    "CREATE OR REPLACE TEMP VIEW staging_sales AS\n",
    "SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    order_date,\n",
    "    quantity,\n",
    "    amount\n",
    "FROM bronze.raw_sales\n",
    "WHERE ingestion_date = '${ingestion_date}';\n",
    "\n",
    "-- Upsert into silver.dim_customer\n",
    "MERGE INTO silver.dim_customer AS target\n",
    "USING staging_customer AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET \n",
    "    target.customer_name = source.customer_name, \n",
    "    target.city = source.city,\n",
    "    target.state = source.state,\n",
    "    target.country = source.country,\n",
    "    target.customer_group = source.customer_group,\n",
    "    target.ingestion_date = '${ingestion_date}'\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (customer_id, customer_name, city, state, country, customer_group, ingestion_date)\n",
    "  VALUES (source.customer_id, source.customer_name, source.city, source.state, source.country, source.customer_group, '${ingestion_date}');\n",
    "\n",
    "-- Upsert into silver.dim_product\n",
    "MERGE INTO silver.dim_product AS target\n",
    "USING staging_product AS source\n",
    "ON target.product_id = source.product_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET \n",
    "    target.product_name = source.product_name,\n",
    "    target.product_category = source.product_category,\n",
    "    target.product_group = source.product_group,\n",
    "    target.ingestion_date = '${ingestion_date}'\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (product_id, product_name, product_category, product_group, ingestion_date)\n",
    "  VALUES (source.product_id, source.product_name, source.product_category, source.product_group, '${ingestion_date}');\n",
    "\n",
    "-- Upsert into silver.fact_sales\n",
    "MERGE INTO silver.fact_sales AS target\n",
    "USING staging_sales AS source\n",
    "ON target.order_id = source.order_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET \n",
    "    target.customer_id = source.customer_id,\n",
    "    target.product_id = source.product_id,\n",
    "    target.order_date = source.order_date,\n",
    "    target.quantity = source.quantity,\n",
    "    target.amount = source.amount,\n",
    "    target.ingestion_date = '${ingestion_date}'\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (order_id, customer_id, product_id, order_date, quantity, amount, ingestion_date)\n",
    "  VALUES (source.order_id, source.customer_id, source.product_id, source.order_date, source.quantity, source.amount, '${ingestion_date}');\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 496076780656032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "0lakehouse",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
