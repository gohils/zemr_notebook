{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b160084d-45f4-46ca-8f77-07d95be52306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "# Example data for dimension tables\n",
    "raw_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\")\n",
    "]\n",
    "\n",
    "raw_product_data = [\n",
    "    (1, \"Product A\", \"Category X\"),\n",
    "    (2, \"Product B\", \"Category Y\"),\n",
    "    (3, \"Product C\", \"Category Z\")\n",
    "]\n",
    "\n",
    "raw_date_data = [\n",
    "    (\"2024-03-01\", \"2024-03-01\", 2024, 3),\n",
    "    (\"2024-03-02\", \"2024-03-02\", 2024, 3),\n",
    "    (\"2024-03-03\", \"2024-03-03\", 2024, 3),\n",
    "    (\"2024-03-04\", \"2024-03-04\", 2024, 3)\n",
    "]\n",
    "\n",
    "# Example data for fact table\n",
    "raw_sales_data = [\n",
    "    (101, 1, 1, \"2024-03-01\", 2, 100),\n",
    "    (102, 2, 2, \"2024-03-02\", 1, 50),\n",
    "    (103, 3, 3, \"2024-03-02\", 3, 200),\n",
    "    (104, 1, 1, \"2024-03-03\", 1, 50),\n",
    "    (105, 2, 2, \"2024-03-04\", 2, 150)\n",
    "]\n",
    "\n",
    "# Create DataFrames for dimension and fact tables\n",
    "raw_customer_df = spark.createDataFrame(raw_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\"])\n",
    "raw_product_df = spark.createDataFrame(raw_product_data, [\"product_id\", \"product_name\", \"product_category\"])\n",
    "raw_date_df = spark.createDataFrame(raw_date_data, [\"order_date\", \"full_date\", \"year\", \"month\"])\n",
    "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"])\n",
    "\n",
    "# Write DataFrames to Delta Lake for demonstration with partitioning and Z-Ordering\n",
    "raw_customer_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "raw_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product\")\n",
    "raw_date_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\", \"month\").option(\"zorder\", \"order_date\").save(\"/tmp/raw_date\")\n",
    "raw_sales_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_date\").option(\"zorder\", \"customer_id\").save(\"/tmp/raw_sales\")\n",
    "\n",
    "# Read data from Delta Lake for demonstration\n",
    "bronze_customer_df = spark.read.format(\"delta\").load(\"/tmp/raw_customer\")\n",
    "bronze_product_df = spark.read.format(\"delta\").load(\"/tmp/raw_product\")\n",
    "bronze_date_df = spark.read.format(\"delta\").load(\"/tmp/raw_date\")\n",
    "bronze_sales_df = spark.read.format(\"delta\").load(\"/tmp/raw_sales\")\n",
    "\n",
    "# Perform transformations to create silver layer\n",
    "\n",
    "# Write dimension tables to Delta Lake for silver layer\n",
    "bronze_customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_customer\")\n",
    "bronze_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_product\")\n",
    "bronze_date_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_dim_date\")\n",
    "\n",
    "# Write fact table to Delta Lake for silver layer\n",
    "bronze_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_fact_sales\")\n",
    "\n",
    "# Aggregate and enrich data to create gold layer\n",
    "\n",
    "# Aggregating sales data by month to create gold layer\n",
    "gold_sales_df = bronze_sales_df.groupBy(year(\"order_date\").alias(\"year\"), month(\"order_date\").alias(\"month\")) \\\n",
    "    .agg({\"amount\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_sales\")\n",
    "\n",
    "# Write aggregated and enriched data to Delta Lake for gold layer\n",
    "gold_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_sales\")\n",
    "\n",
    "# Register Delta tables into catalog with separate schemas for bronze, silver, and gold layers\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
    "\n",
    "# Register Delta tables into catalog with separate schemas for bronze, silver, and gold layers\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
    "\n",
    "# Register tables from bronze layer with bronze schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"USE bronze\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS raw_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS raw_product USING DELTA LOCATION '/tmp/raw_product'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS raw_date USING DELTA LOCATION '/tmp/raw_date'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS raw_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n",
    "\n",
    "# Register tables from silver layer with silver schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"USE silver\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_product USING DELTA LOCATION '/tmp/silver_dim_product'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_date USING DELTA LOCATION '/tmp/silver_dim_date'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS fact_sales USING DELTA LOCATION '/tmp/silver_fact_sales'\")\n",
    "\n",
    "# Register tables from gold layer with gold schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "spark.sql(\"USE gold\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS sales USING DELTA LOCATION '/tmp/gold_sales'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76599e80-5778-4d18-b300-5a5f7b18636e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Register Delta tables into catalog with separate schemas for bronze, silver, and gold layers\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
    "\n",
    "# Register tables from bronze layer with bronze schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"USE bronze\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_product USING DELTA LOCATION '/tmp/raw_product'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_date USING DELTA LOCATION '/tmp/raw_date'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS fact_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n",
    "\n",
    "# Register tables from silver layer with silver schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"USE silver\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_customer USING DELTA LOCATION '/tmp/silver_dim_customer'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_product USING DELTA LOCATION '/tmp/silver_dim_product'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dim_date USING DELTA LOCATION '/tmp/silver_dim_date'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS fact_sales USING DELTA LOCATION '/tmp/silver_fact_sales'\")\n",
    "\n",
    "# Register tables from gold layer with gold schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "spark.sql(\"USE gold\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS sales USING DELTA LOCATION '/tmp/gold_sales'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bea174f-9795-4b5f-a668-d9d266d8749d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example data for dimension table\n",
    "dim_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\")\n",
    "]\n",
    "\n",
    "# Example data for fact table\n",
    "fact_sales_data = [\n",
    "    (101, 1, \"2024-03-01\", 1, 2, 100),\n",
    "    (102, 2, \"2024-03-02\", 2, 1, 50),\n",
    "    (103, 3, \"2024-03-02\", 3, 3, 200),\n",
    "    (104, 1, \"2024-03-03\", 1, 1, 50),\n",
    "    (105, 2, \"2024-03-04\", 2, 2, 150)\n",
    "]\n",
    "\n",
    "# Create DataFrames for dimension and fact tables\n",
    "dim_customer_df = spark.createDataFrame(dim_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\"])\n",
    "fact_sales_df = spark.createDataFrame(fact_sales_data, [\"order_id\", \"customer_id\", \"order_date\", \"product_id\", \"quantity\", \"amount\"])\n",
    "\n",
    "# Write DataFrames to Delta Lake for demonstration with partitioning and Z-Ordering\n",
    "dim_customer_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"/tmp/dim_customer\")\n",
    "fact_sales_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_date\").option(\"zorder\", \"customer_id\").save(\"/tmp/fact_sales\")\n",
    "\n",
    "# Read data from Delta Lake for demonstration\n",
    "dim_customer_df = spark.read.format(\"delta\").load(\"/tmp/dim_customer\")\n",
    "fact_sales_df = spark.read.format(\"delta\").load(\"/tmp/fact_sales\")\n",
    "\n",
    "# 1. Predicate Pushdown: Filter rows as early as possible to minimize data movement and processing.\n",
    "filtered_df = fact_sales_df.filter(col(\"order_date\") == \"2024-03-02\")\n",
    "\n",
    "# 2. Partitioning: Organize data into partitions based on a specified column to enable partition pruning and reduce data shuffling.\n",
    "partitioned_fact_sales_df = fact_sales_df.repartition(\"order_date\")\n",
    "\n",
    "# 3. Broadcast Join: Broadcast smaller DataFrame to all worker nodes to avoid data shuffling during join operation.\n",
    "broadcasted_join_df = fact_sales_df.join(dim_customer_df, \"customer_id\").select(fact_sales_df[\"*\"], dim_customer_df[\"customer_name\"])\n",
    "\n",
    "# 4. Caching and Persisting: Cache DataFrame in memory or persist it to disk to avoid recomputation and improve query performance.\n",
    "fact_sales_df.cache()\n",
    "\n",
    "# 5. Parallelism Control: Control the number of shuffle partitions to optimize resource utilization and improve query performance.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "# 6. Query Optimization: Select only necessary columns in the DataFrame to minimize data movement and processing.\n",
    "optimized_df = fact_sales_df.select(\"order_id\", \"order_date\", \"amount\")\n",
    "\n",
    "# 7. Column Pruning: Select only necessary columns in the DataFrame to minimize data movement and processing.\n",
    "pruned_df = fact_sales_df.select(\"order_id\", \"customer_id\", \"order_date\")\n",
    "\n",
    "# 8. Query Caching: Cache DataFrame in memory to avoid recomputation and improve query performance.\n",
    "cached_result = fact_sales_df.select(\"customer_id\").cache()\n",
    "\n",
    "# 9. Materialized Views: Create a temporary view for DataFrame to optimize query execution and caching.\n",
    "fact_sales_df.createOrReplaceTempView(\"fact_sales_view\")\n",
    "\n",
    "# Create temporary views for DataFrames\n",
    "fact_sales_df.createOrReplaceTempView(\"fact_sales_view\")\n",
    "dim_customer_df.createOrReplaceTempView(\"dim_customer_view\")\n",
    "\n",
    "# 10. Join Reordering: Reorder join operations to optimize query execution.\n",
    "reordered_join_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_sales_view\n",
    "    JOIN dim_customer_view ON fact_sales_view.customer_id = dim_customer_view.customer_id\n",
    "\"\"\")\n",
    "\n",
    "# 11. Filter Pushdown: Push down filters to underlying data sources to minimize data movement and processing.\n",
    "pushed_down_filter_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_sales_view\n",
    "    WHERE order_date = '2024-03-02' AND amount > 100\n",
    "\"\"\")\n",
    "\n",
    "# 12. Vectorized Query Execution: Enable vectorized query execution to process data in batches and improve CPU efficiency.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# 13. Dynamic Partition Pruning: Enable dynamic partition pruning to optimize query performance by skipping unnecessary partitions.\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "\n",
    "# 14. Dynamic Runtime Filters: Use dynamic runtime filters to optimize join operations by filtering data dynamically based on join keys.\n",
    "dynamic_runtime_filters_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_sales_view\n",
    "    JOIN dim_customer_view ON fact_sales_view.customer_id = dim_customer_view.customer_id\n",
    "    WHERE fact_sales_view.order_date = '2024-03-02'\n",
    "\"\"\")\n",
    "\n",
    "# 15. Columnar Storage Compression: Utilize columnar storage compression techniques to reduce storage footprint and improve query performance.\n",
    "spark.conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec604f0-88a3-4aff-bde8-a80cffa0729b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2541916950838317,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "deltalake_lakehouse",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
