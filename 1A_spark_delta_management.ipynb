{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "#1 Set up spark context and SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------------------------------------------------------------+\n",
      "|name |age|row_sha2                                                        |\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "|John |32 |afabe3fe5711744f1a21433e21956111543727cae2f96e1a83bad2836da3c0ce|\n",
      "|David|22 |eb17e164bc6dd6c0521278fa34aeb0da179acfd247192d2f8c5d948518c8f2b7|\n",
      "|Raj  |10 |c37288dc8fe43e4f30f55ac9a93013555ffc1e9146acfb94416cb9f72d051de1|\n",
      "|Andy |25 |737fc02c0333519807e095c2d753d348a09063a20a9ede1c2da2e784e23551d5|\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "|name |age|row_sha2                                                        |\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "|John |33 |d486ab8a3b8333511bd12758895b6f1ae532e1d599f8acbe438d7d5ca4aebd3e|\n",
      "|David|22 |eb17e164bc6dd6c0521278fa34aeb0da179acfd247192d2f8c5d948518c8f2b7|\n",
      "|Raj  |11 |d0f22ed3e129a7b75b3dd00a947e7b45845fd1bfce47cc6f4d3b0634a7363ca2|\n",
      "|Paul |28 |b71b0817d3973d4b1c7ebcfb2bfe2afba533ba034896341c1200a657146845ac|\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "|name |age|row_sha2                                                        |\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "|John |33 |d486ab8a3b8333511bd12758895b6f1ae532e1d599f8acbe438d7d5ca4aebd3e|\n",
      "|David|22 |eb17e164bc6dd6c0521278fa34aeb0da179acfd247192d2f8c5d948518c8f2b7|\n",
      "|Raj  |11 |d0f22ed3e129a7b75b3dd00a947e7b45845fd1bfce47cc6f4d3b0634a7363ca2|\n",
      "|Paul |28 |b71b0817d3973d4b1c7ebcfb2bfe2afba533ba034896341c1200a657146845ac|\n",
      "+-----+---+----------------------------------------------------------------+\n",
      "\n",
      "+-----+---+--------------------------------+\n",
      "|name |age|row_md5                         |\n",
      "+-----+---+--------------------------------+\n",
      "|John |33 |67438d2e20542f920ac11184577a8ad0|\n",
      "|David|22 |c16c85a551ae6eac52a627b6bbed6cec|\n",
      "|Raj  |11 |21c9cf9a5a3fb2a3a3ca492166734e3d|\n",
      "|Paul |28 |95513c00d2aaf79604610b4af5eb3a6e|\n",
      "+-----+---+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "spark_df1 = spark.createDataFrame([\n",
    "            ('John', 32),\n",
    "            ('David', 22),\n",
    "            ('Raj', 10),\n",
    "            ('Andy', 25)],\n",
    "            [\"name\", \"age\"])\n",
    "\n",
    "spark_df1.registerTempTable(\"test_df\")\n",
    "\n",
    "\n",
    "# generate checksum column SHA-2 family of hash functions 256 bits\n",
    "from pyspark.sql.functions import sha2, concat_ws, md5\n",
    "spark_df1.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *spark_df1.columns), 256)).show(truncate=False)\n",
    "history_data_df = spark_df1.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *spark_df1.columns), 256))\n",
    "# changed dataframe \n",
    "spark_df2 = spark.createDataFrame([\n",
    "            ('John', 33),\n",
    "            ('David', 22),\n",
    "            ('Raj', 11),\n",
    "#            ('Andy', 25),\n",
    "            ('Paul', 28)],\n",
    "            [\"name\", \"age\"])\n",
    "\n",
    "\n",
    "col_list_to_hash = ['name','age']\n",
    "# encoding based on column list\n",
    "spark_df2.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *col_list_to_hash), 256)).show(truncate=False)\n",
    "delta_df = spark_df2.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *col_list_to_hash), 256))\n",
    "# encoding based on all columns\n",
    "spark_df2.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *spark_df1.columns), 256)).show(truncate=False)\n",
    "\n",
    "# md5 encoding\n",
    "spark_df2.withColumn(\"row_md5\", md5(concat_ws(\"||\", *col_list_to_hash))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 32|\n",
      "|  Raj| 10|\n",
      "|David| 22|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| Andy| 25|\n",
      "| John| 33|\n",
      "|David| 22|\n",
      "|  Raj| 11|\n",
      "| Paul| 28|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark_df1 is history and spark_df2 delta based on keyCols\n",
    "keyCols = ['name']\n",
    "New_updated_records_replaceDf = spark_df1.alias('a').join(spark_df2.alias('b'), on=keyCols, how='inner').select('a.*')\n",
    "New_updated_records_replaceDf.show()\n",
    "resultDf = spark_df1.subtract(New_updated_records_replaceDf).union(spark_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+\n",
      "| name|age|            row_sha2|\n",
      "+-----+---+--------------------+\n",
      "|David| 22|eb17e164bc6dd6c05...|\n",
      "+-----+---+--------------------+\n",
      "\n",
      "+-----+---+--------------------+\n",
      "| name|age|            row_sha2|\n",
      "+-----+---+--------------------+\n",
      "| Andy| 25|737fc02c033351980...|\n",
      "| John| 32|afabe3fe5711744f1...|\n",
      "|  Raj| 10|c37288dc8fe43e4f3...|\n",
      "| John| 33|d486ab8a3b8333511...|\n",
      "|David| 22|eb17e164bc6dd6c05...|\n",
      "|  Raj| 11|d0f22ed3e129a7b75...|\n",
      "| Paul| 28|b71b0817d3973d4b1...|\n",
      "+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark_df1 is history and spark_df2 delta based on keyCols\n",
    "keyCols = ['name','age']\n",
    "New_updated_records_replaceDf = history_data_df.alias('a').join(delta_df.alias('b'), on=keyCols, how='inner').select('a.*')\n",
    "New_updated_records_replaceDf.show()\n",
    "resultDf = history_data_df.subtract(New_updated_records_replaceDf).union(delta_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+\n",
      "|name|age|            row_sha2|\n",
      "+----+---+--------------------+\n",
      "|Andy| 25|737fc02c033351980...|\n",
      "|John| 32|afabe3fe5711744f1...|\n",
      "| Raj| 10|c37288dc8fe43e4f3...|\n",
      "+----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_entries_to_be_deleted = history_data_df.subtract(delta_df)\n",
    "old_entries_to_be_deleted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+\n",
      "|name|age|            row_sha2|\n",
      "+----+---+--------------------+\n",
      "| Raj| 11|d0f22ed3e129a7b75...|\n",
      "|John| 33|d486ab8a3b8333511...|\n",
      "|Paul| 28|b71b0817d3973d4b1...|\n",
      "+----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_entries_to_be_appened = delta_df.subtract(history_data_df)\n",
    "new_entries_to_be_appened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+\n",
      "|name|age|            row_sha2|\n",
      "+----+---+--------------------+\n",
      "|Paul| 28|b71b0817d3973d4b1...|\n",
      "+----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new keys, so these will be inserted\n",
    "keyCols = ['name']\n",
    "new_entries_to_be_inserted = delta_df[keyCols].subtract(history_data_df[keyCols])\n",
    "new_entries_to_be_inserted.join(delta_df, on=keyCols, how='left').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+---+--------------------+\n",
      "| name|age|            row_sha2|age|            row_sha2|\n",
      "+-----+---+--------------------+---+--------------------+\n",
      "| John| 33|d486ab8a3b8333511...| 32|afabe3fe5711744f1...|\n",
      "|  Raj| 11|d0f22ed3e129a7b75...| 10|c37288dc8fe43e4f3...|\n",
      "|David| 22|eb17e164bc6dd6c05...| 22|eb17e164bc6dd6c05...|\n",
      "+-----+---+--------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# existing keys\n",
    "keyCols = ['name']\n",
    "#changed_entries_to_be_updated = delta_df[keyCols].alias('a').join(history_data_df[keyCols], on=keyCols, how='inner').select('a.*')\n",
    "changed_entries_to_be_updated = delta_df.alias('a').join(history_data_df, on=keyCols, how='inner')\n",
    "changed_entries_to_be_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+---+--------------------+\n",
      "|name|age|            row_sha2|age|            row_sha2|\n",
      "+----+---+--------------------+---+--------------------+\n",
      "|John| 33|d486ab8a3b8333511...| 32|afabe3fe5711744f1...|\n",
      "| Raj| 11|d0f22ed3e129a7b75...| 10|c37288dc8fe43e4f3...|\n",
      "+----+---+--------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# existing keys where records are changed comparing based on hash key\n",
    "delta_df.alias('a').join(history_data_df, on=keyCols, how='inner').filter(history_data_df.row_sha2 != delta_df.row_sha2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulating an UPSERT between 2 datasets using pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing data\n",
    "current_data_df1 = spark.createDataFrame([('1','John','30','1000','01/08/2019  12:00:00'),\n",
    "('2','Peter','35','1500','02/08/2019  12:00:00'),\n",
    "('3','Gabe','21','800','03/08/2019  12:00:00'),\n",
    "('4','Oscar','29','2000','04/08/2019  12:00:00'),\n",
    "('5','Anna','20','1200','05/08/2019  12:00:00')],\n",
    "                             ['id','name','age','salary','last_modified_date'])\n",
    "\n",
    "increamental_data_df1 = spark.createDataFrame([('1','John','43','3000','12/08/2019  12:00:00'),\n",
    "('2','Peter','35','3500','12/08/2019  12:00:00'),\n",
    "('3','Gabe','21','800','12/08/2019  12:00:00'),\n",
    "('6','Patricia','40','2500','12/08/2019  12:00:00')],\n",
    "                             ['id','name','age','salary','last_modified_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+----------------------+-------------------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+\n",
      "|  1| John| 30|  1000|01/08/2019  12:00:00|   2019-08-01 12:00:00|2019-08-01 12:00:00|\n",
      "|  2|Peter| 35|  1500|02/08/2019  12:00:00|   2019-08-02 12:00:00|2019-08-02 12:00:00|\n",
      "|  3| Gabe| 21|   800|03/08/2019  12:00:00|   2019-08-03 12:00:00|2019-08-03 12:00:00|\n",
      "|  4|Oscar| 29|  2000|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|\n",
      "|  5| Anna| 20|  1200|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+\n",
      "\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "| id|    name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "|  1|    John| 43|  3000|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|\n",
      "|  2|   Peter| 35|  3500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|\n",
      "|  3|    Gabe| 21|   800|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|\n",
      "|  6|Patricia| 40|  2500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert string format to datetime format\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import expr, col\n",
    "# faster if else condition in pyspark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "current_data_df1 = current_data_df1.withColumn(\"new_last_modified_date\",expr(\"to_timestamp(last_modified_date, 'dd/MM/yyyy  HH:mm:ss')\"))\n",
    "increamental_data_df1 = increamental_data_df1.withColumn(\"new_last_modified_date\",expr(\"to_timestamp(last_modified_date, 'dd/MM/yyyy  HH:mm:ss')\"))\n",
    "current_data_df1 = current_data_df1.withColumn(\"record_load_time\",F.col('new_last_modified_date'))\n",
    "increamental_data_df1 = increamental_data_df1.withColumn(\"record_load_time\",F.col('new_last_modified_date'))\n",
    "\n",
    "current_data_df1.show()\n",
    "increamental_data_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+-------------------+\n",
      "| id| name|age|salary|  last_modified_date|     NewInvoiceDate|\n",
      "+---+-----+---+------+--------------------+-------------------+\n",
      "|  1| John| 30|  1000| 1/08/2019  12:00:00|2019-08-01 12:00:00|\n",
      "|  2|Peter| 35|  1500| 2/08/2019  12:00:00|2019-08-02 12:00:00|\n",
      "|  3| Gabe| 21|   800| 3/08/2019  12:00:00|2019-08-03 12:00:00|\n",
      "|  4|Oscar| 29|  2000|14/08/2019  12:00:00|2019-08-14 12:00:00|\n",
      "|  5| Anna| 20|  1200| 5/08/2019  12:00:00|2019-08-05 12:00:00|\n",
      "+---+-----+---+------+--------------------+-------------------+\n",
      "\n",
      "Before type changes \n",
      " [('id', 'string'), ('name', 'string'), ('age', 'string'), ('salary', 'string'), ('last_modified_date', 'string'), ('NewInvoiceDate', 'timestamp')]\n",
      "After type changes \n",
      " [('id', 'int'), ('name', 'string'), ('age', 'int'), ('salary', 'double'), ('last_modified_date', 'string'), ('NewInvoiceDate', 'timestamp')]\n"
     ]
    }
   ],
   "source": [
    "# one more way convert string into date field\n",
    "from pyspark.sql.functions import to_utc_timestamp, unix_timestamp, lit, datediff, col\n",
    "df_test1 = spark.createDataFrame([('1','John','30','1000','1/08/2019  12:00:00'),\n",
    "('2','Peter','35','1500','2/08/2019  12:00:00'),\n",
    "('3','Gabe','21','800','3/08/2019  12:00:00'),\n",
    "('4','Oscar','29','2000','14/08/2019  12:00:00'),\n",
    "('5','Anna','20','1200','5/08/2019  12:00:00')],\n",
    "                             ['id','name','age','salary','last_modified_date'])\n",
    "\n",
    "timeFmt = 'dd/MM/yyyy  HH:mm:ss'\n",
    "df_test1 = df_test1.withColumn('NewInvoiceDate'\n",
    "                 , to_utc_timestamp(unix_timestamp(col('last_modified_date'),timeFmt).cast('timestamp')\n",
    "                 , 'UTC'))\n",
    "df_test1.show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "print('Before type changes \\n', df_test1.dtypes)\n",
    "# cast different columns data types to integer, double, string\n",
    "for c in ['id','age']:\n",
    "    df_test1=df_test1.withColumn(c, df_test1[c].cast('integer'))\n",
    "for c in ['salary']:\n",
    "    df_test1=df_test1.withColumn(c, df_test1[c].cast('double'))\n",
    "for c in ['name']:\n",
    "    df_test1=df_test1.withColumn(c, df_test1[c].cast('string'))\n",
    "\n",
    "print('After type changes \\n',df_test1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "|id |name    |age|salary|last_modified_date  |new_last_modified_date|record_load_time   |\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "|1  |John    |30 |1000  |01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-08-01 12:00:00|\n",
      "|2  |Peter   |35 |1500  |02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-08-02 12:00:00|\n",
      "|3  |Gabe    |21 |800   |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-08-03 12:00:00|\n",
      "|4  |Oscar   |29 |2000  |04/08/2019  12:00:00|2019-08-04 12:00:00   |2019-08-04 12:00:00|\n",
      "|5  |Anna    |20 |1200  |05/08/2019  12:00:00|2019-08-05 12:00:00   |2019-08-05 12:00:00|\n",
      "|1  |John    |43 |3000  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|\n",
      "|2  |Peter   |35 |3500  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|\n",
      "|3  |Gabe    |21 |800   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|\n",
      "|6  |Patricia|40 |2500  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Both DataFrames are grouped together with union (which is equivalent to UNION ALL in SQL), creating the 3rd and final DataFrame. \n",
    "df_upsert = current_data_df1.union(increamental_data_df1)\n",
    "df_upsert.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import desc, asc\n",
    "# Set primary keys of dataframe\n",
    "# It could also be a composite primary key, hence the list (e.g ['id1','id2','id3'])\n",
    "#primary_keys = ['id','name','age']\n",
    "primary_keys = ['id']\n",
    "\n",
    "# Partition dataset by primary key and order by the date field\n",
    "# This step will group duplicated IDs together, putting the latest (date) on top\n",
    "w = Window.partitionBy(*primary_keys).orderBy(desc('new_last_modified_date'),asc('id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+\n",
      "|id |name    |age|salary|last_modified_date  |new_last_modified_date|record_load_time   |_row_number|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+\n",
      "|1  |John    |43 |3000  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|1          |\n",
      "|1  |John    |30 |1000  |01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-08-01 12:00:00|2          |\n",
      "|2  |Peter   |35 |3500  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|1          |\n",
      "|2  |Peter   |35 |1500  |02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-08-02 12:00:00|2          |\n",
      "|3  |Gabe    |21 |800   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|1          |\n",
      "|3  |Gabe    |21 |800   |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-08-03 12:00:00|2          |\n",
      "|4  |Oscar   |29 |2000  |04/08/2019  12:00:00|2019-08-04 12:00:00   |2019-08-04 12:00:00|1          |\n",
      "|5  |Anna    |20 |1200  |05/08/2019  12:00:00|2019-08-05 12:00:00   |2019-08-05 12:00:00|1          |\n",
      "|6  |Patricia|40 |2500  |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-08-12 12:00:00|1          |\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now, a temporary column ‘_row_number’ is created using the window created above. Note how the DataFrame looks like, with the primary key ‘id’ grouped together and ordered by ‘last_modified_date’ (descending):\n",
    "\n",
    "df_upsert = df_upsert.withColumn(\"_row_number\", row_number().over(w))\n",
    "\n",
    "df_upsert.filter(\"id <= 10\").sort(col(\"id\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+--------------------+\n",
      "| id|    name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|_row_number|         record_type|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+--------------------+\n",
      "|  1|    John| 43|  3000|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|          1|#keep this new up...|\n",
      "|  1|    John| 30|  1000|01/08/2019  12:00:00|   2019-08-01 12:00:00|2019-08-01 12:00:00|          2| #remove this record|\n",
      "|  2|   Peter| 35|  3500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|          1|#keep this new up...|\n",
      "|  2|   Peter| 35|  1500|02/08/2019  12:00:00|   2019-08-02 12:00:00|2019-08-02 12:00:00|          2| #remove this record|\n",
      "|  3|    Gabe| 21|   800|03/08/2019  12:00:00|   2019-08-03 12:00:00|2019-08-03 12:00:00|          2| #remove this record|\n",
      "|  3|    Gabe| 21|   800|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|          1|#keep this new up...|\n",
      "|  4|   Oscar| 29|  2000|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|          1|#keep this new up...|\n",
      "|  5|    Anna| 20|  1200|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|          1|#keep this new up...|\n",
      "|  6|Patricia| 40|  2500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|          1|#keep this new up...|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "recordTypeDict = {1:'#keep this new updated record',2:'#remove this record'}\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*recordTypeDict.items())])\n",
    "\n",
    "df_upsert = df_upsert.withColumn('record_type', mapping_expr[df_upsert['_row_number']])\n",
    "df_upsert.filter(\"id <= 10\").sort(col(\"id\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+--------------------+----------------------+-------------------+--------------------+\n",
      "| id|    name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|         record_type|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+--------------------+\n",
      "|  1|    John| 43|  3000|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|#keep this new up...|\n",
      "|  2|   Peter| 35|  3500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|#keep this new up...|\n",
      "|  3|    Gabe| 21|   800|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|#keep this new up...|\n",
      "|  4|   Oscar| 29|  2000|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|#keep this new up...|\n",
      "|  5|    Anna| 20|  1200|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|#keep this new up...|\n",
      "|  6|Patricia| 40|  2500|12/08/2019  12:00:00|   2019-08-12 12:00:00|2019-08-12 12:00:00|#keep this new up...|\n",
      "+---+--------+---+------+--------------------+----------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these delta records will be appened to current target table\n",
    "df_to_append = df_upsert.where(df_upsert._row_number == 1).drop(\"_row_number\")\n",
    "df_to_append.orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+----------------------+-------------------+-------------------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|        record_type|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+-------------------+\n",
      "|  1| John| 30|  1000|01/08/2019  12:00:00|   2019-08-01 12:00:00|2019-08-01 12:00:00|#remove this record|\n",
      "|  2|Peter| 35|  1500|02/08/2019  12:00:00|   2019-08-02 12:00:00|2019-08-02 12:00:00|#remove this record|\n",
      "|  3| Gabe| 21|   800|03/08/2019  12:00:00|   2019-08-03 12:00:00|2019-08-03 12:00:00|#remove this record|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these records will be reversed to delete from current target table\n",
    "df_to_reverse = df_upsert.where(df_upsert._row_number > 1).drop(\"_row_number\")\n",
    "df_to_reverse.orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+----------------------+-------------------+-------------------+-----+-------+\n",
      "| id| name|  last_modified_date|new_last_modified_date|   record_load_time|        record_type|  age| salary|\n",
      "+---+-----+--------------------+----------------------+-------------------+-------------------+-----+-------+\n",
      "|  3| Gabe|03/08/2019  12:00:00|   2019-08-03 12:00:00|2019-08-03 12:00:00|#remove this record|-21.0| -800.0|\n",
      "|  1| John|01/08/2019  12:00:00|   2019-08-01 12:00:00|2019-08-01 12:00:00|#remove this record|-30.0|-1000.0|\n",
      "|  2|Peter|02/08/2019  12:00:00|   2019-08-02 12:00:00|2019-08-02 12:00:00|#remove this record|-35.0|-1500.0|\n",
      "+---+-----+--------------------+----------------------+-------------------+-------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# key figures need to be reverse\n",
    "keyfigures = ['age', 'salary']\n",
    "# column list except key figures\n",
    "column_minus_keyfigures_list = [c for c in df_to_reverse.columns if c not in keyfigures]\n",
    "\n",
    "df_to_reverse_final = df_to_reverse.select( *column_minus_keyfigures_list , *[(F.col(col_name)* -1).alias(col_name) for col_name in keyfigures])\n",
    "df_to_reverse_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|salary|\n",
      "+---+------+\n",
      "| 21|   800|\n",
      "| 30|  1000|\n",
      "| 35|  1500|\n",
      "+---+------+\n",
      "\n",
      "+---+-----+--------------------+----------------------+-------------------+-------------------+\n",
      "| id| name|  last_modified_date|new_last_modified_date|   record_load_time|        record_type|\n",
      "+---+-----+--------------------+----------------------+-------------------+-------------------+\n",
      "|  3| Gabe|03/08/2019  12:00:00|   2019-08-03 12:00:00|2019-08-03 12:00:00|#remove this record|\n",
      "|  1| John|01/08/2019  12:00:00|   2019-08-01 12:00:00|2019-08-01 12:00:00|#remove this record|\n",
      "|  2|Peter|02/08/2019  12:00:00|   2019-08-02 12:00:00|2019-08-02 12:00:00|#remove this record|\n",
      "+---+-----+--------------------+----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with column list keyfigures example \n",
    "df_to_reverse.select([c for c in df_to_reverse.columns if c in keyfigures]).show()\n",
    "\n",
    "# without column list keyfigures example \n",
    "df_to_reverse.select([c for c in df_to_reverse.columns if c not in keyfigures]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+----------------------+-------------------+-----------------------------+-----+-------+\n",
      "|id |name    |last_modified_date  |new_last_modified_date|record_load_time   |record_type                  |age  |salary |\n",
      "+---+--------+--------------------+----------------------+-------------------+-----------------------------+-----+-------+\n",
      "|3  |Gabe    |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-11-29 09:57:54|#remove this record          |-21.0|-800.0 |\n",
      "|1  |John    |01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-11-29 09:57:54|#remove this record          |-30.0|-1000.0|\n",
      "|2  |Peter   |02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-11-29 09:57:54|#remove this record          |-35.0|-1500.0|\n",
      "|3  |Gabe    |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|21   |800    |\n",
      "|5  |Anna    |05/08/2019  12:00:00|2019-08-05 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|20   |1200   |\n",
      "|6  |Patricia|12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|40   |2500   |\n",
      "|1  |John    |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|43   |3000   |\n",
      "|4  |Oscar   |04/08/2019  12:00:00|2019-08-04 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|29   |2000   |\n",
      "|2  |Peter   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|#keep this new updated record|35   |3500   |\n",
      "+---+--------+--------------------+----------------------+-------------------+-----------------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Both DataFrames are grouped together with union (which is equivalent to UNION ALL in SQL), creating the 3rd and final DataFrame with update timestamp. \n",
    "# select same column list with same order to union them \n",
    "import time\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit,unix_timestamp\n",
    "reverse_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_to_reverse_final = df_to_reverse_final.withColumn('record_load_time',unix_timestamp(lit(reverse_timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "\n",
    "append_timestamp = datetime.datetime.fromtimestamp(time.time() + 1).strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_to_append = df_to_append.withColumn('record_load_time',unix_timestamp(lit(append_timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "\n",
    "final_update_df = df_to_reverse_final.union(df_to_append.select(df_to_reverse_final.columns))\n",
    "final_update_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'age',\n",
       " 'salary',\n",
       " 'last_modified_date',\n",
       " 'new_last_modified_date',\n",
       " 'record_load_time']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-------+--------------------+----------------------+-------------------+\n",
      "|id |name    |age  |salary |last_modified_date  |new_last_modified_date|record_load_time   |\n",
      "+---+--------+-----+-------+--------------------+----------------------+-------------------+\n",
      "|1  |John    |30   |1000   |01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-08-01 12:00:00|\n",
      "|2  |Peter   |35   |1500   |02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-08-02 12:00:00|\n",
      "|3  |Gabe    |21   |800    |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-08-03 12:00:00|\n",
      "|4  |Oscar   |29   |2000   |04/08/2019  12:00:00|2019-08-04 12:00:00   |2019-08-04 12:00:00|\n",
      "|5  |Anna    |20   |1200   |05/08/2019  12:00:00|2019-08-05 12:00:00   |2019-08-05 12:00:00|\n",
      "|3  |Gabe    |-21.0|-800.0 |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-11-29 09:57:54|\n",
      "|1  |John    |-30.0|-1000.0|01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-11-29 09:57:54|\n",
      "|2  |Peter   |-35.0|-1500.0|02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-11-29 09:57:54|\n",
      "|3  |Gabe    |21   |800    |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|5  |Anna    |20   |1200   |05/08/2019  12:00:00|2019-08-05 12:00:00   |2019-11-29 09:57:55|\n",
      "|6  |Patricia|40   |2500   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|1  |John    |43   |3000   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|4  |Oscar   |29   |2000   |04/08/2019  12:00:00|2019-08-04 12:00:00   |2019-11-29 09:57:55|\n",
      "|2  |Peter   |35   |3500   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "+---+--------+-----+-------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finally merge delta with target current table\n",
    "final_current_df = current_data_df1.union(final_update_df.select(current_data_df1.columns))\n",
    "final_current_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "|id |name |age  |salary |last_modified_date  |new_last_modified_date|record_load_time   |\n",
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "|1  |John |43   |3000   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|1  |John |-30.0|-1000.0|01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-11-29 09:57:54|\n",
      "|1  |John |30   |1000   |01/08/2019  12:00:00|2019-08-01 12:00:00   |2019-08-01 12:00:00|\n",
      "|2  |Peter|35   |3500   |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|2  |Peter|-35.0|-1500.0|02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-11-29 09:57:54|\n",
      "|2  |Peter|35   |1500   |02/08/2019  12:00:00|2019-08-02 12:00:00   |2019-08-02 12:00:00|\n",
      "|3  |Gabe |21   |800    |12/08/2019  12:00:00|2019-08-12 12:00:00   |2019-11-29 09:57:55|\n",
      "|3  |Gabe |-21.0|-800.0 |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-11-29 09:57:54|\n",
      "|3  |Gabe |21   |800    |03/08/2019  12:00:00|2019-08-03 12:00:00   |2019-08-03 12:00:00|\n",
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final_current_df.filter(\"id <= 3\").sort(col(\"id\").asc()).show(truncate=False)\n",
    "final_current_df.filter(\"id <= 3\").sort(asc(\"id\"), desc(\"record_load_time\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('age', 'string'),\n",
       " ('salary', 'string'),\n",
       " ('last_modified_date', 'string'),\n",
       " ('new_last_modified_date', 'timestamp'),\n",
       " ('record_load_time', 'timestamp')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_current_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|new_id|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "|  4|Oscar| 29|  2000|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|  null|\n",
      "|  5| Anna| 20|  1200|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|  null|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify the record to be delete between previous dataframe and new dataframe, use () with & operator to add multiple columns join condition\n",
    "increamental_data_df2 = increamental_data_df1.withColumnRenamed(\"id\", \"new_id\")\n",
    "record_to_delete_df = current_data_df1.join(increamental_data_df2, (current_data_df1.id == increamental_data_df2.new_id) & (current_data_df1.name == increamental_data_df2.name),how='left').filter(col('new_id').isNull()).select(current_data_df1[\"*\"],increamental_data_df2[\"new_id\"])\n",
    "record_to_delete_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "| id| name|  age| salary|  last_modified_date|new_last_modified_date|   record_load_time|\n",
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "|  4|Oscar|-29.0|-2000.0|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|\n",
      "|  5| Anna|-20.0|-1200.0|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|\n",
      "+---+-----+-----+-------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if records need to be deleted\n",
    "import pyspark.sql.functions as F\n",
    "# key figures need to be reverse\n",
    "keyfigures = ['age', 'salary']\n",
    "# column list except key figures\n",
    "column_minus_keyfigures_list = [c for c in record_to_delete_df.columns if c not in keyfigures]\n",
    "\n",
    "df_to_delete = record_to_delete_df.select( *column_minus_keyfigures_list , *[(col(col_name)* -1).alias(col_name) for col_name in keyfigures])\n",
    "df_to_delete = df_to_delete.drop('new_id')\n",
    "# reorder columns as per target table\n",
    "df_to_delete.select([c for c in current_data_df1.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date|   record_load_time|new_id|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "|  4|Oscar| 29|  2000|04/08/2019  12:00:00|   2019-08-04 12:00:00|2019-08-04 12:00:00|  null|\n",
      "|  5| Anna| 20|  1200|05/08/2019  12:00:00|   2019-08-05 12:00:00|2019-08-05 12:00:00|  null|\n",
      "+---+-----+---+------+--------------------+----------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify the record to be delete between previous dataframe and new dataframe based on multiple columns\n",
    "df = current_data_df1.join(increamental_data_df2, (current_data_df1.id == increamental_data_df2.new_id) & (current_data_df1.name == increamental_data_df2.name ),'left').filter(col('new_id').isNull()).select(current_data_df1[\"*\"],increamental_data_df2[\"new_id\"])\n",
    "col_list=[\"id\",\"name\",\"age\"]\n",
    "col_list=[c for c in df.columns]\n",
    "df.select(col_list).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+---+\n",
      "| id| v1| v2| v3|  v4|id2|\n",
      "+---+---+---+---+----+---+\n",
      "|  0|1.0|3.0|4.0| 3.0|  0|\n",
      "|  2|2.0|5.0|7.0|10.0|  4|\n",
      "+---+---+---+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A Transformer pipeline using SQLTransformer objects (or any other Transformer) is another approach which makes chaining easy.\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 1.0, 3.0),\n",
    "    (2, 2.0, 5.0)\n",
    "], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "\n",
    "sqlSelectExpr = SQLTransformer(statement=\"SELECT *, (id * 2) AS id2 FROM __THIS__\")\n",
    "\n",
    "pipeline = Pipeline(stages=[sqlTrans, sqlSelectExpr])\n",
    "pipelineModel = pipeline.fit(df)\n",
    "pipelineModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- v1: double (nullable = true)\n",
      " |-- v2: double (nullable = true)\n",
      "\n",
      "+---------+---------+---------+\n",
      "|prefix_id|prefix_v1|prefix_v2|\n",
      "+---------+---------+---------+\n",
      "|        0|      1.0|      3.0|\n",
      "|        2|      2.0|      5.0|\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prefix all columns of dataframe\n",
    "df.printSchema()\n",
    "def prefix_dataframe_columns(datafame , prefix):\n",
    "    for col_name in datafame.columns:\n",
    "        new_c = prefix + col_name \n",
    "        datafame = datafame.withColumnRenamed(col_name, new_c)\n",
    "    return datafame\n",
    "    \n",
    "df1 = prefix_dataframe_columns(df,'prefix_')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------+\n",
      "| name|age|         maturity|\n",
      "+-----+---+-----------------+\n",
      "| John|  1|         Under 12|\n",
      "|David| 15|Between 13 and 19|\n",
      "|  Raj| 31|Between 19 and 65|\n",
      "+-----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use if else to derive the new column as UDF\n",
    "def return_age_bracket(age):\n",
    "    if (age <= 12):\n",
    "        return 'Under 12'\n",
    "    elif (age >= 13 and age <= 19):\n",
    "        return 'Between 13 and 19'\n",
    "    elif (age > 19 and age < 65):\n",
    "        return 'Between 19 and 65'\n",
    "    elif (age >= 65):\n",
    "        return 'Over 65'\n",
    "    else: return 'N/A'\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "maturity_udf = udf(return_age_bracket)\n",
    "df = spark.createDataFrame([\n",
    "    ('John', 1),\n",
    "    ('David', 15),\n",
    "    ('Raj', 31)\n",
    "], [\"name\", \"age\"])\n",
    "df = df.withColumn(\"maturity\", maturity_udf(df.age))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| name|Age|age_bracket|\n",
      "+-----+---+-----------+\n",
      "| John|  1|     Infant|\n",
      "|David| 15| Adolescent|\n",
      "|  Raj| 31|      Adult|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# faster if else condition in pyspark\n",
    "from pyspark.sql import functions as F\n",
    "input_df = spark.createDataFrame([\n",
    "    ('John', 1),\n",
    "    ('David', 15),\n",
    "    ('Raj', 31)\n",
    "], [\"name\", \"Age\"])\n",
    "input_df = input_df.withColumn(\"age_bracket\", F.when(input_df.Age <= 12, 'Infant').when(( (input_df.Age >= 13) & (input_df.Age <= 19)), 'Adolescent').when(( (input_df.Age >= 19) & (input_df.Age < 65)), 'Adult').when(input_df.Age >= 65, 'Retired').otherwise('N/A'))\n",
    "input_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+--------------------+---------------------------+----------------------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date_time|new_last_modified_date|\n",
      "+---+-----+---+------+--------------------+---------------------------+----------------------+\n",
      "|  1| John| 30|  1000|01/08/2019  12:00...|        2019-08-01 12:00:00|            2019-08-01|\n",
      "|  2|Peter| 35|  1500|02/08/2019  12:00...|        2019-08-02 12:00:00|            2019-08-02|\n",
      "|  3| Gabe| 21|   800|03/08/2019  12:00...|                       null|                  null|\n",
      "|  4|Oscar| 29|  2000|04/08/2019  12:00...|        2019-08-04 12:00:40|            2019-08-04|\n",
      "|  5| Anna| 20|  1200|invalid date/08/2...|                       null|                  null|\n",
      "+---+-----+---+------+--------------------+---------------------------+----------------------+\n",
      "\n",
      "[('id', 'int'), ('name', 'string'), ('age', 'int'), ('salary', 'int'), ('last_modified_date', 'string'), ('new_last_modified_date_time', 'timestamp'), ('new_last_modified_date', 'date')]\n"
     ]
    }
   ],
   "source": [
    "# date timestamp management\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "# convert string format to datetime format\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "file_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), False),\n",
    "  StructField(\"name\", StringType(), False),\n",
    "  StructField(\"age\", IntegerType(), False),\n",
    "  StructField(\"salary\", IntegerType(), False),\n",
    "  StructField(\"last_modified_date\", StringType(), False),\n",
    "])\n",
    "\n",
    "# existing data\n",
    "test1 = spark.createDataFrame([(1, 'John',30,1000,'01/08/2019  12:00:00 AM'),\n",
    "(2,'Peter',35,1500,'02/08/2019  12:00:00 AM'),\n",
    "(3, 'Gabe',21,800,'03/08/2019  12:00:90 AM'),\n",
    "(4,'Oscar',29,2000,'04/08/2019  12:00:40 AM'),\n",
    "(5, 'Anna',20,1200,'invalid date/08/2019  12:00:00 AM')], file_schema)\n",
    "\n",
    "test1 = test1.withColumn(\"new_last_modified_date_time\",expr(\"to_timestamp(last_modified_date, 'dd/MM/yyyy  HH:mm:ss')\"))\n",
    "test1 = test1.withColumn(\"new_last_modified_date\",expr(\"to_date(last_modified_date, 'dd/MM/yyyy  HH:mm:ss')\"))\n",
    "\n",
    "test1.show()\n",
    "print(test1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- salary: integer (nullable = false)\n",
      " |-- last_modified_date: string (nullable = false)\n",
      " |-- new_last_modified_date_time: timestamp (nullable = true)\n",
      " |-- new_last_modified_date: date (nullable = true)\n",
      " |-- Order_date_new: date (nullable = true)\n",
      " |-- Order_date_new2: date (nullable = true)\n",
      "\n",
      "+---+-----+---+------+--------------------+---------------------------+----------------------+--------------+---------------+\n",
      "| id| name|age|salary|  last_modified_date|new_last_modified_date_time|new_last_modified_date|Order_date_new|Order_date_new2|\n",
      "+---+-----+---+------+--------------------+---------------------------+----------------------+--------------+---------------+\n",
      "|  1| John| 30|  1000|01/08/2019  12:00...|        2019-08-01 12:00:00|            2019-08-01|    2019-08-01|     2019-08-01|\n",
      "|  2|Peter| 35|  1500|02/08/2019  12:00...|        2019-08-02 12:00:00|            2019-08-02|    2019-08-02|     2019-08-02|\n",
      "|  3| Gabe| 21|   800|03/08/2019  12:00...|                       null|                  null|    2019-08-03|     2019-08-03|\n",
      "|  4|Oscar| 29|  2000|04/08/2019  12:00...|        2019-08-04 12:00:40|            2019-08-04|    2019-08-04|     2019-08-04|\n",
      "|  5| Anna| 20|  1200|invalid date/08/2...|                       null|                  null|          null|           null|\n",
      "+---+-----+---+------+--------------------+---------------------------+----------------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert string of dd/MM/yyyy into date column by splitting string when invalid date need to be processed\n",
    "from pyspark.sql.functions import expr\n",
    "rfm_df1 = test1.withColumn(\"Order_date_new\",expr(\"to_date(last_modified_date, 'dd/MM/yyyy')\"))\n",
    "rfm_df1 = rfm_df1.withColumn(\"Order_date_new2\",expr(\"to_date(concat(substring(last_modified_date,7,4),substring(last_modified_date,4,2),substring(last_modified_date,1,2)), 'yyyyMMdd')\"))\n",
    "rfm_df1.printSchema()\n",
    "rfm_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
