{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"671bf7f4-896d-4d32-92bc-7b2b6351e479"}}},{"cell_type":"markdown","source":["# Cleaning Data\n\nMost transformations completed with Spark SQL will be familiar to SQL-savvy developers.\n\nAs we inspect and clean our data, we'll need to construct various column expressions and queries to express transformations to apply on our dataset.  \n\nColumn expressions are constructed from existing columns, operators, and built-in Spark SQL functions. They can be used in **`SELECT`** statements to express transformations that create new columns from datasets. \n\nAlong with **`SELECT`**, many additional query commands can be used to express transformations in Spark SQL, including **`WHERE`**, **`DISTINCT`**, **`ORDER BY`**, **`GROUP BY`**, etc.\n\nIn this notebook, we'll review a few concepts that might differ from other systems you're used to, as well as calling out a few useful functions for common operations.\n\nWe'll pay special attention to behaviors around **`NULL`** values, as well as formatting strings and datetime fields.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Summarize datasets and describe null behaviors\n- Retrieve and removing duplicates\n- Validate datasets for expected counts, missing values, and duplicate records\n- Apply common transformations to clean and transform data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cb7f1b0-5a62-42cd-ace7-4a6a540a53e6"}}},{"cell_type":"markdown","source":["## Run Setup\n\nThe setup script will create the data and declare necessary values for the rest of this notebook to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa57be2b-2417-4307-b020-bd81472f41b3"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-04.6"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"344fa094-4198-46ab-b8aa-fb120df2eda9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll work with new users records in **`users_dirty`** table for this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"362ec069-08ec-49f5-9964-b5ee5b927e42"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f732cef7-14ef-4163-b7e2-679251ee9709"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Inspect Data\n\nLet's start by counting values in each field of our data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ce49fa5-c834-4056-9443-1b1d9e391922"}}},{"cell_type":"code","source":["%sql\nSELECT count(user_id), count(user_first_touch_timestamp), count(email), count(updated), count(*)\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15632271-ea44-4ea7-84c3-08dfdb1db291"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that **`count(col)`** skips **`NULL`** values when counting specific columns or expressions.\n\nHowever, **`count(*)`** is a special case that counts the total number of rows (including rows that are only **`NULL`** values).\n\nTo count null values, use the **`count_if`** function or **`WHERE`** clause to provide a condition that filters for records where the value **`IS NULL`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c411cb74-ab49-46ee-a627-b2033b829e7a"}}},{"cell_type":"code","source":["%sql\nSELECT\n  count_if(user_id IS NULL) AS missing_user_ids, \n  count_if(user_first_touch_timestamp IS NULL) AS missing_timestamps, \n  count_if(email IS NULL) AS missing_emails,\n  count_if(updated IS NULL) AS missing_updates\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1544843-3451-4f1f-8ddb-4438024f2f87"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Clearly there are at least a handful of null values in all of our fields. Let's try to discover what is causing this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bc864b4-9269-4141-a289-8c0689ceef0a"}}},{"cell_type":"markdown","source":["## Distinct Records\n\nStart by looking for distinct rows."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8275a3c-d4da-41c1-b22f-1ca584826cbe"}}},{"cell_type":"code","source":["%sql\nSELECT count(DISTINCT(*))\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0709bca3-81eb-4a59-bf16-0e36c18c326f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT count(DISTINCT(user_id))\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"012e7f01-5141-4f92-b71c-ad62e047201d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because **`user_id`** is generated alongside the **`user_first_touch_timestamp`**, these fields should always be in parity for counts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34b11665-5f78-4243-a3fd-e02ad6dd4265"}}},{"cell_type":"code","source":["%sql\nSELECT count(DISTINCT(user_first_touch_timestamp))\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79dfc89e-56f3-45fd-9530-979a79f6859a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here we note that while there are some duplicate records relative to our total row count, we have a much higher number of distinct values.\n\nLet's go ahead and combine our distinct counts with columnar counts to see these values side-by-side."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59e7662b-bafb-423a-a47f-2e1b104ab844"}}},{"cell_type":"code","source":["%sql\nSELECT \n  count(user_id) AS total_ids,\n  count(DISTINCT user_id) AS unique_ids,\n  count(email) AS total_emails,\n  count(DISTINCT email) AS unique_emails,\n  count(updated) AS total_updates,\n  count(DISTINCT(updated)) AS unique_updates,\n  count(*) AS total_rows, \n  count(DISTINCT(*)) AS unique_non_null_rows\nFROM users_dirty"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98a6e01e-1082-47e6-bc65-c025d6d61b59"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Based on the above summary, we know:\n* All of our emails are unique\n* Our emails contain the largest number of null values\n* The **`updated`** column contains only 1 distinct value, but most are non-null"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df73efa0-948e-45ee-8d9e-d0d5fa976e2b"}}},{"cell_type":"markdown","source":["## Deduplicate Rows\nBased on the above behavior, what do you expect will happen if we use **`DISTINCT *`** to try to remove duplicate records?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb1a0702-2f8f-4878-b311-f3df048eb6de"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW users_deduped AS\n  SELECT DISTINCT(*) FROM users_dirty;\n\nSELECT * FROM users_deduped"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f0f6dae-1d8a-48dd-80d5-02bb78fa1629"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note in the preview above that there appears to be null values, even though our **`COUNT(DISTINCT(*))`** ignored these nulls.\n\nHow many rows do you expect passed through this **`DISTINCT`** command?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f84418d9-7971-477c-ba13-afc83f093732"}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM users_deduped"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b41c17b0-9e94-4704-aae5-e0bca37657a3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we now have a completely new number.\n\nSpark skips null values while counting values in a column or counting distinct values for a field, but does not omit rows with nulls from a **`DISTINCT`** query.\n\nIndeed, the reason we're seeing a new number that is 1 higher than previous counts is because we have 3 rows that are all nulls (here included as a single distinct row)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1bef578-1031-4988-a290-b9044259dc31"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM users_dirty\nWHERE\n  user_id IS NULL AND\n  user_first_touch_timestamp IS NULL AND\n  email IS NULL AND\n  updated IS NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bf66ac0-b3e8-4f4f-a21e-09e14299fec8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deduplicate Based on Specific Columns\n\nRecall that **`user_id`** and **`user_first_touch_timestamp`** should form unique tuples, as they are both generated when a given user is first encountered.\n\nWe can see that we have some null values in each of these fields; exclude nulls counting the distinct number of pairs for these fields will get us the correct count for distinct values in our table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59bfaa0f-8093-4459-8fb7-96937874fad0"}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\nFROM users_dirty\nWHERE user_id IS NOT NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a011c1e-f9cb-4564-b3e5-5f1745c92ecf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here, we'll use these distinct pairs to remove unwanted rows from our data.\n\nThe code below uses **`GROUP BY`** to remove duplicate records based on **`user_id`** and **`user_first_touch_timestamp`**.\n\nThe **`max()`** aggregate function is used on the **`email`** column as a hack to capture non-null emails when multiple records are present; in this batch, all **`updated`** values were equivalent, but we need to use an aggregate function to keep this value in the result of our group by."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b08dc46-b4e6-45c4-ab32-f899c153ea65"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW deduped_users AS\nSELECT user_id, user_first_touch_timestamp, max(email) AS email, max(updated) AS updated\nFROM users_dirty\nWHERE user_id IS NOT NULL\nGROUP BY user_id, user_first_touch_timestamp;\n\nSELECT count(*) FROM deduped_users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6aa8a13-d14f-4b5a-8e4f-298cd62f3d82"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Validate Datasets\nWe've visually confirmed that our counts are as expected, based our manual review.\n \nBelow, we programmatically do some validation using simple filters and **`WHERE`** clauses.\n\nValidate that the **`user_id`** for each row is unique."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90a54a2e-f0c8-4e4e-b9c5-7e52b8a0e604"}}},{"cell_type":"code","source":["%sql\nSELECT max(row_count) <= 1 no_duplicate_ids FROM (\n  SELECT user_id, count(*) AS row_count\n  FROM deduped_users\n  GROUP BY user_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9893299-b4ff-4b9e-9143-db952fa05aff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Confirm that each email is associated with at most one **`user_id`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3f8afb3-ecd4-4f1e-9385-248d559e5f6a"}}},{"cell_type":"code","source":["%sql\nSELECT max(user_id_count) <= 1 at_most_one_id FROM (\n  SELECT email, count(user_id) AS user_id_count\n  FROM deduped_users\n  WHERE email IS NOT NULL\n  GROUP BY email)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b128f948-c2ae-4c69-bedd-cc7c294f9955"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Date Format and Regex\nNow that we've removed null fields and eliminated duplicates, we may wish to extract further value out of the data.\n\nThe code below:\n- Correctly scales and casts the **`user_first_touch_timestamp`** to a valid timestamp\n- Extracts the calendar data and clock time for this timestamp in human readable format\n- Uses **`regexp_extract`** to extract the domains from the email column using regex"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e86ebbb-4d93-4ced-a69b-842cd88b948d"}}},{"cell_type":"code","source":["%sql\nSELECT *,\n  date_format(first_touch, \"MMM d, yyyy\") AS first_touch_date,\n  date_format(first_touch, \"HH:mm:ss\") AS first_touch_time,\n  regexp_extract(email, \"(?<=@).+\", 0) AS email_domain\nFROM (\n  SELECT *,\n    CAST(user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch \n  FROM deduped_users\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cda292b-66a4-4e31-a7fc-05a1d64cfd76"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4396885e-1145-40ca-8f6e-8f1f4313f871"}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be6e77af-52ae-42a2-9715-85f607096c1c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7a1a530-3d97-45b9-9c76-50b867018fbb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 4.6 - Cleaning Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731305831}},"nbformat":4,"nbformat_minor":0}
