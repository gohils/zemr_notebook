{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"085884c1-9215-4bd4-991e-b397f543eb2a"}}},{"cell_type":"markdown","source":["# Propagating Deletes with Change Data Feed\n\nWhile the PII for users has been pseudonymized, generalized, and redacted through several approaches, we have not yet addressed how deletes can be effectively and efficiently handled in the Lakehouse.\n\nIn this notebook, we'll combine Structured Streaming, Delta Lake, and Change Data Feed to demonstrate processing delete requests incrementally and propagating deletes through the Lakehouse.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_users.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Commit arbitrary messages to the Delta log to record important events\n- Apply deletes using Delta Lake DDL\n- Propagate deletes using Change Data Feed\n- Leverage incremental syntax to ensure deletes are committed fully\n- Describe default data retention settings for Change Data Feed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2314857c-4679-409e-88bf-6d3844615356"}}},{"cell_type":"markdown","source":["Begin by running the following cell to set up relevant databases and paths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d37cc91b-8c00-45fb-bf45-ac4bd4c47f72"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-7.2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25f0826d-1f4c-464f-b3de-ab43842a87c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Requirements for Fulfilling Requests to Be Forgotten\n\nThe **`user_lookup`** table contains the link between the **`alt_id`** used as the primary key for the **`users`** table and natural keys found elsewhere in the lakehouse.\n\nDifferent industries will have different requirements for data deletion and data retention. Here, we'll assume the following:\n1. All PII in the **`users`** table must be deleted\n1. Links between pseudonymized keys and natural keys should be forgotten\n1. A policy to remove historic data containing PII from raw data sources and logs should be enacted\n\nThis notebook will focus on the first two of these requirements; the third will be handled in the following lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7d72f04-0323-4806-9509-c80497ca3003"}}},{"cell_type":"markdown","source":["## Processing Right to Be Forgotten Requests\n\nWhile it is possible to process deletes at the same time as appends and updates, the fines around right to be forgotten requests may warrant a separate process.\n\nBelow, logic for setting up a simple table to process delete requests through the users data is displayed. A simple deadline of 30 days after the request is inserted, allowing internal automated audits to leverage this table to ensure compliance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fd17aab-e61a-4a3c-bf6a-9a6ee5658043"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nsalt = \"BEANS\"\n\nschema = \"\"\"\n    user_id LONG, \n    update_type STRING, \n    timestamp FLOAT, \n    dob STRING, \n    sex STRING, \n    gender STRING, \n    first_name STRING, \n    last_name STRING, \n    address STRUCT<street_address: STRING, \n                   city: STRING, \n                   state: STRING, \n                   zip: INT>\"\"\"\n\nrequests_df = (spark.readStream\n                    .table(\"bronze\")\n                    .filter(\"topic = 'user_info'\")\n                    .dropDuplicates([\"value\"]) # Drop duplicate data, not just duplicate event deliveries.\n                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n                    .select(\"v.*\", F.col('v.timestamp').cast(\"timestamp\").alias(\"requested\"))\n                    .filter(\"update_type = 'delete'\")\n                    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n                            \"requested\",\n                            F.date_add(\"requested\", 30).alias(\"deadline\"), \n                            F.lit(\"requested\").alias(\"status\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7265798-8275-45b5-b547-df4c82833c42"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Preview the results of this operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8d64b98-fea0-4cec-9fbf-6d198f8b1a7c"}}},{"cell_type":"code","source":["display(requests_df, streamName = \"requests\")\nDA.block_until_stream_is_ready(name = \"requests\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdadb248-ce95-4726-b651-1109c0a02901"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Adding Commit Messages\n\nDelta Lake supports arbitrary commit messages that will be recorded to the Delta transaction log and viewable in the table history. This can help with later auditing.\n\nSetting this with SQL will create a global commit message that will be used for all subsequent operations in our notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db411dd8-cbfd-4e0c-b557-66c67f505393"}}},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.commitInfo.userMetadata=Deletes committed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cb1b9a8-7f10-4457-8d38-0c10b698a67a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With DataFrames, commit messages can also be specified as part of the write options using the **`userMetadata`** option.\n\nHere, we'll indicate that we're manually processing these requests in a notebook, rather than using an automated job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74a232a8-629f-4608-bcff-5197c8f54bf1"}}},{"cell_type":"code","source":["query = (requests_df.writeStream\n                    .outputMode(\"append\")\n                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/delete_requests\")\n                    .option(\"userMetadata\", \"Requests processed interactively\")\n                    .trigger(availableNow=True)\n                    .table(\"delete_requests\"))\n\nquery.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e26ed0c-8f6c-424b-89bd-c7540cd85c60"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["These messages are clearly visible in the table history in the far right column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b3723f-f99d-4b36-bfc3-2d4da005256a"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8602d729-872a-4a6b-b490-d8bf376bb50c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Processing Delete Requests\n\nThe **`delete_requests`** table will be used to track users' requests to be forgotten. Note that it is possible to process delete requests alongside inserts and updates to existing data as part of a normal **`MERGE`** statement.\n\nBecause PII exists in several places through the current lakehouse, tracking requests and processing them asynchronously may provide better performance for production jobs with low latency SLAs. The approach modeled here also indicates the time at which the delete was requested and the deadline, and provides a field to indicate the current processing status of the request.\n\nReview the **`delete_requests`** table below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb997501-8235-42ae-9e57-9d49e137e173"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cdfd00f-9fbe-4249-89dc-71c71b2458ee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Enable Change Data Feed to Power Incremental Deletes\n\nWe'll be using Change Data Feed to power deletes to many tables from a single source.\n\nBecause the **`user_lookup`** table links identifying information between different pipelines, we'll make this the point where deletes propagate from.\n\nStart by altering the table properties to enable Change Data Feed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"912af545-3727-4bbd-885b-70b473227e1a"}}},{"cell_type":"code","source":["%sql\nALTER TABLE user_lookup \nSET TBLPROPERTIES (delta.enableChangeDataFeed = true);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d21fd13-4f12-42fb-afc0-2e27f9b67d77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Confirm that Change Data Feed is enabled by looking at the table history."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3e54f30-6a3d-4fe3-b69c-9941547e2396"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY user_lookup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fef16d31-d87c-4fc3-8094-56619e656d21"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that because Change Data Feed was enabled after initial table creation, we will only be able to review change data starting with the current table version.\n\nThe cell below will capture this value for use in the next section."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b190db5a-5c82-40f6-9671-f11296367198"}}},{"cell_type":"code","source":["start_version = spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\nprint(start_version)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10cc6528-ce53-4d59-b6c1-802fec7f7a54"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Committing Deletes\nWhen working with static data, committing deletes is simple. \n\nThe following logic modifies the **`user_lookup`** table by rewriting all data files containing records affected by the **`DELETE`** statement. Recall that with Delta Lake, deleting data will create new data files rather than deleting existing data files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"594b5e83-6878-48c8-ab40-5096aacbc592"}}},{"cell_type":"code","source":["%sql\nDELETE FROM user_lookup\nWHERE alt_id IN (SELECT alt_id FROM delete_requests WHERE status = 'requested')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa700e64-69b5-48ba-9724-9280bf5b3443"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Propagate Deletes\nWhile the lakehouse architecture implemented here typically uses the **`user_lookup`** as a static table in joins with incremental data, the Change Data Feed can be separately leveraged as an incremental record of data changes.\n\nThe code below configures as incremental read of all changes committed to the **`user_lookup`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11e3059b-7905-486d-995e-04373309b694"}}},{"cell_type":"code","source":["deleteDF = (spark.readStream\n                 .format(\"delta\")\n                 .option(\"readChangeFeed\", \"true\")\n                 .option(\"startingVersion\", start_version)\n                 .table(\"user_lookup\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c89238f-44fc-4c81-b52e-27edf47ec128"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The relationships between our natural keys (**`user_id`**, **`device_id`**, and **`mac_address`**) are stored in our **`user_lookup`**. These allow us to link a user's data between various pipelines/sources. The Change Data Feed from this table will maintain all these fields, allowing successful identification of records to be deleted or modified in downstream tables.\n\nThe function below demonstrates committing deletes to two tables using different keys and syntax. Note that in this case, the **`MERGE`** syntax demonstrated is not necessary to process the deletes to the **`users`** table; this code block does demonstrate the basic syntax that could be expanded if inserts and updates were to be processed in the same code block as deletes.\n\nAssuming successful completion of these two table modifications, an update will be process back to the **`delete_requests`** table. Note that we're leveraging data that has been successfully deleted from the **`user_lookup`** table to update a value in the **`delete_requests`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b458d15e-6b0e-4586-82c7-706e89173f46"}}},{"cell_type":"code","source":["def process_deletes(microBatchDF, batchId):\n    \n    (microBatchDF\n        .filter(\"_change_type = 'delete'\")\n        .createOrReplaceTempView(\"deletes\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING deletes d\n        ON u.alt_id = d.alt_id\n        WHEN MATCHED\n            THEN DELETE\n    \"\"\")\n\n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        DELETE FROM user_bins\n        WHERE user_id IN (SELECT user_id FROM deletes)\n    \"\"\")\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO delete_requests dr\n        USING deletes d\n        ON d.alt_id = dr.alt_id\n        WHEN MATCHED\n          THEN UPDATE SET status = \"deleted\"\n    \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ba267a8-c34b-404a-a822-c165fe441520"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Recall that this workload is being driven by incremental changes to the **`user_lookup`** table (tracked through the Change Data Feed).\n\nExecuting the following cell will propagate deletes to a single table to multiple tables throughout the lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aeb4e77-a116-4d05-b222-57f6aa2ed72a"}}},{"cell_type":"code","source":["query = (deleteDF.writeStream\n                 .foreachBatch(process_deletes)\n                 .outputMode(\"update\")\n                 .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/deletes\")\n                 .trigger(availableNow=True)\n                 .start())\n\nquery.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02ab4d0b-0728-443f-bbbe-72f90970d888"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Review Delete Commits\nNote that with our current implementation, if a user registration never made it into the **`user_lookup`** table, data for this user will not be deleted from other tables. However, the status for these records in the **`delete_requests`** table will also remain **`requested`**, so a redundant approach could be applied if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbd162d-38c9-4c2d-8c03-b77ab573a227"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"690d616f-8ebd-4935-9991-a3872c3f81d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that our commit message will be in the far right column of our history, under the column **`userMetadata`**.\n\nFor the **`users`** table, the operation field in the history will indicate a merge because of the chosen syntax, even though only deletes were committed. The number of deleted rows can be reviewed in the **`operationMetrics`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31e10d9b-3d4e-433e-ad6c-cbc0ca06311e"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff91c432-91ed-4ac4-ab09-0aea6c5794f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As expected, **`user_bins`** will show a delete."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5390e273-306e-4158-93ad-f63f4b07a13e"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY user_bins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e99a21c-1326-4997-a284-4cbdd9f44372"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The changes to **`delete_requests`** also show a merge operation, and appropriately show that records have been updated rather than deleted in this table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"362f6ef1-0603-40fa-9e2a-5a5d2127a610"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02956cf3-339f-4a17-b009-52b6acf7b3ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Are Deletes Fully Committed?\n\nNot exactly.\n\nBecause of how Delta Lake's history and CDF features are implemented, deleted values are still present in older versions of the data.\n\nThe query below shows the records deleted in v1 of the **`user_bins`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"887acd39-0993-4373-a085-a7420d9516d0"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM user_bins@v0 u1\nEXCEPT \nSELECT * FROM user_bins u2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94d16cb8-375e-4469-9831-3dfad7203f80"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Similarly, while we've already applied our logic on the incremental data produced by deletes committed to the **`user_lookup`** table, this information is still available within the change feed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e616acb-1a14-4ae6-a19f-f8df65ccf546"}}},{"cell_type":"code","source":["df = (spark.read\n           .option(\"readChangeFeed\", \"true\")\n           .option(\"startingVersion\", start_version)\n           .table(\"user_lookup\")\n           .filter(\"_change_type = 'delete'\"))\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892a07ec-b8e8-430d-87fd-78fe6564fc38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The next notebook will explore fully committing these deletes, as well as providing guidance for removing access to historic raw data containing PII."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e3b95a7-570d-44d5-8830-b00e8271856b"}}},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8695c28-e7f9-4d39-b087-bad409de049f"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd872701-284a-4527-b2a8-11ae0f59ff89"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbb4c096-4a9c-4653-942b-ecef876e7afb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADE 6.2 - CDC Propagating Deletes with CDF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731308771}},"nbformat":4,"nbformat_minor":0}
