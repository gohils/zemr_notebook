{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa8e5a5f-de71-46b9-9248-e63c63b6f9ac"}}},{"cell_type":"markdown","source":["# Deidentified PII Access\n\nThis lesson explores approaches for reducing risk of PII leakage while working with potentially sensitive information for analytics and reporting.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_user_bins.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Apply dynamic views to sensitive data to obscure columns containing PII\n- Use dynamic views to filter data, only showing relevant rows to relevant audiences\n- Create binned tables to generalize data and obscure PII"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"229f6283-940c-4be0-97aa-8002d6268894"}}},{"cell_type":"markdown","source":["Begin by running the following cell to set up relevant databases and paths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b818a6c4-c917-4bec-b953-d95720c47717"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-6.3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9ea8e65-1ceb-41bf-a5b1-f556239d5972"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Dynamic Views\n\nDatabricks <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#dynamic-view-functions\" target=\"_blank\">dynamic views</a> allow user or group identity ACLs to be applied to data at the column (or row) level.\n\nDatabase administrators can configure data access privileges to disallow access to a source table and only allow users to query a redacted view. \n\nUsers with sufficient privileges will be able to see all fields, while restricted users will be shown arbitrary results, as defined at view creation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fabfd76c-3882-46c4-994e-f137a7e485a1"}}},{"cell_type":"markdown","source":["Consider our **`users`** table with the following columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c7b2d5f-084f-4a91-97b2-d225f2ae7b12"}}},{"cell_type":"code","source":["%sql \nDESCRIBE TABLE users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"431506e1-aecc-4cd1-8137-41068a4a557e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Obviously first name, last name, date of birth, and street address are problematic. \n\nWe'll also obfuscate zip code (as zip code combined with date of birth has a very high confidence in identifying data)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3937de05-eddb-40b1-8993-773f825cf441"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE VIEW users_vw AS\n  SELECT\n    alt_id,\n    CASE \n      WHEN is_member('ade_demo') THEN dob\n      ELSE 'REDACTED'\n    END AS dob,\n    sex,\n    gender,\n    CASE \n      WHEN is_member('ade_demo') THEN first_name\n      ELSE 'REDACTED'\n    END AS first_name,\n    CASE \n      WHEN is_member('ade_demo') THEN last_name\n      ELSE 'REDACTED'\n    END AS last_name,\n    CASE \n      WHEN is_member('ade_demo') THEN street_address\n      ELSE 'REDACTED'\n    END AS street_address,\n    city,\n    state,\n    CASE \n      WHEN is_member('ade_demo') THEN zip\n      ELSE 'REDACTED'\n    END AS zip,\n    updated\n  FROM users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21620b7d-4a86-4ba2-b03f-0b80bc94e4c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now when we query from **`users_vw`**, only members of the group **`ade_demo`** will be able to see results in plain text.\n\n**NOTE**: You may not have privileges to create groups or assign membership. Your instructor should be able to demonstrate how group membership will change query results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b27d1067-703f-4588-ba66-f5887bf34164"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM users_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc9ff45a-4f9a-46bb-a227-006f4a39cc02"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Adding Conditional Row Access\n\nAdding views with **`WHERE`** clauses to filter source data on different conditions for teams throughout an organization can be a beneficial option for granting access to only the necessary data to each audience. Dynamic views add the option to create these views with full access to underlying data for users with elevated privileges.\n\nNote the views can be layered on top of one another; below, the **`users_vw`** from the previous step is modified with conditional access. Users that aren't members of the specified group will only be able to see records from the city of Los Angeles that have been updated after the specified date."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e4f75ff-10d8-459c-8918-baab09fd0985"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE VIEW users_la_vw AS\nSELECT * FROM users_vw\nWHERE \n  CASE \n    WHEN is_member('ade_demo') THEN TRUE\n    ELSE city = \"Los Angeles\" AND updated > \"2019-12-12\"\n  END"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90cd0ea8-9017-4456-8a8c-cc2fdc2b82f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM users_la_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80db7289-a766-4526-9a3d-9dbad01f1152"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Provide Provisional Access to **`user_lookup`** Table\n\nOur **`user_lookup`** table allows our ETL pipelines to match up our various identifiers with our **`alt_id`** and pull demographic information, as necessary.\n\nMost of our team will not need access to our full PII, but may need to use this table to match up various natural keys from different systems.\n\nDefine a dynamic view named **`user_lookup_vw`** below that provides conditional access to the **`alt_id`** but full access to the other info in our **`user_lookup`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f43a563-5d96-4c0c-b9de-8ff3f59a7348"}}},{"cell_type":"code","source":["%sql\n-- ANSWER\nCREATE OR REPLACE VIEW user_lookup_vw AS\nSELECT \n  CASE \n    WHEN is_member('ade_demo') THEN alt_id\n    ELSE 'REDACTED'\n  END AS alt_id,\n  device_id, mac_address, user_id\nFROM user_lookup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b24dc19-08aa-4a51-829f-9b4e7d5c707c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM user_lookup_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4b63db0-e1e9-45a3-8fb2-5ed2c4a14f0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Generalize PII in Aggregate Tables\n\nAnother approach to reducing chance of exposing PII is only providing access to data at a less specific level.\n\nIn this section, we'll assign users to age bins while maintaining their gender, city, and state information. \n\nThis will provide sufficient demographic information to build comparative dashboards without revealing specific user identity."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2440dda-1102-4ec6-b498-ecac7a24d4d6"}}},{"cell_type":"markdown","source":["Here we're just defining custom logic for replacing values with manually-specified labels."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40d3902a-7e8c-4ad5-8eaa-00bffd931a7d"}}},{"cell_type":"code","source":["def age_bins(dob_col):\n    age_col = F.floor(F.months_between(F.current_date(), dob_col)/12).alias(\"age\")\n    \n    return (F.when((age_col < 18), \"under 18\")\n             .when((age_col >= 18) & (age_col < 25), \"18-25\")\n             .when((age_col >= 25) & (age_col < 35), \"25-35\")\n             .when((age_col >= 35) & (age_col < 45), \"35-45\")\n             .when((age_col >= 45) & (age_col < 55), \"45-55\")\n             .when((age_col >= 55) & (age_col < 65), \"55-65\")\n             .when((age_col >= 65) & (age_col < 75), \"65-75\")\n             .when((age_col >= 75) & (age_col < 85), \"75-85\")\n             .when((age_col >= 85) & (age_col < 95), \"85-95\")\n             .when((age_col >= 95), \"95+\")\n             .otherwise(\"invalid age\").alias(\"age\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfcee954-ca1e-43fb-b6aa-b352c865e8c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because this aggregate view of demographic information is no longer personally identifiable, we can safely store this using our natural key.\n\nWe'll reference our **`user_lookup`** table to match our IDs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97165a9a-5039-45d4-b9e3-b1343be21c83"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nusers_df = spark.table(\"users\")\nlookup_df = spark.table(\"user_lookup\").select(\"alt_id\", \"user_id\")\n\nbins_df = users_df.join(lookup_df, [\"alt_id\"], \"left\").select(\"user_id\", age_bins(F.col(\"dob\")),\"gender\", \"city\", \"state\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf75edf5-d9c2-4e0d-a547-4f0cde37ab60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(bins_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06c04a57-8290-4fad-962d-c2d105115923"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This binned demographic data will be saved to a table for our analysts to reference."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aab0b2ab-6c01-401e-a36f-32107aa58017"}}},{"cell_type":"code","source":["(bins_df.write\n        .format(\"delta\")\n        .option(\"path\", f\"{DA.paths.working_dir}/user_bins\")\n        .mode(\"overwrite\")\n        .saveAsTable(\"user_bins\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5ec2e81-03fa-4e52-bf8d-353b82cc0d85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM user_bins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9cd1b33-1acf-45a6-b4b6-8403343056db"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that as currently implemented, each time this logic is processed, all records will be overwritten with newly calculated values. To decrease chances of identifying birth date at binned boundaries, random noise could be added to the values used to calculate age bins (generally keeping age bins accurate, but reducing the likelihood of transitioning a user to a new bin on their exact birthday)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2678ffe0-1e4e-49b9-a8cc-a21701792151"}}},{"cell_type":"markdown","source":["#### Data object privileges - Manage object privileges\nNote\n\nAn owner or an administrator of an object can perform GRANT, DENY, REVOKE, and SHOW GRANTS operations. However, an administrator cannot deny privileges to or revoke privileges from an owner.\n\nA principal thatâ€™s not an owner or administrator can perform an operation only if the required privilege has been granted.\n\nTo grant, deny, or revoke a privilege for all users, specify the keyword users after TO. For example,\n\nSQL\nCopy to clipboardCopy\nGRANT SELECT ON ANY FILE TO users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeaf0d7c-f477-4ccb-8f5d-5da5bc857d66"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ccf20eb-22a6-411a-8b1b-34fc32c3b644"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE SCHEMA accounting;\nGRANT USAGE ON SCHEMA accounting TO finance;\nGRANT CREATE ON SCHEMA accounting TO finance;\n\nGRANT SELECT ON SCHEMA <schema-name> TO `<user>@<domain-name>`\nGRANT SELECT ON ANONYMOUS FUNCTION TO `<user>@<domain-name>`\nGRANT SELECT ON ANY FILE TO `<user>@<domain-name>`\n\nSHOW GRANTS `<user>@<domain-name>` ON SCHEMA <schema-name>\n\nDENY SELECT ON <table-name> TO `<user>@<domain-name>`\n\nREVOKE ALL PRIVILEGES ON SCHEMA default FROM `<user>@<domain-name>`\nREVOKE SELECT ON <table-name> FROM `<user>@<domain-name>`\n\nGRANT SELECT ON ANY FILE TO users\n\n-- Dynamic view functions -  \n-- current_user(): return the current user name.\n-- is_member(): determine if the current user is a member of a specific Databricks group.\n-- Return: true if the user is a member and false if they are not\nSELECT\n  current_user as user,\n-- Check to see if the current user is a member of the \"Managers\" group.\n  is_member(\"Managers\") as admin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db82d571-e775-463b-b2d6-d56776307536"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Trying to perform permission action on Hive Metastore /CATALOG/`hive_metastore`/DATABASE/`accounting` but Table Access Control is not enabled on this cluster. \n\tat com.databricks.sql.acl.ResolvePermissionManagement.apply(ResolvePermissionManagement.scala:119)\n\tat com.databricks.sql.acl.ResolvePermissionManagement.apply(ResolvePermissionManagement.scala:34)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:803)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:798)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:613)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:613)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: SparkException: Trying to perform permission action on Hive Metastore /CATALOG/`hive_metastore`/DATABASE/`accounting` but Table Access Control is not enabled on this cluster. ","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Trying to perform permission action on Hive Metastore /CATALOG/`hive_metastore`/DATABASE/`accounting` but Table Access Control is not enabled on this cluster. \n\tat com.databricks.sql.acl.ResolvePermissionManagement.apply(ResolvePermissionManagement.scala:119)\n\tat com.databricks.sql.acl.ResolvePermissionManagement.apply(ResolvePermissionManagement.scala:34)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:803)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:798)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:613)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:613)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Column-level permissions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d433d595-5652-46cf-b324-eb0a342b1893"}}},{"cell_type":"code","source":["%sql\n-- Alias the field 'email' to itself (as 'email') to prevent the\n-- permission logic from showing up directly in the column name results.\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  CASE WHEN\n    is_member('auditors') THEN email\n    ELSE 'REDACTED'\n  END AS email,\n  country,\n  product,\n  total\nFROM sales_raw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"775ac83b-2c45-4eda-a511-903683b1e379"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Row-level permissions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92338c87-06f4-4e75-b931-c36b4214283a"}}},{"cell_type":"code","source":["%sql\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  country,\n  product,\n  total\nFROM sales_raw\nWHERE\n  CASE\n    WHEN is_member('managers') THEN TRUE\n    ELSE total <= 1000000\n  END;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"778660ad-04b3-4828-92a4-c358a1323e30"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### data masking"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b74849f1-e454-4a20-9e4c-b942b73fcb34"}}},{"cell_type":"code","source":["%sql\n-- The regexp_extract function takes an email address such as\n-- user.x.lastname@example.com and extracts 'example', allowing\n-- analysts to query the domain name\n\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  region,\n  CASE\n    WHEN is_member('auditors') THEN email\n    ELSE regexp_extract(email, '^.*@(.*)$', 1)\n  END\n  FROM sales_raw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"decdf6f5-c3be-4159-b214-cde9eb8adca0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e635e722-7215-4cbd-9292-d84fe5842589"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a437241-93c0-45f8-afb1-b9d8342ff7fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5818fa2-f768-41f6-894a-a37252d37048"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADE 5.3 - Data permission PII Access","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731308884}},"nbformat":4,"nbformat_minor":0}
