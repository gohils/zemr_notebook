{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f96d3fc5-a2e9-4b61-bd38-c4cda49db865"}}},{"cell_type":"markdown","source":["# Scheduling Efficient Structured Streaming Jobs\n\nWe'll use this notebook as a framework to launch multiple streams on shared resources.\n\nThis notebook contains partially refactored code with all the updates and additions that will allow us to schedule our pipelines and run them as new data arrives, including logic for dealing with partition deletes from our **`bronze`** table.\n\nAlso included is logic to assign each stream to a scheduler pool. Review the code below and then follow the instructions in the following cell to schedule a streaming job.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c00f47c7-14ae-4cb8-afb6-6290b03116d7"}}},{"cell_type":"markdown","source":["## Scheduling this Notebook\n\nThis notebook is designed to be scheduled against a jobs cluster, but can use an interactive cluster to avoid cluster start up times. \n\nNote that executing additional code against an all purpose cluster will result in significant query slowdown.\n\nThe recomended cluster configuration for this demo includes:\n* The latest LTS version of the DBR\n* A single-node cluster\n* A single VM with ~32 cores"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f0220d4-a89c-4f8d-a159-b45cd272611b"}}},{"cell_type":"markdown","source":["## Shuffle Partitions\nBecause shuffles will be triggered by some workloads we need to manage the **`spark.sql.shuffle.partitions`**.\n\nThe default number of shuffle partitions (200) can cripple many streaming jobs.\n\nAs such, it's a reasonably good practice to simply use the maximum number of cores as the high end, and if smaller, maintain a factor of the number of course.\n\nNaturally, this generalized advice changes as you increase the number of streams running on a single cluster.\n\n<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> Note that this value cannot be changed between runs without creating a new checkpoint for each stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8d6ddcb-0d39-416e-b6f3-6694244ef059"}}},{"cell_type":"code","source":["print(f\"Executor cores: {sc.defaultParallelism}\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"becd18ff-68f4-4510-b5c0-1373060a75d7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Widgets\n\nJobs utilize the **`widgets`** submodule to pass parameters to notebooks.\n\nThe **`widgets`** submodule includes a number of methods to allow interactive variables to be set while working with notebooks in the workspace with an interactive cluster. To learn more about this functionality, refer to the <a href=\"https://docs.databricks.com/notebooks/widgets.html#widgets\" target=\"_blank\">Databricks documentation</a>.\n\nThis notebook will focus on only two of these methods, emphasizing their utility when running a notebook as a job:\n1. **`dbutils.widgets.text`** accepts a parameter name and a default value. This is the method through which external values can be passed into scheduled notebooks.\n1. **`dbutils.widgets.get`** accepts a parameter name and retrieves the associated value from the widget with that parameter name.\n\nTaken together, **`dbutils.widgets.text`** allows the passing of external values and **`dbutils.widgets.get`** allows those values to be referenced.\n\n**NOTE**: To run this notebook in triggered batch mode, pass key **`once`** and value **`True`** as a parameter to your scheduled job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06d7fde8-be0a-48ed-94c4-5b1ba7e60c6a"}}},{"cell_type":"code","source":["once = eval(dbutils.widgets.get(\"once\"))\nprint(f\"Once: {once}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c31cb602-79fa-4fa0-a57a-3dd6d9617ce9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<trx-123>\n# Use RocksDB for State Store\n\nRocksDB efficiently managed state in the native memory and local SSD of the cluster, while also automatically saving changes to the provided checkpoint directory for each stream. While not necessary for all Structured Streaming jobs, it can be useful for queries with a large amount of state information being managed.\n\n**NOTE**: The state management scheme cannot be changed between query restarts. Successful execution of this notebook requires that the checkpoints being used in the queries to be scheduled have been completely reset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e59360b7-33eb-4d12-bb62-1405d1ad19b4"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb1fb92-7e8a-4688-86ea-e82cbee4c65c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Setup\nThe following cell loads variables and paths used throughout this notebook.\n\nNote that the [Reset Pipelines]($./1 - Reset Pipelines) notebook included here should be run before scheduling jobs to ensure data is in a fresh state for testing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e63fd09d-c68f-44df-8bec-ee6c3e037150"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-8.4.2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc1af95d-6289-4ad9-a2c2-3ccaffba3705"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Custom Streaming Query Listener\n\nSome production streaming applications require real-time monitoring of streaming query progress. \n\nGenerally, these results will be streamed backed into a pub/sub system for real-time dashboarding. \n\nHere, we'll append the output logs to a JSON directory that we can later read in with Auto Loader."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"800f4de0-a5a5-4295-81a7-b7d59fccf225"}}},{"cell_type":"code","source":["%run ../../Includes/StreamingQueryListener"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bf8c4a6-5480-42f0-bccc-15ab751f3975"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Auto Optimize and Auto Compaction\n\nWe'll want to ensure that our bronze table and 3 parsed silver tables don't contain too many small files. Turning on Auto Optimize and Auto Compaction help us to avoid this problem. For more information on these settings, <a href=\"https://docs.databricks.com/delta/optimizations/auto-optimize.html\" target=\"_blank\">consult our documentation</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42c94ee3-97aa-4901-b28f-df7eb2fa77bb"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be8fdeed-af47-4a87-9ee5-7ba8e389224e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c7ceb2e-6d03-4432-afe7-cce80b587354"}}},{"cell_type":"code","source":["date_lookup_df = spark.table(\"date_lookup\").select(\"date\", \"week_part\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cc35d05-a832-4d7e-b376-4ec5bcf4a52a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def process_bronze(source, table_name, checkpoint, once=False, processing_time=\"5 seconds\"):\n    from pyspark.sql import functions as F\n    \n    schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n    \n    data_stream_writer = (spark\n            .readStream\n            .format(\"cloudFiles\")\n            .schema(schema)\n            .option(\"maxFilesPerTrigger\", 2)\n            .option(\"cloudFiles.format\", \"json\")\n            .load(source)\n            .join(F.broadcast(date_lookup_df), [F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\")], \"left\")\n            .writeStream\n            .option(\"checkpointLocation\", checkpoint)\n            .partitionBy(\"topic\", \"week_part\")\n            .queryName(\"bronze\")\n         )\n    \n    if once == True:\n        return data_stream_writer.trigger(availableNow=True).table(table_name)\n    else:\n        return data_stream_writer.trigger(processingTime=processing_time).table(table_name)\n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dec88ce6-6f21-402c-ab43-f36412c62f22"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Configure Apache Spark Scheduler Pools for Efficiency\n\nBy default, all queries started in a notebook run in the same <a href=\"https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application\" target=\"_blank\">fair scheduling pool</a>. Therefore, jobs generated by triggers from all of the streaming queries in a notebook run one after another in first in, first out (FIFO) order. This can cause unnecessary delays in the queries, because they are not efficiently sharing the cluster resources.\n\nIn particular, resource-intensive streams can hog the available compute in a cluster, preventing smaller streams from achieving low latency. Configuring pools provides the capacity to fine tune your cluster to ensure processing time.\n\nTo enable all streaming queries to execute jobs concurrently and to share the cluster efficiently, you can set the queries to execute in separate scheduler pools. This **local property configuration** will be in the same notebook cell where we start the streaming query. For example:\n\n** Run streaming query1 in scheduler pool1 **\n\n<strong><code>\nspark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")<br/>\ndf.writeStream.queryName(\"query1\").format(\"parquet\").start(path1)\n</code></strong>\n\n** Run streaming query2 in scheduler pool2 **\n\n<strong><code>\nspark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\")<br/>\ndf.writeStream.queryName(\"query2\").format(\"delta\").start(path2)\n</code></strong>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ab8a2a7-e449-4206-a7c6-c0eaec186601"}}},{"cell_type":"code","source":["spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze\")\n\nbronze_query = process_bronze(DA.paths.producer_30m, \"bronze_dev\", f\"{DA.paths.checkpoints}/bronze\", once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18c9403e-83ea-4050-9901-c7ce85d8fe16"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Parse Silver Tables\n\nIn the next cell, we define a Python class to handle the queries that result in our **`heart_rate_silver`** and **`workouts_silver`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82071001-53fa-47ed-934a-bfa1a1a4337f"}}},{"cell_type":"code","source":["class Upsert:\n    def __init__(self, query, update_temp=\"stream_updates\"):\n        self.query = query\n        self.update_temp = update_temp \n        \n    def upsertToDelta(self, microBatchDF, batch):\n        microBatchDF.createOrReplaceTempView(self.update_temp)\n        microBatchDF._jdf.sparkSession().sql(self.query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d59805a-de5e-4ddb-b48a-9a5bd0413c4d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# heart_rate_silver\ndef heart_rate_silver(source_table=\"bronze\", once=False, processing_time=\"10 seconds\"):\n    from pyspark.sql import functions as F\n    \n    query = \"\"\"\n        MERGE INTO heart_rate_silver a\n        USING heart_rate_updates b\n        ON a.device_id=b.device_id AND a.time=b.time\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\"\n\n    streamingMerge=Upsert(query, \"heart_rate_updates\")\n    \n    data_stream_writer = (spark\n        .readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'bpm'\")\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n        .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\"))\n        .withWatermark(\"time\", \"30 seconds\")\n        .dropDuplicates([\"device_id\", \"time\"])\n        .writeStream\n        .foreachBatch(streamingMerge.upsertToDelta)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/heart_rate_silver\")\n        .queryName(\"heart_rate_silver\")\n    )\n  \n    if once == True:\n        return data_stream_writer.trigger(availableNow=True).start()\n    else:\n        return data_stream_writer.trigger(processingTime=processing_time).start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eff06982-26b6-4d23-85b4-10cab6fa42af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# workouts_silver\ndef workouts_silver(source_table=\"bronze\", once=False, processing_time=\"15 seconds\"):\n    from pyspark.sql import functions as F\n    \n    query = \"\"\"\n        MERGE INTO workouts_silver a\n        USING workout_updates b\n        ON a.user_id=b.user_id AND a.time=b.time\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\"\n\n    streamingMerge=Upsert(query, \"workout_updates\")\n    \n    data_stream_writer = (spark\n        .readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'workout'\")\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\").alias(\"v\"))\n        .select(\"v.*\")\n        .select(\"user_id\", \"workout_id\", F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \"action\", \"session_id\")\n        .withWatermark(\"time\", \"30 seconds\")\n        .dropDuplicates([\"user_id\", \"time\"])\n        .writeStream\n        .foreachBatch(streamingMerge.upsertToDelta)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/workouts_silver\")\n        .queryName(\"workouts_silver\")\n\n    )\n\n    if once == True:\n        return data_stream_writer.trigger(availableNow=True).start()\n    else:\n        return data_stream_writer.trigger(processingTime=processing_time).start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2b6adc5-b6f3-45b2-9e79-95cf57ad3495"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# users\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    from pyspark.sql.window import Window\n    from pyspark.sql import functions as F\n\n    window = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n    \n    (microBatchDF\n        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n        .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n        WHEN MATCHED AND u.updated < r.updated\n          THEN UPDATE SET *\n        WHEN NOT MATCHED\n          THEN INSERT *\n    \"\"\")\n\ndef users_silver(source_table=\"bronze\", once=False, processing_time=\"30 seconds\"):\n    from pyspark.sql import functions as F\n\n    schema = \"\"\"\n        user_id LONG, \n        update_type STRING, \n        timestamp FLOAT, \n        dob STRING, \n        sex STRING, \n        gender STRING, \n        first_name STRING, \n        last_name STRING, \n        address STRUCT<\n            street_address: STRING, \n            city: STRING, \n            state: STRING, \n            zip: INT\n        >\"\"\"\n\n    salt = \"BEANS\"\n\n    data_stream_writer = (spark\n        .readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'user_info'\")\n        .dropDuplicates()\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n        .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n                F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n                F.to_date('dob','MM/dd/yyyy').alias('dob'),\n                'sex', 'gender','first_name','last_name',\n                'address.*', \"update_type\")\n        .writeStream\n        .foreachBatch(batch_rank_upsert)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/users\")\n        .queryName(\"users\")\n    )\n    \n    if once == True:\n        return data_stream_writer.trigger(availableNow=True).start()\n    else:\n        return data_stream_writer.trigger(processingTime=processing_time).start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"002427b5-0f24-4a84-b9ee-577ed7b62346"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_parsed\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f59a590-517e-4a5f-a5cf-56700cef1ebd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["heart_rate_silver_query = heart_rate_silver(source_table=\"bronze_dev\", once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62376c93-3d0f-46d4-930c-66d1b8763ae2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["workouts_silver_query = workouts_silver(source_table=\"bronze_dev\", once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c927c34f-e604-4c5e-a046-50d933eb0a52"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users_query = users_silver(source_table=\"bronze_dev\", once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c5e16ec-97b8-427f-9131-de4ebf6623e7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if once:\n    # While triggered once, we still want them to run in\n    #  parallel but, we want to block here until they are done.\n    bronze_query.awaitTermination()\n    heart_rate_silver_query.awaitTermination()\n    workouts_silver_query.awaitTermination()\n    users_query.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e646d455-69dc-4489-8a2e-58a3e0388d96"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Unlike other lessons, we will **NOT** be be executing our **`DA.cleanup()`** command<br/>\nas we want these assets to persist through all the notebooks in this demo.\n\nHowever, we don't want to leave this demo running forever so we will stop all streams after 30 minutes.<br/>\nThis is approximately the same amount of time that our Streaming Factory will run from start to finish."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e74a251-586c-469d-a1d0-b6fdb72662f7"}}},{"cell_type":"code","source":["import time\nif not once: \n    time.sleep(30*60)\n    print(\"Time's up!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17c13be2-28f2-41b0-ad8c-4efdba643b02"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now that the 30 minutes have passed, we will stop all streams."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39303152-f0aa-4a6a-8fbf-51e398f03f9c"}}},{"cell_type":"code","source":["# If once, streams would have auto-terminated.\n# Otherwise, we need to stop them now that our 30 min demo is over\nfor stream in spark.streams.active:\n    print(f\"Stopping the stream {stream.name}\")\n    stream.stop()\n    stream.awaitTermination()\n        \nprint(\"All done.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"463f5eae-110f-4e55-a9ca-9a0f1197a3ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3012219a-efed-4a04-b374-b2ed5163f45c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2 - Schedule Streaming Jobs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731310217}},"nbformat":4,"nbformat_minor":0}
