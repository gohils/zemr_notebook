{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6bc036-c730-497d-a83c-9f9f0040f55d"}}},{"cell_type":"markdown","source":["# Advanced SQL Transformations\n\nQuerying tabular data stored in the data lakehouse with Spark SQL is easy, efficient, and fast.\n\nThis gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Use **`.`** and **`:`** syntax to query nested data\n- Work with JSON\n- Flatten and unpacking arrays and structs\n- Combine datasets using joins and set operators\n- Reshape data using pivot tables\n- Use higher order functions for working with arrays"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce12c853-c22b-4afb-b9ab-830abf84f680"}}},{"cell_type":"markdown","source":["## Run Setup\n\nThe setup script will create the data and declare necessary values for the rest of this notebook to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"085a2815-38bd-4875-8e38-894c664cce91"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-04.7"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92deffd9-d66d-4007-9d60-eb655d334ef1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Interacting with JSON Data\n\nThe **`events_raw`** table was registered against data representing a Kafka payload.\n\nIn most cases, Kafka data will be binary-encoded JSON values. We'll cast the **`key`** and **`value`** as strings below to look at these in a human-readable format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c90f5048-9258-4670-a4e0-bcee2399f28d"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW events_strings AS\n  SELECT string(key), string(value) \n  FROM events_raw;\n  \nSELECT * FROM events_strings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f69bdf7d-f797-430b-ba63-13b06b9b9c47"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark SQL has built-in functionality to directly interact with JSON data stored as strings. We can use the **`:`** syntax to traverse nested data structures."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f134dc2-1366-4e8d-8356-630534712685"}}},{"cell_type":"code","source":["%sql\nSELECT value:device, value:geo:city \nFROM events_strings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d191e730-3db5-4be0-a609-ce58aaba2880"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark SQL also has the ability to parse JSON objects into struct types (a native Spark type with nested attributes).\n\nHowever, the **`from_json`** function requires a schema. To derive the schema of our current data, we'll start by executing a query we know will return a JSON value with no null fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"949d0b91-c3fa-4ee0-a402-2f21dec2091b"}}},{"cell_type":"code","source":["%sql\nSELECT value \nFROM events_strings \nWHERE value:event_name = \"finalize\" \nORDER BY key\nLIMIT 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8235e053-c5df-467b-9525-163c92214f45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark SQL also has a **`schema_of_json`** function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the **`from_json`** function to cast our **`value`** field to a struct type."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97739521-b7ac-493c-8f91-8e659ab74a19"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW parsed_events AS\n  SELECT from_json(value, schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}')) AS json \n  FROM events_strings;\n  \nSELECT * FROM parsed_events"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95020247-a94c-4426-9241-838c5b170fbe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Once a JSON string is unpacked to a struct type, Spark supports **`*`** (star) unpacking to flatten fields into columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89d4da35-9d76-4e05-8cf2-80994274bb66"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW new_events_final AS\n  SELECT json.* \n  FROM parsed_events;\n  \nSELECT * FROM new_events_final"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ff4932b-999c-4c0b-9bda-e467f260eab2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Explore Data Structures\n\nSpark SQL has robust syntax for working with complex and nested data types.\n\nStart by looking at the fields in the **`events`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9724bd05-c919-4abb-89b3-33e9bb34f89a"}}},{"cell_type":"code","source":["%sql\nDESCRIBE events"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"628f8556-c42e-4e46-a5a2-184583c367ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The **`ecommerce`** field is a struct that contains a double and 2 longs.\n\nWe can interact with the subfields in this field using standard **`.`** syntax similar to how we might traverse nested data in JSON."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f208cb3-0113-41af-a0ac-dcdc2e1802ed"}}},{"cell_type":"code","source":["%sql\nSELECT ecommerce.purchase_revenue_in_usd \nFROM events\nWHERE ecommerce.purchase_revenue_in_usd IS NOT NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05541e3a-16e0-4221-88f8-fb30cba906be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Explode Arrays\nThe **`items`** field in the **`events`** table is an array of structs.\n\nSpark SQL has a number of functions specifically to deal with arrays.\n\nThe **`explode`** function lets us put each element in an array on its own row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cffcd48-e717-44bf-b8bb-5f82487801d4"}}},{"cell_type":"code","source":["%sql\nSELECT user_id, event_timestamp, event_name, explode(items) AS item \nFROM events"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1004ae3c-aa7c-408d-8def-94462612d5c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Collect Arrays\n\nThe **`collect_set`** function can collect unique values for a field, including fields within arrays.\n\nThe **`flatten`** function allows multiple arrays to be combined into a single array.\n\nThe **`array_distinct`** function removes duplicate elements from an array.\n\nHere, we combine these queries to create a simple table that shows the unique collection of actions and the items in a user's cart."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d5564d0-e618-4332-ad15-68b606324cf9"}}},{"cell_type":"code","source":["%sql\nSELECT user_id,\n  collect_set(event_name) AS event_history,\n  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\nFROM events\nGROUP BY user_id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23e82ed7-e085-4c51-9128-0271f9fdab8c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Join Tables\n\nSpark SQL supports standard join operations (inner, outer, left, right, anti, cross, semi).\n\nHere we chain a join with a lookup table to an **`explode`** operation to grab the standard printed item name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ddce2ab-0146-4c49-97d2-cc35ccd647de"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE VIEW sales_enriched AS\nSELECT *\nFROM (\n  SELECT *, explode(items) AS item \n  FROM sales) a\nINNER JOIN item_lookup b\nON a.item.item_id = b.item_id;\n\nSELECT * FROM sales_enriched"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8f5640b-2573-4dde-beb6-14aceded8bc0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Set Operators\nSpark SQL supports **`UNION`**, **`MINUS`**, and **`INTERSECT`** set operators.\n\n**`UNION`** returns the collection of two queries. \n\nThe query below returns the same results as if we inserted our **`new_events_final`** into the **`events`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a50f9fab-c36b-40ba-a2fa-088400446f36"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM events \nUNION \nSELECT * FROM new_events_final"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c969103-14ac-4702-afa2-7d60ff735362"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**`INTERSECT`** returns all rows found in both relations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03022dc9-9c41-4f6d-aeca-e3d67add6d0c"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM events \nINTERSECT \nSELECT * FROM new_events_final"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5596d77c-1089-45e1-b2a0-7512ac3c2dd7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The above query returns no results because our two datasets have no values in common.\n\n**`MINUS`** returns all the rows found in one dataset but not the other; we'll skip executing this here as our previous query demonstrates we have no values in common."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"957827f7-bfef-4c43-8c16-a29a76bf7aff"}}},{"cell_type":"markdown","source":["## Pivot Tables\nThe **`PIVOT`** clause is used for data perspective. We can get the aggregated values based on specific column values, which will be turned to multiple columns used in **`SELECT`** clause. The **`PIVOT`** clause can be specified after the table name or subquery.\n\n**`SELECT * FROM ()`**: The **`SELECT`** statement inside the parentheses is the input for this table.\n\n**`PIVOT`**: The first argument in the clause is an aggregate function and the column to be aggregated. Then, we specify the pivot column in the **`FOR`** subclause. The **`IN`** operator contains the pivot column values. \n\nHere we use **`PIVOT`** to create a new **`transactions`** table that flattens out the information contained in the **`sales`** table.\n\nThis flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d73792c-3284-4b98-8753-bbda96ad206e"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE transactions AS\n\nSELECT * FROM (\n  SELECT\n    email,\n    order_id,\n    transaction_timestamp,\n    total_item_quantity,\n    purchase_revenue_in_usd,\n    unique_items,\n    item.item_id AS item_id,\n    item.quantity AS quantity\n  FROM sales_enriched\n) PIVOT (\n  sum(quantity) FOR item_id in (\n    'P_FOAM_K',\n    'M_STAN_Q',\n    'P_FOAM_S',\n    'M_PREM_Q',\n    'M_STAN_F',\n    'M_STAN_T',\n    'M_PREM_K',\n    'M_PREM_F',\n    'M_STAN_K',\n    'M_PREM_T',\n    'P_DOWN_S',\n    'P_DOWN_K'\n  )\n);\n\nSELECT * FROM transactions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3914684-67e4-49af-a438-26d2773777ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Higher Order Functions\nHigher order functions in Spark SQL allow you to work directly with complex data types. When working with hierarchical data, records are frequently stored as array or map type objects. Higher-order functions allow you to transform data while preserving the original structure.\n\nHigher order functions include:\n- **`FILTER`** filters an array using the given lambda function.\n- **`EXIST`** tests whether a statement is true for one or more elements in an array. \n- **`TRANSFORM`** uses the given lambda function to transform all elements in an array.\n- **`REDUCE`** takes two lambda functions to reduce the elements of an array to a single value by merging the elements into a buffer, and the apply a finishing function on the final buffer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ed0f60-8f41-4c09-9b23-9c1a7d00780f"}}},{"cell_type":"markdown","source":["## Filter\nRemove items that are not king-sized from all records in our **`items`** column. We can use the **`FILTER`** function to create a new column that excludes that value from each array.\n\n**`FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items`**\n\nIn the statement above:\n- **`FILTER`** : the name of the higher-order function <br>\n- **`items`** : the name of our input array <br>\n- **`i`** : the name of the iterator variable. You choose this name and then use it in the lambda function. It iterates over the array, cycling each value into the function one at a time.<br>\n- **`->`** :  Indicates the start of a function <br>\n- **`i.item_id LIKE \"%K\"`** : This is the function. Each value is checked to see if it ends with the capital letter K. If it is, it gets filtered into the new column, **`king_items`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc4db6d-d5b9-40c1-85f9-0653b89a0423"}}},{"cell_type":"code","source":["%sql\n-- filter for sales of only king sized items\nSELECT\n  order_id,\n  items,\n  FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\nFROM sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89b1df8d-68b9-4e74-a6ec-22f4ce88c48e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You may write a filter that produces a lot of empty arrays in the created column. When that happens, it can be useful to use a **`WHERE`** clause to show only non-empty array values in the returned column. \n\nIn this example, we accomplish that by using a subquery (a query within a query). They are useful for performing an operation in multiple steps. In this case, we're using it to create the named column that we will use with a **`WHERE`** clause."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1bcef88-f66f-42f3-8005-e3b7f7b3c5e6"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW king_size_sales AS\n\nSELECT order_id, king_items\nFROM (\n  SELECT\n    order_id,\n    FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\n  FROM sales)\nWHERE size(king_items) > 0;\n  \nSELECT * FROM king_size_sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc596abc-b14a-4f53-b051-6738d3cd1014"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transform\nBuilt-in functions are designed to operate on a single, simple data type within a cell; they cannot process array values. **`TRANSFORM`** can be particularly useful when you want to apply an existing function to each element in an array. \n\nCompute the total revenue from king-sized items per order.\n\n**`TRANSFORM(king_items, k -> CAST(k.item_revenue_in_usd * 100 AS INT)) AS item_revenues`**\n\nIn the statement above, for each value in the input array, we extract the item's revenue value, multiply it by 100, and cast the result to integer. Note that we're using the same kind as references as in the previous command, but we name the iterator with a new variable, **`k`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1c5d250-5797-4cac-96be-1f3608f86b3d"}}},{"cell_type":"code","source":["%sql\n-- get total revenue from king items per order\nCREATE OR REPLACE TEMP VIEW king_item_revenues AS\n\nSELECT\n  order_id,\n  king_items,\n  TRANSFORM (\n    king_items,\n    k -> CAST(k.item_revenue_in_usd * 100 AS INT)\n  ) AS item_revenues\nFROM king_size_sales;\n\nSELECT * FROM king_item_revenues\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a1c0498-8813-4ec5-aa64-6d2bbb78bc32"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Summary\nSpark SQL offers a comprehensive set of native functionality for interacting with and manipulating highly nested data.\n\nWhile some syntax for this functionality may be unfamiliar to SQL users, leveraging built-in functions like higher order functions can prevent SQL engineers from needing to rely on custom logic when dealing with highly complex data structures."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"691751c3-af91-44be-881f-6648c378054d"}}},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdd79e32-09ca-4424-ac59-479ed99b94fa"}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6030f74f-7395-4b50-9bef-00e54b5df591"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7610dc39-cfc1-444c-b4cc-03a1e657c5ef"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 4.7 - Advanced SQL Transformations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731306838}},"nbformat":4,"nbformat_minor":0}
