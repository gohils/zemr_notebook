{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"518a1dd8-329f-4fab-ac83-534eea0edfe2"}}},{"cell_type":"markdown","source":["# Managing Delta Tables\n\nIf you know any flavor of SQL, you already have much of the knowledge you'll need to work effectively in the data lakehouse.\n\nIn this notebook, we'll explore basic manipulation of data and tables with SQL on Databricks.\n\nNote that Delta Lake is the default format for all tables created with Databricks; if you've been running SQL statements on Databricks, you're likely already working with Delta Lake.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Create Delta Lake tables\n* Query data from Delta Lake tables\n* Insert, update, and delete records in Delta Lake tables\n* Write upsert statements with Delta Lake\n* Drop Delta Lake tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79f14552-3f89-4799-92eb-6f150cec5522"}}},{"cell_type":"markdown","source":["## Run Setup\nThe first thing we're going to do is run a setup script. It will define a username, userhome, and database that is scoped to each user."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db20b775-7561-4496-a1ae-12fce2eeedf4"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-02.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1192896-44a3-4e64-a34f-e2d6f4b45d8a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Creating a Delta Table\n\nThere's not much code you need to write to create a table with Delta Lake. There are a number of ways to create Delta Lake tables that we'll see throughout the course. We'll begin with one of the easiest methods: registering an empty Delta Lake table.\n\nWe need: \n- A **`CREATE TABLE`** statement\n- A table name (below we use **`students`**)\n- A schema\n\n**NOTE:** In Databricks Runtime 8.0 and above, Delta Lake is the default format and you donâ€™t need **`USING DELTA`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58cdb281-abd2-4d1d-a961-bdeaca52704d"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE students\n  (id INT, name STRING, value DOUBLE);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afe797ae-f04c-4bc9-a7bd-fa17209fa1e3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If we try to go back and run that cell again...it will error out! This is expected - because the table exists already, we receive an error.\n\nWe can add in an additional argument, **`IF NOT EXISTS`** which checks if the table exists. This will overcome our error."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cbaf2e8-34b8-4a31-b853-522f608b0eb0"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE IF NOT EXISTS students \n  (id INT, name STRING, value DOUBLE)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13f074fa-d58a-4f50-847b-39157c62c98e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Inserting Data\nMost often, data will be inserted to tables as the result of a query from another source.\n\nHowever, just as in standard SQL, you can also insert values directly, as shown here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bb48d70-17c3-4942-bf34-ae5eeefb00a7"}}},{"cell_type":"code","source":["%sql\nINSERT INTO students VALUES (1, \"Yve\", 1.0);\nINSERT INTO students VALUES (2, \"Omar\", 2.5);\nINSERT INTO students VALUES (3, \"Elia\", 3.3);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281a87eb-0006-493a-86be-757f7b428d2b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the cell above, we completed three separate **`INSERT`** statements. Each of these is processed as a separate transaction with its own ACID guarantees. Most frequently, we'll insert many records in a single transaction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4f9e68d-fb47-428e-91e7-01c39931f93e"}}},{"cell_type":"code","source":["%sql\nINSERT INTO students\nVALUES \n  (4, \"Ted\", 4.7),\n  (5, \"Tiffany\", 5.5),\n  (6, \"Vini\", 6.3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fdfbcde-893c-470c-9a12-998e9671dfeb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that Databricks doesn't have a **`COMMIT`** keyword; transactions run as soon as they're executed, and commit as they succeed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13a96c9-b170-48e5-8b04-db8dd1189135"}}},{"cell_type":"markdown","source":["## Querying a Delta Table\n\nYou probably won't be surprised that querying a Delta Lake table is as easy as using a standard **`SELECT`** statement."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b169d69d-c05b-4eb4-ba64-33ea38b3a183"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM students"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"555d4c7f-e873-41cb-99c6-768f007dcceb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What may surprise you is that Delta Lake guarantees that any read against a table will **always** return the most recent version of the table, and that you'll never encounter a state of deadlock due to ongoing operations.\n\nTo repeat: table reads can never conflict with other operations, and the newest version of your data is immediately available to all clients that can query your lakehouse. Because all transaction information is stored in cloud object storage alongside your data files, concurrent reads on Delta Lake tables is limited only by the hard limits of object storage on cloud vendors. (**NOTE**: It's not infinite, but it's at least thousands of reads per second.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c069c0db-bfc3-4d39-a50e-cce0d966b51f"}}},{"cell_type":"markdown","source":["## Updating Records\n\nUpdating records provides atomic guarantees as well: we perform a snapshot read of the current version of our table, find all fields that match our **`WHERE`** clause, and then apply the changes as described.\n\nBelow, we find all students that have a name starting with the letter **T** and add 1 to the number in their **`value`** column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93b1816a-81e5-4968-bf90-cd6c7a467131"}}},{"cell_type":"code","source":["%sql\nUPDATE students \nSET value = value + 1\nWHERE name LIKE \"T%\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99e8beda-c656-424c-8505-ecf2651aed98"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Query the table again to see these changes applied."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5455ef4b-74d2-4ced-b75d-ee768418c9e8"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM students"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2b9b918-057c-4d9b-9659-394571fee305"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deleting Records\n\nDeletes are also atomic, so there's no risk of only partially succeeding when removing data from your data lakehouse.\n\nA **`DELETE`** statement can remove one or many records, but will always result in a single transaction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3a25c47-df8a-44b5-9d9f-a175b3ad5dca"}}},{"cell_type":"code","source":["%sql\nDELETE FROM students \nWHERE value > 6"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1764ad7-5485-43d5-8e9a-dcfa6dde0f49"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Using Merge\n\nSome SQL systems have the concept of an upsert, which allows updates, inserts, and other data manipulations to be run as a single command.\n\nDatabricks uses the **`MERGE`** keyword to perform this operation.\n\nConsider the following temporary view, which contains 4 records that might be output by a Change Data Capture (CDC) feed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a775a549-a477-47f1-8faf-ee6b389f94cc"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n  (2, \"Omar\", 15.2, \"update\"),\n  (3, \"\", null, \"delete\"),\n  (7, \"Blue\", 7.7, \"insert\"),\n  (11, \"Diya\", 8.8, \"update\");\n  \nSELECT * FROM updates;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99539fde-21e8-4491-b496-22b5186ec657"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Using the syntax we've seen so far, we could filter from this view by type to write 3 statements, one each to insert, update, and delete records. But this would result in 3 separate transactions; if any of these transactions were to fail, it might leave our data in an invalid state.\n\nInstead, we combine these actions into a single atomic transaction, applying all 3 types of changes together.\n\n**`MERGE`** statements must have at least one field to match on, and each **`WHEN MATCHED`** or **`WHEN NOT MATCHED`** clause can have any number of additional conditional statements.\n\nHere, we match on our **`id`** field and then filter on the **`type`** field to appropriately update, delete, or insert our records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"842773da-6398-4c56-a317-9243203f741c"}}},{"cell_type":"code","source":["%sql\nMERGE INTO students b\nUSING updates u\nON b.id=u.id\nWHEN MATCHED AND u.type = \"update\"\n  THEN UPDATE SET *\nWHEN MATCHED AND u.type = \"delete\"\n  THEN DELETE\nWHEN NOT MATCHED AND u.type = \"insert\"\n  THEN INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9a9abb7-f416-4842-a8af-9612cbe8512a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that only 3 records were impacted by our **`MERGE`** statement; one of the records in our updates table did not have a matching **`id`** in the students table but was marked as an **`update`**. Based on our custom logic, we ignored this record rather than inserting it. \n\nHow would you modify the above statement to include unmatched records marked **`update`** in the final **`INSERT`** clause?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcf101dc-4706-4424-a6fe-f1df97f9b8f4"}}},{"cell_type":"markdown","source":["## Dropping a Table\n\nAssuming that you have proper permissions on the target table, you can permanently delete data in the lakehouse using a **`DROP TABLE`** command.\n\n**NOTE**: Later in the course, we'll discuss Table Access Control Lists (ACLs) and default permissions. In a properly configured lakehouse, users should **not** be able to delete production tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd05424e-76d6-4c4f-9d68-0a58054a3ca5"}}},{"cell_type":"code","source":["%sql\nDROP TABLE students"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07511e78-04d4-450a-9ab6-6ba58d3472ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bafd861-803d-4ef2-8045-359cac3f9add"}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce652ba4-078a-4f2d-bab6-8f7bef6a1af2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf0b41c-39fa-4b43-859f-6bb5fe823fd6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 2.1 - Managing Delta Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731305290}},"nbformat":4,"nbformat_minor":0}
