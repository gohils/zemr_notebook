{"cells":[{"cell_type":"markdown","source":["# Streaming Query\n\n##### Objectives\n1. Build streaming DataFrames\n1. Display streaming query results\n1. Write streaming query results\n1. Monitor streaming query\n\n##### Classes\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html\" target=\"_blank\">DataStreamReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html\" target=\"_blank\">DataStreamWriter</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html\" target=\"_blank\">StreamingQuery</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c3c4551-67f0-4132-8593-ad8f316fea69"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96560e4b-8dd0-44ce-940d-90c771b162c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Build streaming DataFrames\n\nObtain an initial streaming DataFrame from a Delta-format file source."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c7a7996-c429-4e50-a9b6-4aa334325f38"}}},{"cell_type":"code","source":["df = (spark\n      .readStream\n      .option(\"maxFilesPerTrigger\", 1)\n      .format(\"delta\")\n      .load(DA.paths.events)\n     )\n\ndf.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fd02f6b-2a72-4156-b3fb-e25f40ad333a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Apply some transformations, producing new streaming DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"465a3f10-ddda-495e-a391-d7ea75ae2ff4"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, approx_count_distinct, count\n\nemail_traffic_df = (df\n                    .filter(col(\"traffic_source\") == \"email\")\n                    .withColumn(\"mobile\", col(\"device\").isin([\"iOS\", \"Android\"]))\n                    .select(\"user_id\", \"event_timestamp\", \"mobile\")\n                   )\n\nemail_traffic_df.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b730e2a-8a27-413a-8fe4-9fef3d1fe6ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write streaming query results\n\nTake the final streaming DataFrame (our result table) and write it to a file sink in \"append\" mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"943b3734-9a89-4998-bb84-f51863a41c3b"}}},{"cell_type":"code","source":["checkpoint_path = f\"{DA.paths.checkpoints}/email_traffic\"\noutput_path = f\"{DA.paths.working_dir}/email_traffic/output\"\n\ndevices_query = (email_traffic_df\n                 .writeStream\n                 .outputMode(\"append\")\n                 .format(\"delta\")\n                 .queryName(\"email_traffic\")\n                 .trigger(processingTime=\"1 second\")\n                 .option(\"checkpointLocation\", checkpoint_path)\n                 .start(output_path)\n                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea5cbb4e-c3d6-40cf-b5c7-f22ad63e3a3c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Monitor streaming query\n\nUse the streaming query \"handle\" to monitor and control it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4f76872-f208-4bef-aa00-4d4f480443e5"}}},{"cell_type":"code","source":["devices_query.id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5309765a-f82b-4abf-9ad8-633ce423644b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devices_query.status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a6cac5b-143d-4926-8078-a7fb147fac44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devices_query.lastProgress"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1def7822-56cb-4e04-8c79-0b540557d5f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import time\n# Run for 10 more seconds\ntime.sleep(10) \n\ndevices_query.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cb3f739-d6b3-425e-97e9-5914c02d271b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devices_query.awaitTermination()\n\n%md\n# Coupon Sales Lab\nProcess and append streaming data on transactions using coupons.\n1. Read data stream\n2. Filter for transactions with coupons codes\n3. Write streaming query results to Delta\n4. Monitor streaming query\n5. Stop streaming query\n\n##### Classes\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html\" target=\"_blank\">DataStreamReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html\" target=\"_blank\">DataStreamWriter</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html\" target=\"_blank\">StreamingQuery</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1e8ea50-7bea-4687-ab7c-e67c2660a821"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"caf0c6b9-ca1b-4cca-a27f-34d2619abf65"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Read data stream\n- Set to process 1 file per trigger\n- Read from Delta files in the source directory specified by **`DA.paths.sales`**\n\nAssign the resulting DataFrame to **`df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2a86a2f-9665-4bf1-a48b-2e60bec08c50"}}},{"cell_type":"code","source":["# ANSWER\ndf = (spark\n      .readStream\n      .option(\"maxFilesPerTrigger\", 1)\n      .format(\"delta\")\n      .load(DA.paths.sales)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c1adb89-91e2-4da4-897a-97d5dd2103b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ed948d6-1149-4b27-94c0-cf73734dd4c5"}}},{"cell_type":"code","source":["assert df.isStreaming\nassert df.columns == [\"order_id\", \"email\", \"transaction_timestamp\", \"total_item_quantity\", \"purchase_revenue_in_usd\", \"unique_items\", \"items\"]\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb78df18-c1c8-450a-928c-a596bdf63406"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Filter for transactions with coupon codes\n- Explode the **`items`** field in **`df`** with the results replacing the existing **`items`** field\n- Filter for records where **`items.coupon`** is not null\n\nAssign the resulting DataFrame to **`coupon_sales_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98b664ec-668f-4335-a049-5c13eeccf120"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, explode\n\ncoupon_sales_df = (df\n                   .withColumn(\"items\", explode(col(\"items\")))\n                   .filter(col(\"items.coupon\").isNotNull())\n                  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7bce3ae-2492-48d1-92c8-ec5ae7a06a5c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b76b0ba0-095e-42e3-aecb-500962983c48"}}},{"cell_type":"code","source":["schema_str = str(coupon_sales_df.schema)\nassert \"StructField(items,StructType(List(StructField(coupon\" in schema_str, \"items column was not exploded\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2403d213-ff38-4b47-ad68-576a7316c165"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Write streaming query results to Delta\n- Configure the streaming query to write Delta format files in \"append\" mode\n- Set the query name to \"coupon_sales\"\n- Set a trigger interval of 1 second\n- Set the checkpoint location to **`coupons_checkpoint_path`**\n- Set the output path to **`coupons_output_path`**\n\nStart the streaming query and assign the resulting handle to **`coupon_sales_query`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"744def6e-c98c-489a-afaf-b0ec58f60e1f"}}},{"cell_type":"code","source":["# ANSWER\n\ncoupons_checkpoint_path = f\"{DA.paths.checkpoints}/coupon-sales\"\ncoupons_output_path = f\"{DA.paths.working_dir}/coupon-sales/output\"\n\ncoupon_sales_query = (coupon_sales_df\n                      .writeStream\n                      .outputMode(\"append\")\n                      .format(\"delta\")\n                      .queryName(\"coupon_sales\")\n                      .trigger(processingTime=\"1 second\")\n                      .option(\"checkpointLocation\", coupons_checkpoint_path)\n                      .start(coupons_output_path)\n                     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6b0f624-cda0-4377-9c3b-1f4163a1cbee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41df8eaf-d1c8-4ac9-bad1-4286e6a9245e"}}},{"cell_type":"code","source":["DA.block_until_stream_is_ready(\"coupon_sales\")\nassert coupon_sales_query.isActive\nassert len(dbutils.fs.ls(coupons_output_path)) > 0\nassert len(dbutils.fs.ls(coupons_checkpoint_path)) > 0\nassert \"coupon_sales\" in coupon_sales_query.lastProgress[\"name\"]\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e90ae4e5-aa9e-4f3f-81d1-d2e29d7b118a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Monitor streaming query\n- Get the ID of streaming query and store it in **`queryID`**\n- Get the status of streaming query and store it in **`queryStatus`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7afa486-1de4-42dc-80d7-6244008c14d9"}}},{"cell_type":"code","source":["# ANSWER\nquery_id = coupon_sales_query.id\nprint(query_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be8f6a41-f16f-4d55-8f7b-4d2944b9b3a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nquery_status = coupon_sales_query.status\nprint(query_status)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5f714ed-b10f-42a6-9217-1a6d63be66de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d742785b-a5cd-4c64-a7c4-6909405026be"}}},{"cell_type":"code","source":["assert type(query_id) == str\nassert list(query_status.keys()) == [\"message\", \"isDataAvailable\", \"isTriggerActive\"]\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb48ea01-42aa-43ea-bac2-d2497a14be62"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. Stop streaming query\n- Stop the streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eff21573-f3a7-4375-900a-c69431d72202"}}},{"cell_type":"code","source":["# ANSWER\ncoupon_sales_query.stop()\ncoupon_sales_query.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf189a3-6891-4fa9-bfa9-5a719eca8b5c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**5.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c60812a3-c6e4-492b-9e0a-a54e5b5cf556"}}},{"cell_type":"code","source":["assert not coupon_sales_query.isActive\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3334a560-6858-4dcf-a1b4-a3a16373feba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6. Verify the records were written in Delta format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4de8e42f-feb9-4a5a-8b52-df7b8208af59"}}},{"cell_type":"code","source":["# ANSWER\ndisplay(spark.read.format(\"delta\").load(coupons_output_path))\n\n%md\n## Hourly Activity by Traffic Lab\nProcess streaming data to display the total active users by traffic source with a 1 hour window.\n1. Cast to timestamp and add watermark for 2 hours\n2. Aggregate active users by traffic source for 1 hour windows\n3. Execute query with **`display`** and plot results\n5. Use query name to stop streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cbd6a85-06ea-48e9-a8a2-52bb0e6f73d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Setup\nRun the cells below to generate hourly JSON files of event data for July 3, 2020."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0286de19-ac1c-490a-85d2-8c353c388be9"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4746e55-3259-4d48-8e2e-0184bc5f3729"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\n# Directory of hourly events logged from the BedBricks website on July 3, 2020\nhourly_events_path = f\"{DA.paths.datasets}/ecommerce/events/events-2020-07-03.json\"\n\ndf = (spark\n      .readStream\n      .schema(schema)\n      .option(\"maxFilesPerTrigger\", 1)\n      .json(hourly_events_path)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"250ea00b-5ce3-4e10-9f22-98345ad33cfa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Cast to timestamp and add watermark for 2 hours\n- Add a **`createdAt`** column by dividing **`event_timestamp`** by 1M and casting to timestamp\n- Set a watermark of 2 hours on the **`createdAt`** column\n\nAssign the resulting DataFrame to **`events_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"939c146d-5db5-4204-a7ca-8070eb0475ff"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\nevents_df = (df\n             .withColumn(\"createdAt\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n             .withWatermark(\"createdAt\", \"2 hours\")\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb762813-7240-4bc0-b63c-7633a9e71ddd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b45ec89a-62b3-4648-811a-e51c4aa99450"}}},{"cell_type":"code","source":["assert \"StructField(createdAt,TimestampType,true\" in str(events_df.schema)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa11583-16ae-4e95-93cb-aaa89187ff81"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Aggregate active users by traffic source for 1 hour windows\n\n- Set the default shuffle partitions to the number of cores on your cluster\n- Group by **`traffic_source`** with 1-hour tumbling windows based on the **`createdAt`** column\n- Aggregate the approximate count of distinct users per **`user_id`** and alias the resulting column to **`active_users`**\n- Select **`traffic_source`**, **`active_users`**, and the **`hour`** extracted from **`window.start`** with an alias of **`hour`**\n- Sort by **`hour`** in ascending order\nAssign the resulting DataFrame to **`traffic_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e18d4413-d7d5-4282-8d6a-a43f21a02643"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import approx_count_distinct, hour, window\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n\ntraffic_df = (events_df\n              .groupBy(\"traffic_source\", window(col(\"createdAt\"), \"1 hour\"))\n              .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n              .select(col(\"traffic_source\"), col(\"active_users\"), hour(col(\"window.start\")).alias(\"hour\"))\n              .sort(\"hour\")\n             )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06139a2e-e2f5-47ea-bffd-d0e8c40a2e07"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dea1fc10-1683-4217-a87c-767a4db63663"}}},{"cell_type":"code","source":["assert str(traffic_df.schema) == \"StructType(List(StructField(traffic_source,StringType,true),StructField(active_users,LongType,false),StructField(hour,IntegerType,true)))\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b63d303-3caf-4319-824c-98f3d0a973ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Execute query with display() and plot results\n- Use **`display`** to start **`traffic_df`** as a streaming query and display the resulting memory sink\n  - Assign \"hourly_traffic\" as the name of the query by setting the **`streamName`** parameter of **`display`**\n- Plot the streaming query results as a bar graph\n- Configure the following plot options:\n  - Keys: **`hour`**\n  - Series groupings: **`traffic_source`**\n  - Values: **`active_users`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e72ec614-9bef-4082-af2b-b8ceccb7475b"}}},{"cell_type":"code","source":["# ANSWER\ndisplay(traffic_df, streamName=\"hourly_traffic\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e700bb04-af25-48a1-8808-e2b267d7290e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**\n\n- The bar chart should plot **`hour`** on the x-axis and **`active_users`** on the y-axis\n- Six bars should appear at every hour for all traffic sources\n- The chart should stop at hour 23"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"175d2943-1bda-4e17-8d93-c9c03d4a5dbf"}}},{"cell_type":"markdown","source":["### 4. Manage streaming query\n- Iterate over SparkSession's list of active streams to find one with name \"hourly_traffic\"\n- Stop the streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e100ce62-d9e5-46c0-8a58-0070ee1cf6af"}}},{"cell_type":"code","source":["# ANSWER\nDA.block_until_stream_is_ready(\"hourly_traffic\")\n\nfor s in spark.streams.active:\n    if s.name == \"hourly_traffic\":\n        s.stop()\n        s.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"049c8ed7-3c3c-42ad-ab67-525af2a73bab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**\nPrint all active streams to check that \"hourly_traffic\" is no longer there"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3814687b-0243-430e-bbad-0c857ce298f6"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(s.name)\n\n%md\n# Activity by Traffic Lab\nProcess streaming data to display total active users by traffic source.\n\n##### Objectives\n1. Read data stream\n2. Get active users by traffic source\n3. Execute query with display() and plot results\n4. Execute the same streaming query with DataStreamWriter\n5. View results being updated in the query table\n6. List and stop all active streams\n\n##### Classes\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html\" target=\"_blank\">DataStreamReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html\" target=\"_blank\">DataStreamWriter</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html\" target=\"_blank\">StreamingQuery</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e75361-e230-4b8b-94fc-34320483588b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Setup\nRun the cells below to generate data and create the **`schema`** string needed for this lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3da49a3b-e962-4e72-9859-811601c0bcf0"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ca37107-a104-4aad-878c-b34a7ff57623"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Read data stream\n- Set to process 1 file per trigger\n- Read from Delta with filepath stored in **`DA.paths.events`**\n\nAssign the resulting DataFrame to **`df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af0205ca-2ec7-4564-86b0-879c04afad98"}}},{"cell_type":"code","source":["# ANSWER\ndf = (spark\n      .readStream\n      .option(\"maxFilesPerTrigger\", 1)\n      .format(\"delta\")\n      .load(DA.paths.events)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e49b5e2e-5c2a-4a0e-b95e-49c5460d3a1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea0796dc-5d59-4ad2-941a-0000ca091a13"}}},{"cell_type":"code","source":["assert df.isStreaming\nassert df.columns == [\"device\", \"ecommerce\", \"event_name\", \"event_previous_timestamp\", \"event_timestamp\", \"geo\", \"items\", \"traffic_source\", \"user_first_touch_timestamp\", \"user_id\"]\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b096da99-06f4-48da-b314-5169c97cf858"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get active users by traffic source\n- Set default shuffle partitions to number of cores on your cluster (not required, but runs faster)\n- Group by **`traffic_source`**\n  - Aggregate the approximate count of distinct users and alias with \"active_users\"\n- Sort by **`traffic_source`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0a8bbcd-dca5-4d6e-9be4-748eef07bf6a"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, approx_count_distinct, count\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n\ntraffic_df = (df\n              .groupBy(\"traffic_source\")\n              .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n              .sort(\"traffic_source\")\n             )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f460757-8a0f-4ead-97cf-fa8b51c110de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eef425c4-bd05-4712-83e8-c8d80ef2e6ee"}}},{"cell_type":"code","source":["assert str(traffic_df.schema) == \"StructType(List(StructField(traffic_source,StringType,true),StructField(active_users,LongType,false)))\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89455c96-555a-4c2c-a26e-e28440ebab06"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Execute query with display() and plot results\n- Execute results for **`traffic_df`** using display()\n- Plot the streaming query results as a bar graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1e76d5d-163d-48f1-a6e6-ac75324a6e42"}}},{"cell_type":"code","source":["# ANSWER\ndisplay(traffic_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73bb772e-bd18-4001-b5c0-31cdf3e3913b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**\n- You bar chart should plot **`traffic_source`** on the x-axis and **`active_users`** on the y-axis\n- The top three traffic sources in descending order should be **`google`**, **`facebook`**, and **`instagram`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c156f0b1-a25f-443d-b63f-25a6a7288394"}}},{"cell_type":"markdown","source":["### 4. Execute the same streaming query with DataStreamWriter\n- Name the query \"active_users_by_traffic\"\n- Set to \"memory\" format and \"complete\" output mode\n- Set a trigger interval of 1 second"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7646f2c5-401b-44d1-af84-4a19fdb159e7"}}},{"cell_type":"code","source":["# ANSWER\ntraffic_query = (traffic_df\n                 .writeStream\n                 .queryName(\"active_users_by_traffic\")\n                 .format(\"memory\")\n                 .outputMode(\"complete\")\n                 .trigger(processingTime=\"1 second\")\n                 .start()\n                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6b7e45f-ab7d-43db-bf4c-0c5b8fff6f48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09e3e524-57de-4b1d-82c1-f7006f5b38c3"}}},{"cell_type":"code","source":["DA.block_until_stream_is_ready(\"active_users_by_traffic\")\nassert traffic_query.isActive\nassert \"active_users_by_traffic\" in traffic_query.name\nassert traffic_query.lastProgress[\"sink\"][\"description\"] == \"MemorySink\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"241d394b-6c14-435e-b496-181064457e68"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. View results being updated in the query table\nRun a query in a SQL cell to display the results from the **`active_users_by_traffic`** table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8154e415-0a90-4c42-9352-fd001decf454"}}},{"cell_type":"code","source":["%sql\n-- ANSWER\nSELECT * FROM active_users_by_traffic"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7250b183-a619-432b-869e-3f174b6226bb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**5.1: CHECK YOUR WORK**\nYour query should eventually result in the following values.\n\n|traffic_source|active_users|\n|---|---|\n|direct|438886|\n|email|281525|\n|facebook|956769|\n|google|1781961|\n|instagram|530050|\n|youtube|253321|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9298bb3e-eee8-4a87-8c96-eb094ba327e3"}}},{"cell_type":"markdown","source":["### 6. List and stop all active streams\n- Use SparkSession to get list of all active streams\n- Iterate over the list and stop each query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94cf9994-a872-4e50-9727-22346e62b259"}}},{"cell_type":"code","source":["# ANSWER\nfor s in spark.streams.active:\n    print(s.name)\n    s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80448546-560f-4e4f-b95e-e230c164f6b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**6.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f15487d4-f68f-4acf-a5c3-a2a1c2e36367"}}},{"cell_type":"code","source":["assert not traffic_query.isActive\nprint(\"All test pass\")\n\n%md # Delta Lake\n\n##### Objectives\n1. Create a Delta Table\n1. Understand the transaction Log\n1. Read data from your Delta Table\n1. Update data in your Delta Table\n1. Access previous versions of table using time travel\n1. Vacuum\n\n##### Documentation\n- <a href=\"https://docs.delta.io/latest/quick-start.html#create-a-table\" target=\"_blank\">Delta Table</a> \n- <a href=\"https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html\" target=\"_blank\">Transaction Log</a> \n- <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\" target=\"_blank\">Time Travel</a> "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab615ae-a93d-4427-b41d-4dae0c553888"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13cd54b-eec1-4b3b-a02f-8e35da2d082b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Create a Delta Table\nLet's first read the Parquet-format BedBricks events dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"127c269a-fe21-4390-a5b6-c827a522b07a"}}},{"cell_type":"code","source":["events_df = spark.read.format(\"parquet\").load(f\"{DA.paths.datasets}/ecommerce/events/events.parquet\")\ndisplay(events_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23e172b7-a7a1-48af-a273-3c453613687c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Write the data in Delta format to the directory given by **`delta_path`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2faead4c-077e-481a-91ad-501c9a08321f"}}},{"cell_type":"code","source":["delta_path = f\"{DA.paths.working_dir}/delta-events\"\nevents_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5a946d4-fcf7-4a74-88d9-5f0e923ec257"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Write the data in Delta format as a managed table in the metastore."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9729b17c-c515-4d42-960a-a6dfb39bb9d7"}}},{"cell_type":"code","source":["events_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_events\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f16e203a-3e53-418f-9ad7-b1d18b7644ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As with other file formats, Delta supports partitioning your data in storage using the unique values in a specified column (often referred to as \"Hive partitioning\").\n\nLet's **overwrite** the Delta dataset in the **`delta_path`** directory to partition by state. This can accelerate queries that filter by state."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cca9566-b3cd-4c7e-82a2-60bf70fd73f2"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nstate_events_df = events_df.withColumn(\"state\", col(\"geo.state\"))\n\nstate_events_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").option(\"overwriteSchema\", \"true\").save(delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63bc2100-5675-49ec-b54d-fd7fe0b3d40f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Understand the Transaction Log\nWe can see how Delta stores the different state partitions in separate directories.\n\nAdditionally, we can also see a directory called **`_delta_log`**, which is the transaction log.\n\nWhen a Delta Lake dataset is created, its transaction log is automatically created in the **`_delta_log`** subdirectory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"871217d2-2ef5-4fe5-bd35-0d7730f5c324"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(delta_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fb8e97c-710a-41a0-8466-0bd99652ff05"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When changes are made to that table, these changes are recorded as ordered, atomic commits in the transaction log.\n\nEach commit is written out as a JSON file, starting with 00000000000000000000.json.\n\nAdditional changes to the table generate subsequent JSON files in ascending numerical order.\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://user-images.githubusercontent.com/20408077/87174138-609fe600-c29c-11ea-90cc-84df0c1357f1.png\" width=\"500\"/>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a818b78-eee4-4f15-b4b3-9ac2c90b2699"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(f\"{delta_path}/_delta_log/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"464639af-cddf-4652-9656-710083402b85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, let's take a look at a transaction log File.\n\n\nThe <a href=\"https://docs.databricks.com/delta/delta-utility.html\" target=\"_blank\">four columns</a> each represent a different part of the very first commit to the Delta Table, creating the table.\n- The **`add`** column has statistics about the DataFrame as a whole and individual columns.\n- The **`commitInfo`** column has useful information about what the operation was (WRITE or READ) and who executed the operation.\n- The **`metaData`** column contains information about the column schema.\n- The **`protocol`** version contains information about the minimum Delta version necessary to either write or read to this Delta Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d38974b5-d3d9-4f6a-9aae-b416599f1d6d"}}},{"cell_type":"code","source":["display(spark.read.json(f\"{delta_path}/_delta_log/00000000000000000000.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fe4dcd2-2c60-4869-9e45-60c17818b482"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One key difference between these two transaction logs is the size of the JSON file, this file has 206 rows compared to the previous 7.\n\nTo understand why, let's take a look at the **`commitInfo`** column. We can see that in the **`operationParameters`** section, **`partitionBy`** has been filled in by the **`state`** column. Furthermore, if we look at the add section on row 3, we can see that a new section called **`partitionValues`** has appeared. As we saw above, Delta stores partitions separately in memory, however, it stores information about these partitions in the same transaction log file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c71f348-d16b-4978-a9b5-821eb7c61d87"}}},{"cell_type":"code","source":["display(spark.read.json(f\"{delta_path}/_delta_log/00000000000000000001.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06870be7-28c9-4dac-8be1-378fe1628058"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, let's take a look at the files inside one of the state partitions. The files inside corresponds to the partition commit (file 01) in the _delta_log directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"268c4905-9794-438d-8251-c4d57eb76e07"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(f\"{delta_path}/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df7a6a32-5298-4575-8f69-589e22b9b926"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read from your Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6863938d-fd57-4ad4-bbf0-24c81e6a9397"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(delta_path)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97dcd049-41f2-42b7-87de-6ce1199db399"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Update your Delta Table\n\nLet's filter for rows where the event takes place on a mobile device."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03d7ce34-d389-4709-97e3-3c961dc040f0"}}},{"cell_type":"code","source":["df_update = state_events_df.filter(col(\"device\").isin([\"Android\", \"iOS\"]))\ndisplay(df_update)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49cf99c1-ac06-4afc-bc2e-0bcf76ea66b6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_update.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d85c0692-8890-443d-b6a1-b61d8130a160"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(delta_path)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e74438a-8383-49fb-be40-6eabf907a96b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the files in the California partition post-update. Remember, the different files in this directory are snapshots of your DataFrame corresponding to different commits."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f3f6c0c-5fe3-4d09-b6aa-710b52531258"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(f\"{delta_path}/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a942389c-70d2-4c94-a4f8-25dadbb670c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Access previous versions of table using Time  Travel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3609eaf-fdd8-445a-b47a-6afa391fdca1"}}},{"cell_type":"markdown","source":["Oops, it turns out we actually we need the entire dataset! You can access a previous version of your Delta Table using Time Travel. Use the following two cells to access your version history. Delta Lake will keep a 30 day version history by default, but if necessary, Delta can store a version history for longer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f4c9203-aba5-4b7d-b645-d71ddc14ece7"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS train_delta\")\nspark.sql(f\"CREATE TABLE train_delta USING DELTA LOCATION '{delta_path}'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0238d8cb-9255-44bf-833e-4ca17fcb94ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY train_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f362f2b3-4b41-40af-925c-f05c436d03ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Using the **`versionAsOf`** option allows you to easily access previous versions of our Delta Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80142a57-fa35-4e86-a8d1-4216fd234c74"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64b18a76-0976-4ff7-82d0-baa434257659"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also access older versions using a timestamp.\n\nReplace the timestamp string with the information from your version history. \n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"> Note: You can use a date without the time information if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf40d8a-d462-486e-83f5-653f20e0f365"}}},{"cell_type":"code","source":["# ANSWER\n\ntemp_df = spark.sql(\"DESCRIBE HISTORY train_delta\").select(\"timestamp\").orderBy(col(\"timestamp\").asc())\ntime_stamp = temp_df.first()[\"timestamp\"]\n\nas_of_df = spark.read.format(\"delta\").option(\"timestampAsOf\", time_stamp).load(delta_path)\ndisplay(as_of_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a9fa2c8-7567-41db-b835-3a2ce0b7fc08"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Vacuum \n\nNow that we're happy with our Delta Table, we can clean up our directory using **`VACUUM`**. Vacuum accepts a retention period in hours as an input."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be7f49a8-dd23-47ee-8a36-13b91317c4bb"}}},{"cell_type":"markdown","source":["It looks like our code doesn't run! By default, to prevent accidentally vacuuming recent commits, Delta Lake will not let users vacuum a period under 7 days or 168 hours. Once vacuumed, you cannot return to a prior commit through time travel, only your most recent Delta Table will be saved."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11000ea2-dbf1-48fb-b74b-8111c4cce66a"}}},{"cell_type":"code","source":["# from delta.tables import *\n\n# delta_table = DeltaTable.forPath(spark, delta_path)\n# delta_table.vacuum(0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57a392fc-5561-4f8a-9822-28f0425d08c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can workaround this by setting a spark configuration that will bypass the default retention period check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7c40f2e-4ddb-4e87-b71a-fb9edf2436d8"}}},{"cell_type":"code","source":["from delta.tables import *\n\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\ndelta_table = DeltaTable.forPath(spark, delta_path)\ndelta_table.vacuum(0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e72ec3b-34e5-435d-b944-d5637ec36c46"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at our Delta Table files now. After vacuuming, the directory only holds the partition of our most recent Delta Table commit."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58074e03-1c35-4a59-a1b6-33279be80fdd"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(delta_path + \"/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5bfbf89-8925-4c4a-a869-fafec1792ef3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Since vacuuming deletes files referenced by the Delta Table, we can no longer access past versions. \n\nThe code below should throw an error.\n\nUncomment it and give it a try."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d7ae730-eb79-4d70-964c-bb0a6b4fb24c"}}},{"cell_type":"code","source":["# df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n# display(df)\n\n%md # Delta Lake Lab\n##### Tasks\n1. Write sales data to Delta\n1. Modify sales data to show item count instead of item array\n1. Rewrite sales data to same Delta path\n1. Create table and view version history\n1. Time travel to read previous version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"181d9b31-9c7f-450e-b8cd-5f42452f20f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90210fed-c93d-437b-850e-4568bfff2604"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sales_df = spark.read.parquet(f\"{DA.paths.datasets}/ecommerce/sales/sales.parquet\")\ndelta_sales_path = f\"{DA.paths.working_dir}/delta-sales\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec2d270f-9474-4ade-a798-9ee9ddd4ffaa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Write sales data to Delta\nWrite **`sales_df`** to **`delta_sales_path`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31c265b9-b99e-46f4-b2a4-72d5c3a1feb8"}}},{"cell_type":"code","source":["# ANSWER\nsales_df.write.format(\"delta\").mode(\"overwrite\").save(delta_sales_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b76aba5a-f48f-49f0-9f89-f5cfe83daef1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43d99597-bc95-4b56-8e64-d734b8b7a4ec"}}},{"cell_type":"code","source":["assert len(dbutils.fs.ls(delta_sales_path)) > 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd8275ad-1c09-4adf-a10d-71124e89d6f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Modify sales data to show item count instead of item array\nReplace values in the **`items`** column with an integer value of the items array size.\nAssign the resulting DataFrame to **`updated_sales_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d0231e0-96e3-4fb7-a0ce-c8d63a289e2d"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import size, col\n\nupdated_sales_df = sales_df.withColumn(\"items\", size(col(\"items\")))\ndisplay(updated_sales_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08e159c6-40d6-4acd-8327-db6be0732077"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66a3ff17-bf52-4559-9bfb-9b987ab5735d"}}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\n\nassert updated_sales_df.schema[6].dataType == IntegerType()\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb825196-d41b-4410-a76a-03b1ee77dba0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Rewrite sales data to same Delta path\nWrite **`updated_sales_df`** to the same Delta location **`delta_sales_path`**.\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> This will fail without an option to overwrite the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ca9c436-1c72-46e7-819b-72dbffe313cd"}}},{"cell_type":"code","source":["# ANSWER\nupdated_sales_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_sales_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a6d2db6-c077-4800-a46b-23d9e15e056a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6278644-ebd0-4573-a86b-78b68296c48d"}}},{"cell_type":"code","source":["assert spark.read.format(\"delta\").load(delta_sales_path).schema[6].dataType == IntegerType()\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7e97695-d67d-441d-857a-906df01e8024"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Create table and view version history\nRun SQL queries by writing SQL inside of `spark.sql()` to perform the following steps.\n- Drop table **`sales_delta`** if it exists\n- Create **`sales_delta`** table using the **`delta_sales_path`** location\n- List version history for the **`sales_delta`** table\n\nAn example of a SQL query inside of `spark.sql()` would be something like ```spark.sql(\"SELECT * FROM sales_data\")```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9258fcef-ecfd-4ba5-97f4-9bd4f0bbde2e"}}},{"cell_type":"code","source":["# ANSWER\nspark.sql(\"DROP TABLE IF EXISTS sales_delta\")\nspark.sql(\"CREATE TABLE sales_delta USING DELTA LOCATION '{}'\".format(delta_sales_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"973215a9-ebd5-4b94-b2ae-9a2861e50a0a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\ndisplay(spark.sql(\"DESCRIBE HISTORY sales_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a488622-deb0-46e8-803d-6c92017576ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc3a84b1-79a5-445d-a4d8-874145e3cfc6"}}},{"cell_type":"code","source":["sales_delta_df = spark.sql(\"SELECT * FROM sales_delta\")\nassert sales_delta_df.count() == 210370\nassert sales_delta_df.schema[6].dataType == IntegerType()\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93046cfd-1a84-4249-bf74-5ad63fc3a5f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. Time travel to read previous version\nRead delta table at **`delta_sales_path`** at version 0.\nAssign the resulting DataFrame to **`old_sales_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af5046de-ab4d-406e-9ab1-9e9dd8bd8ae3"}}},{"cell_type":"code","source":["# ANSWER\nold_sales_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_sales_path)\ndisplay(old_sales_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a06a71a0-87d9-4398-a2d0-25fe65657dc9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**5.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15dc3d8f-f728-4f83-9f42-a2f61524f4e8"}}},{"cell_type":"code","source":["assert old_sales_df.select(size(col(\"items\"))).first()[0] == 1\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19717f37-7b1a-43d5-b49e-4a18536d89cf"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_streaming_syntax","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":484899027341368}},"nbformat":4,"nbformat_minor":0}
