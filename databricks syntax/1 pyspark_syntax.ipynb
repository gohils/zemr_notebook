{"cells":[{"cell_type":"markdown","source":["# Reader & Writer\n##### Objectives\n1. Read from CSV files\n1. Read from JSON files\n1. Write DataFrame to files\n1. Write DataFrame to tables\n1. Write DataFrame to a Delta table\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html\" target=\"_blank\">DataFrameReader</a>: **`csv`**, **`json`**, **`option`**, **`schema`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html\" target=\"_blank\">DataFrameWriter</a>: **`mode`**, **`option`**, **`parquet`**, **`format`**, **`saveAsTable`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html?highlight=structtype#pyspark.sql.types.StructType\" target=\"_blank\">StructType</a>: **`toDDL`**\n\n##### Spark Types\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\" target=\"_blank\">Types</a>: **`ArrayType`**, **`DoubleType`**, **`IntegerType`**, **`LongType`**, **`StringType`**, **`StructType`**, **`StructField`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11acaf3e-85fb-4c69-a058-3a243cafca7f"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07450b19-2ed1-48e8-8171-a9191bc5c686"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrameReader\nInterface used to load a DataFrame from external storage systems\n\n**`spark.read.parquet(\"path/to/files\")`**\n\nDataFrameReader is accessible through the SparkSession attribute **`read`**. This class includes methods to load DataFrames from different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"331d954b-ebfc-4df7-bfdf-d798b74ac963"}}},{"cell_type":"markdown","source":["### Read from CSV files\nRead from CSV with the DataFrameReader's **`csv`** method and the following options:\n\nTab separator, use first line as header, infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8249b378-efe8-45d2-b930-35990265f6ca"}}},{"cell_type":"code","source":["users_csv_path = f\"{DA.paths.datasets}/ecommerce/users/users-500k.csv\"\n\nusers_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .option(\"inferSchema\", True)\n           .csv(users_csv_path)\n          )\n\nusers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f85bdbd4-fc3f-4acc-a3d6-997bb2d24b0c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark's Python API also allows you to specify the DataFrameReader options as parameters to the **`csv`** method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4663dccc-c6c4-44c6-abb2-778fb67da38f"}}},{"cell_type":"code","source":["users_df = (spark\n           .read\n           .csv(users_csv_path, sep=\"\\t\", header=True, inferSchema=True)\n          )\n\nusers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c02c0ba-24d8-4d6f-932e-75882452431e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Manually define the schema by creating a **`StructType`** with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1627165-abae-4f76-9eb4-19a9a58cba0e"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType, StringType, StructType, StructField\n\nuser_defined_schema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"email\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5e3f2ca-3640-43d3-9703-b472ecba61fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read from CSV using this user-defined schema instead of inferring the schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3adbf4bb-d044-42b0-ba3f-1b92235b2845"}}},{"cell_type":"code","source":["users_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(user_defined_schema)\n           .csv(users_csv_path)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b0c862e-04ee-4391-be1b-a5e7a3f26f1f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternatively, define the schema using <a href=\"https://en.wikipedia.org/wiki/Data_definition_language\" target=\"_blank\">data definition language (DDL)</a> syntax."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e415b686-fc0e-4fbb-887c-3042bc5eb58d"}}},{"cell_type":"code","source":["ddl_schema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusers_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(ddl_schema)\n           .csv(users_csv_path)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d60b34c8-3baf-4292-8bfd-5a960822c193"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read from JSON files\n\nRead from JSON with DataFrameReader's **`json`** method and the infer schema option"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b86e01c8-2558-4663-a0d7-d3bc987f2fb2"}}},{"cell_type":"code","source":["events_json_path = f\"{DA.paths.datasets}/ecommerce/events/events-500k.json\"\n\nevents_df = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .json(events_json_path)\n           )\n\nevents_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"590a83e9-b340-449e-8c6d-f6cdcf7a89a9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read data faster by creating a **`StructType`** with the schema names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67abdfb1-260f-4b65-9b80-0f2a2c625a29"}}},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuser_defined_schema = StructType([\n    StructField(\"device\", StringType(), True),\n    StructField(\"ecommerce\", StructType([\n        StructField(\"purchaseRevenue\", DoubleType(), True),\n        StructField(\"total_item_quantity\", LongType(), True),\n        StructField(\"unique_items\", LongType(), True)\n    ]), True),\n    StructField(\"event_name\", StringType(), True),\n    StructField(\"event_previous_timestamp\", LongType(), True),\n    StructField(\"event_timestamp\", LongType(), True),\n    StructField(\"geo\", StructType([\n        StructField(\"city\", StringType(), True),\n        StructField(\"state\", StringType(), True)\n    ]), True),\n    StructField(\"items\", ArrayType(\n        StructType([\n            StructField(\"coupon\", StringType(), True),\n            StructField(\"item_id\", StringType(), True),\n            StructField(\"item_name\", StringType(), True),\n            StructField(\"item_revenue_in_usd\", DoubleType(), True),\n            StructField(\"price_in_usd\", DoubleType(), True),\n            StructField(\"quantity\", LongType(), True)\n        ])\n    ), True),\n    StructField(\"traffic_source\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"user_id\", StringType(), True)\n])\n\nevents_df = (spark\n            .read\n            .schema(user_defined_schema)\n            .json(events_json_path)\n           )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87638e1c-8eaa-412f-a538-ff756c8c6387"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can use the **`StructType`** Scala method **`toDDL`** to have a DDL-formatted string created for you.\n\nThis is convenient when you need to get the DDL-formated string for ingesting CSV and JSON but you don't want to hand craft it or the **`StructType`** variant of the schema.\n\nHowever, this functionality is not available in Python but the power of the notebooks allows us to use both languages."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"575b7cd5-ac1e-4bae-99a8-60092398965d"}}},{"cell_type":"code","source":["# Step 1 - use this trick to transfer a value (the dataset path) between Python and Scala using the shared spark-config\nspark.conf.set(\"whatever_your_scope.events\", events_json_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ae71fca-5d65-492a-9dc7-ba715ab35241"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In a Python notebook like this one, create a Scala cell to injest the data and produce the DDL formatted schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f178feae-5e72-45ee-aa1d-4e93d3d1bd63"}}},{"cell_type":"code","source":["%scala\n// Step 2 - pull the value from the config (or copy & paste it)\nval eventsJsonPath = spark.conf.get(\"whatever_your_scope.events\")\n\n// Step 3 - Read in the JSON, but let it infer the schema\nval eventsSchema = spark.read\n                        .option(\"inferSchema\", true)\n                        .json(eventsJsonPath)\n                        .schema.toDDL\n\n// Step 4 - print the schema, select it, and copy it.\nprintln(\"=\"*80)\nprintln(eventsSchema)\nprintln(\"=\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"231a3212-60dd-4738-b938-fc72db6b4ee5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Step 5 - paste the schema from above and assign it to a variable as seen here\nevents_schema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n\n# Step 6 - Read in the JSON data using our new DDL formatted string\nevents_df = (spark.read\n                 .schema(events_schema)\n                 .json(events_json_path))\n\ndisplay(events_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cb90f81-65a0-4775-b054-f4eeea4c5eb5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This is a great \"trick\" for producing a schema for a net-new dataset and for accelerating development.\n\nWhen you are done (e.g. for Step #7), make sure to delete your temporary code.\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> WARNING: **Do not use this trick in production**</br>\nthe inference of a schema can be REALLY slow as it<br/>\nforces a full read of the source dataset to infer the schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95bfe4c3-35c4-4524-a605-426e475fe52e"}}},{"cell_type":"markdown","source":["## DataFrameWriter\nInterface used to write a DataFrame to external storage systems\n\n<strong><code>\n(df  \n&nbsp;  .write                         \n&nbsp;  .option(\"compression\", \"snappy\")  \n&nbsp;  .mode(\"overwrite\")      \n&nbsp;  .parquet(output_dir)       \n)\n</code></strong>\n\nDataFrameWriter is accessible through the SparkSession attribute **`write`**. This class includes methods to write DataFrames to different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57be408a-6eae-4294-b276-dcd54d1d4537"}}},{"cell_type":"markdown","source":["### Write DataFrames to files\n\nWrite **`users_df`** to parquet with DataFrameWriter's **`parquet`** method and the following configurations:\n\nSnappy compression, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e70d9658-72de-4225-94db-c48e79de47d0"}}},{"cell_type":"code","source":["users_output_dir = f\"{DA.paths.working_dir}/users.parquet\"\n\n(users_df\n .write\n .option(\"compression\", \"snappy\")\n .mode(\"overwrite\")\n .parquet(users_output_dir)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6516936d-1010-4c37-8a1b-0fce69c2a9d0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(\n    dbutils.fs.ls(users_output_dir)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b846691c-6bd9-4aee-80b0-88a09af66293"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the **`parquet`** method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ff430ed-9438-4b94-8d25-cab8991c6abc"}}},{"cell_type":"code","source":["(users_df\n .write\n .parquet(users_output_dir, compression=\"snappy\", mode=\"overwrite\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"645b40ca-62b4-477c-85d2-d526e899ba25"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write DataFrames to tables\n\nWrite **`events_df`** to a table using the DataFrameWriter method **`saveAsTable`**\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This creates a global table, unlike the local view created by the DataFrame method **`createOrReplaceTempView`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c81f1d1d-0830-471f-a2b8-9c2fc4799f79"}}},{"cell_type":"code","source":["events_df.write.mode(\"overwrite\").saveAsTable(\"events\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bc6ebbe-0da2-4372-87b8-80d1299f6a4a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This table was saved in the database created for you in classroom setup.\n\nSee database name printed below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89ce23f4-b9dd-43a1-be4a-42cb0a10b95e"}}},{"cell_type":"code","source":["print(f\"Database Name: {DA.db_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5ecfe4b-f427-4c5a-b9e5-f9067ea9528e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["... or even the tables in that database:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9a65e63-8c59-472d-acca-91b9b0d49c95"}}},{"cell_type":"code","source":["%sql\nSHOW TABLES IN ${DA.db_name}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73ae9922-a225-4ee4-83a0-aa6f90184d7d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Delta Lake\n\nIn almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. \n\n<a href=\"https://delta.io/\" target=\"_blank\">Delta Lake</a> is an open source technology designed to work with Spark to bring reliability to data lakes.\n\n![delta](https://files.training.databricks.com/images/aspwd/delta_storage_layer.png)\n\n#### Delta Lake's Key Features\n- ACID transactions\n- Scalable metadata handling\n- Unified streaming and batch processing\n- Time travel (data versioning)\n- Schema enforcement and evolution\n- Audit history\n- Parquet format\n- Compatible with Apache Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53236894-92d6-4f03-b12d-d92ee899ad4f"}}},{"cell_type":"markdown","source":["### Write Results to a Delta Table\n\nWrite **`events_df`** with the DataFrameWriter's **`save`** method and the following configurations: Delta format & overwrite mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"695efaa6-c79b-4486-a695-7c9767da736e"}}},{"cell_type":"code","source":["events_output_path = f\"{DA.paths.working_dir}/delta/events\"\n\n(events_df\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(events_output_path)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3642d5d-fa75-4ad0-9723-e596e801f410"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3426744b-8977-444c-b1bc-98775ff35a5f"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f417b657-887d-4942-a739-7076b83cc00d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Datetime Functions\n\n##### Objectives\n1. Cast to timestamp\n2. Format datetimes\n3. Extract from timestamp\n4. Convert to date\n5. Manipulate datetimes\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html\" target=\"_blank\">Column</a>: **`cast`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#datetime-functions\" target=\"_blank\">Built-In Functions</a>: **`date_format`**, **`to_date`**, **`date_add`**, **`year`**, **`month`**, **`dayofweek`**, **`minute`**, **`second`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6306043-0ca0-4bfb-bc36-fe940eb3f5b3"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab8c27d-cd90-4c7d-80ed-d5a9333d8353"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use a subset of the BedBricks events dataset to practice working with date times."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b17fe259-56bf-48b6-bbf4-c64a523d6574"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = spark.read.format(\"delta\").load(DA.paths.events).select(\"user_id\", col(\"event_timestamp\").alias(\"timestamp\"))\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbeddc4c-3b99-47e0-9dc1-300f6bdf8443"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Built-In Functions: Date Time Functions\nHere are a few built-in functions to manipulate dates and times in Spark.\n\n| Method | Description |\n| --- | --- |\n| **`add_months`** | Returns the date that is numMonths after startDate |\n| **`current_timestamp`** | Returns the current timestamp at the start of query evaluation as a timestamp column |\n| **`date_format`** | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n| **`dayofweek`** | Extracts the day of the month as an integer from a given date/timestamp/string |\n| **`from_unixtime`** | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format |\n| **`minute`** | Extracts the minutes as an integer from a given date/timestamp/string. |\n| **`unix_timestamp`** | Converts time string with given pattern to Unix timestamp (in seconds) |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9982e5e7-7620-4977-a57f-bec4593eedf8"}}},{"cell_type":"markdown","source":["### Cast to Timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c25837-9963-439f-8908-74a82b731425"}}},{"cell_type":"markdown","source":["#### **`cast()`**\nCasts column to a different data type, specified using string representation or DataType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93ecfe82-c83a-4b02-90d1-0d704d1c7a3d"}}},{"cell_type":"code","source":["timestamp_df = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(\"timestamp\"))\ndisplay(timestamp_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47788ec9-f230-4735-826b-d0b4e5892738"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\n\ntimestamp_df = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(TimestampType()))\ndisplay(timestamp_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dc512db-eac0-4fb6-8bd6-a2780f6370ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Datetime Patterns for Formatting and Parsing\nThere are several common scenarios for datetime usage in Spark:\n\n- CSV/JSON datasources use the pattern string for parsing and formatting datetime content.\n- Datetime functions related to convert StringType to/from DateType or TimestampType e.g. **`unix_timestamp`**, **`date_format`**, **`from_unixtime`**, **`to_date`**, **`to_timestamp`**, etc.\n\nSpark uses <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">pattern letters for date and timestamp parsing and formatting</a>. A subset of these patterns are shown below.\n\n| Symbol | Meaning         | Presentation | Examples               |\n| ------ | --------------- | ------------ | ---------------------- |\n| G      | era             | text         | AD; Anno Domini        |\n| y      | year            | year         | 2020; 20               |\n| D      | day-of-year     | number(3)    | 189                    |\n| M/L    | month-of-year   | month        | 7; 07; Jul; July       |\n| d      | day-of-month    | number(3)    | 28                     |\n| Q/q    | quarter-of-year | number/text  | 3; 03; Q3; 3rd quarter |\n| E      | day-of-week     | text         | Tue; Tuesday           |\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Spark's handling of dates and timestamps changed in version 3.0, and the patterns used for parsing and formatting these values changed as well. For a discussion of these changes, please reference <a href=\"https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html\" target=\"_blank\">this Databricks blog post</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1783ca43-552b-4997-910a-59a3bb7f798d"}}},{"cell_type":"markdown","source":["### Format date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae092140-f035-476b-b310-10496798dda2"}}},{"cell_type":"markdown","source":["#### **`date_format()`**\nConverts a date/timestamp/string to a string formatted with the given date time pattern."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5c026a7-1fac-46a9-b710-9a82e25bed6a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\n\nformatted_df = (timestamp_df\n                .withColumn(\"date string\", date_format(\"timestamp\", \"MMMM dd, yyyy\"))\n                .withColumn(\"time string\", date_format(\"timestamp\", \"HH:mm:ss.SSSSSS\"))\n               )\ndisplay(formatted_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a85fdb3-a483-45b9-be7f-3d4c6c1650c9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Extract datetime attribute from timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2621b287-2c83-4d90-8c6a-bc916cc68c2e"}}},{"cell_type":"markdown","source":["#### **`year`**\nExtracts the year as an integer from a given date/timestamp/string.\n\n##### Similar methods: **`month`**, **`dayofweek`**, **`minute`**, **`second`**, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19f7551d-207c-4de2-88f5-21300d684806"}}},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, dayofweek, minute, second\n\ndatetime_df = (timestamp_df\n               .withColumn(\"year\", year(col(\"timestamp\")))\n               .withColumn(\"month\", month(col(\"timestamp\")))\n               .withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n               .withColumn(\"minute\", minute(col(\"timestamp\")))\n               .withColumn(\"second\", second(col(\"timestamp\")))\n              )\ndisplay(datetime_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37457dee-521d-4801-9fac-162349cc8e40"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Convert to Date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8adb9929-e8a4-469c-82c6-5e6227646f33"}}},{"cell_type":"markdown","source":["#### **`to_date`**\nConverts the column into DateType by casting rules to DateType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66f64507-2f28-4a6e-9b01-803e80e8039e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n\ndate_df = timestamp_df.withColumn(\"date\", to_date(col(\"timestamp\")))\ndisplay(date_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1792e91-38de-42cd-b676-a10de31a82f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Manipulate Datetimes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2904a8b-d3de-40ac-9cba-30b6c946f226"}}},{"cell_type":"markdown","source":["#### **`date_add`**\nReturns the date that is the given number of days after start"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a54bae4-b5a6-489e-bee8-cfc99a5e20d3"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add\n\nplus_2_df = timestamp_df.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\ndisplay(plus_2_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"210cc5e1-0a56-493d-9e57-0d1cff627f83"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"982dd146-819d-4f37-9c10-db3475b355fd"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d339cc7-0205-45ad-b3b0-4f621c09e7f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# DataFrame & Column\n##### Objectives\n1. Construct columns\n1. Subset columns\n1. Add or replace columns\n1. Subset rows\n1. Sort rows\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`select`**, **`selectExpr`**, **`drop`**, **`withColumn`**, **`withColumnRenamed`**, **`filter`**, **`distinct`**, **`limit`**, **`sort`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html\" target=\"_blank\">Column</a>: **`alias`**, **`isin`**, **`cast`**, **`isNotNull`**, **`desc`**, operators"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff8c9b13-27ae-42ec-adb7-55e5a9414a13"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d51d07b-7273-424c-a09c-a7d2f693f654"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use the BedBricks events dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ff0a4ff-fcab-4ff1-9e9e-ec3c772664cb"}}},{"cell_type":"code","source":["events_df = spark.read.format(\"delta\").load(DA.paths.events)\ndisplay(events_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6462a979-5be4-4b35-8f9d-d47b8a78a1fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Column Expressions\n\nA <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html\" target=\"_blank\">Column</a> is a logical construction that will be computed based on the data in a DataFrame using an expression\n\nConstruct a new Column based on existing columns in a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04a402e7-4a1d-4c28-8e99-3e414f4a2279"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nprint(events_df.device)\nprint(events_df[\"device\"])\nprint(col(\"device\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b697d647-c5f6-4e87-a0ee-11be6a9496e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Scala supports an additional syntax for creating a new Column based on existing columns in a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0b72e1c-cfdf-4ec7-8202-be4ed2ec1028"}}},{"cell_type":"code","source":["%scala\n$\"device\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85ec9722-70be-41e2-98ad-8c94e1fbe952"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Column Operators and Methods\n| Method | Description |\n| --- | --- |\n| \\*, + , <, >= | Math and comparison operators |\n| ==, != | Equality and inequality tests (Scala operators are **`===`** and **`=!=`**) |\n| alias | Gives the column an alias |\n| cast, astype | Casts the column to a different data type |\n| isNull, isNotNull, isNan | Is null, is not null, is NaN |\n| asc, desc | Returns a sort expression based on ascending/descending order of the column |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"894d3a99-4ea5-485a-a999-62a2558d5d55"}}},{"cell_type":"markdown","source":["Create complex expressions with existing columns, operators, and methods."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3300b7e-b0e4-4f94-b316-a5963129f0e6"}}},{"cell_type":"code","source":["col(\"ecommerce.purchase_revenue_in_usd\") + col(\"ecommerce.total_item_quantity\")\ncol(\"event_timestamp\").desc()\n(col(\"ecommerce.purchase_revenue_in_usd\") * 100).cast(\"int\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e50c3f6-3b74-4391-a328-b36c326413c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here's an example of using these column expressions in the context of a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de199529-195f-4d2a-9c79-0e216d3310ba"}}},{"cell_type":"code","source":["rev_df = (events_df\n         .filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\n         .withColumn(\"purchase_revenue\", (col(\"ecommerce.purchase_revenue_in_usd\") * 100).cast(\"int\"))\n         .withColumn(\"avg_purchase_revenue\", col(\"ecommerce.purchase_revenue_in_usd\") / col(\"ecommerce.total_item_quantity\"))\n         .sort(col(\"avg_purchase_revenue\").desc())\n        )\n\ndisplay(rev_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db0c5e6d-c4b5-4703-98c8-17f1cb16d4d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrame Transformation Methods\n| Method | Description |\n| --- | --- |\n| **`select`** | Returns a new DataFrame by computing given expression for each element |\n| **`drop`** | Returns a new DataFrame with a column dropped |\n| **`withColumnRenamed`** | Returns a new DataFrame with a column renamed |\n| **`withColumn`** | Returns a new DataFrame by adding a column or replacing the existing column that has the same name |\n| **`filter`**, **`where`** | Filters rows using the given condition |\n| **`sort`**, **`orderBy`** | Returns a new DataFrame sorted by the given expressions |\n| **`dropDuplicates`**, **`distinct`** | Returns a new DataFrame with duplicate rows removed |\n| **`limit`** | Returns a new DataFrame by taking the first n rows |\n| **`groupBy`** | Groups the DataFrame using the specified columns, so we can run aggregation on them |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31eaf77c-9a2e-4b56-91ca-1d651f971950"}}},{"cell_type":"markdown","source":["### Subset columns\nUse DataFrame transformations to subset columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"094b769f-25e8-42fc-92b3-89139d93fa65"}}},{"cell_type":"markdown","source":["#### **`select()`**\nSelects a list of columns or column based expressions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00871d46-7fae-4380-b699-b920c18035a5"}}},{"cell_type":"code","source":["devices_df = events_df.select(\"user_id\", \"device\")\ndisplay(devices_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"787fa067-5b3f-4ad2-9137-77c4043d828c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nlocations_df = events_df.select(\n    \"user_id\", \n    col(\"geo.city\").alias(\"city\"), \n    col(\"geo.state\").alias(\"state\")\n)\ndisplay(locations_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4964c48e-f636-433c-b43f-64223ab97839"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`selectExpr()`**\nSelects a list of SQL expressions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"871f9fb3-871a-41aa-a813-ee493f7f1908"}}},{"cell_type":"code","source":["apple_df = events_df.selectExpr(\"user_id\", \"device in ('macOS', 'iOS') as apple_user\")\ndisplay(apple_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac2121bc-24f9-4e0d-b313-b35d458232d7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`drop()`**\nReturns a new DataFrame after dropping the given column, specified as a string or Column object\n\nUse strings to specify multiple columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a66e2b5-6bea-4b60-ab7f-aaff814bbd09"}}},{"cell_type":"code","source":["anonymous_df = events_df.drop(\"user_id\", \"geo\", \"device\")\ndisplay(anonymous_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb00eaee-c684-4193-a058-c35a65c43577"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["no_sales_df = events_df.drop(col(\"ecommerce\"))\ndisplay(no_sales_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c0f411d-1e32-4454-805b-cdada338748d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Add or replace columns\nUse DataFrame transformations to add or replace columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76db3c87-501b-449c-a07f-2c976ceea4b2"}}},{"cell_type":"markdown","source":["#### **`withColumn()`**\nReturns a new DataFrame by adding a column or replacing an existing column that has the same name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fbe6ea4-2163-49c8-8dba-7d8c67eda2ea"}}},{"cell_type":"code","source":["mobile_df = events_df.withColumn(\"mobile\", col(\"device\").isin(\"iOS\", \"Android\"))\ndisplay(mobile_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"807bf98d-946a-4c15-9d81-3322d6a932cb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["purchase_quantity_df = events_df.withColumn(\"purchase_quantity\", col(\"ecommerce.total_item_quantity\").cast(\"int\"))\npurchase_quantity_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b867ed8-734e-457f-9046-099cf7dd2707"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`withColumnRenamed()`**\nReturns a new DataFrame with a column renamed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08d87d05-fa32-47a2-b82f-3d60e770d6c7"}}},{"cell_type":"code","source":["location_df = events_df.withColumnRenamed(\"geo\", \"location\")\ndisplay(location_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c37295c-8449-457b-94f5-50057d8fe469"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Subset Rows\nUse DataFrame transformations to subset rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b946848-1a17-400a-af09-941a2bafd820"}}},{"cell_type":"markdown","source":["#### **`filter()`**\nFilters rows using the given SQL expression or column based condition.\n\n##### Alias: **`where`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9a9a538-9d8d-4903-b9c2-b44ca2af82b7"}}},{"cell_type":"code","source":["purchases_df = events_df.filter(\"ecommerce.total_item_quantity > 0\")\ndisplay(purchases_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cacfd11-73fc-4143-ae96-729e55bc58af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["revenue_df = events_df.filter(col(\"ecommerce.purchase_revenue_in_usd\").isNotNull())\ndisplay(revenue_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e7be411-da6f-42ba-8fbb-0496924c5ccf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["android_df = events_df.filter((col(\"traffic_source\") != \"direct\") & (col(\"device\") == \"Android\"))\ndisplay(android_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"064a24fe-c58a-4b7f-9b8a-69d84e570aaa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`dropDuplicates()`**\nReturns a new DataFrame with duplicate rows removed, optionally considering only a subset of columns.\n\n##### Alias: **`distinct`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c1ed4e3-a586-4691-850d-a6b29642c284"}}},{"cell_type":"code","source":["display(events_df.distinct())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"797ddc80-0593-4f70-95a1-888bdc3db1fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["distinct_users_df = events_df.dropDuplicates([\"user_id\"])\ndisplay(distinct_users_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20ed4df0-1467-47f9-b7ad-bd94de0e6c3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`limit()`**\nReturns a new DataFrame by taking the first n rows."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87d0b1fd-fb50-4bad-909a-672cbbf5a04e"}}},{"cell_type":"code","source":["limit_df = events_df.limit(100)\ndisplay(limit_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dee9f63-51a9-4201-8ade-080c8848da47"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Sort rows\nUse DataFrame transformations to sort rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d672d765-53ea-4803-86f6-f00505dd94f4"}}},{"cell_type":"markdown","source":["#### **`sort()`**\nReturns a new DataFrame sorted by the given columns or expressions.\n\n##### Alias: **`orderBy`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3e8c23a-1681-43a8-8974-ba6173ec3378"}}},{"cell_type":"code","source":["increase_timestamps_df = events_df.sort(\"event_timestamp\")\ndisplay(increase_timestamps_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5257df07-6f62-430b-974a-e5c2a492e74d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["decrease_timestamp_df = events_df.sort(col(\"event_timestamp\").desc())\ndisplay(decrease_timestamp_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7872078a-e73b-4e67-ba7a-d01a1b6f4c50"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["increase_sessions_df = events_df.orderBy([\"user_first_touch_timestamp\", \"event_timestamp\"])\ndisplay(increase_sessions_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"097024f0-0da3-4583-b4f3-7261f06e004d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["decrease_sessions_df = events_df.sort(col(\"user_first_touch_timestamp\").desc(), col(\"event_timestamp\"))\ndisplay(decrease_sessions_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f159b1d-f738-4ed9-9209-62af6973164b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35b3db27-d754-4b5d-bd98-d67a7330c1c6"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a61b8a8-1107-4125-9d16-0f39a002ca74"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Active Users Lab\nPlot daily active users and average active users by day of week.\n1. Extract timestamp and date of events\n2. Get daily active users\n3. Get average number of active users by day of week\n4. Sort day of week in correct order"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c66c414-6807-4879-a361-20d17faf2b71"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"386c6afd-9f0e-4b50-ad4c-187de2fe17ec"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame of user IDs and timestamps of events logged on the BedBricks website."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12887523-cd8f-4a14-b652-de4d0fb9a4e1"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = (spark\n      .read\n      .format(\"delta\")\n      .load(DA.paths.events)\n      .select(\"user_id\", col(\"event_timestamp\").alias(\"ts\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28ec9856-6a38-4a27-bf00-ef1f0fd2edff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Extract timestamp and date of events\n- Convert **`ts`** from microseconds to seconds by dividing by 1 million and cast to timestamp\n- Add **`date`** column by converting **`ts`** to date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a072c0aa-c15c-47cb-8983-39a72e5b3ef8"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import to_date\n\ndatetime_df = (df\n               .withColumn(\"ts\", (col(\"ts\") / 1e6).cast(\"timestamp\"))\n               .withColumn(\"date\", to_date(\"ts\"))\n              )\ndisplay(datetime_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2a012de-28c2-4f5c-b357-a29004f92980"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5db9345e-76fb-44eb-8d24-4f21ea279b3c"}}},{"cell_type":"code","source":["from pyspark.sql.types import DateType, StringType, StructField, StructType, TimestampType\n\nexpected1a = StructType([StructField(\"user_id\", StringType(), True),\n                         StructField(\"ts\", TimestampType(), True),\n                         StructField(\"date\", DateType(), True)])\n\nresult1a = datetime_df.schema\n\nassert expected1a == result1a, \"datetime_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3189665-a144-475a-830b-ebd35effbd39"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\n\nexpected1b = datetime.date(2020, 6, 19)\nresult1b = datetime_df.sort(\"date\").first().date\n\nassert expected1b == result1b, \"datetime_df does not have the expected date values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bf9bedd-e409-480a-82f0-9b86c6fcc209"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get daily active users\n- Group by date\n- Aggregate approximate count of distinct **`user_id`** and alias to \"active_users\"\n  - Recall built-in function to get **approximate count distinct** (also recall:  approximate count distinct is different than count distinct!)\n- Sort by date\n- Plot as line graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"276bdc50-5a81-4d6b-a265-2b94dd8f07ba"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import approx_count_distinct\n\nactive_users_df = (datetime_df\n                   .groupBy(\"date\")\n                   .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n                   .sort(\"date\")\n                  )\ndisplay(active_users_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e6678a1-f7da-4fa8-84cb-27383f51f7f8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b03bc370-5d46-45ad-add6-5309957260a1"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\nexpected2a = StructType([StructField(\"date\", DateType(), True),\n                         StructField(\"active_users\", LongType(), False)])\n\nresult2a = active_users_df.schema\n\nassert expected2a == result2a, \"active_users_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f65c2e4-87d1-4574-9bd8-ffaa20a861bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected2b = [(datetime.date(2020, 6, 19), 251573), (datetime.date(2020, 6, 20), 357215), (datetime.date(2020, 6, 21), 305055), (datetime.date(2020, 6, 22), 239094), (datetime.date(2020, 6, 23), 243117)]\n\nresult2b = [(row.date, row.active_users) for row in active_users_df.orderBy(\"date\").take(5)]\n\nassert expected2b == result2b, \"active_users_df does not have the expected values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b60dfcd2-398a-40b2-8193-4dc3f0b4f5f6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Get average number of active users by day of week\n- Add **`day`** column by extracting day of week from **`date`** using a datetime pattern string - the expected output here will be a day name, not a number (e.g. **`Mon`**, not **`1`**)\n- Group by **`day`**\n- Aggregate average of **`active_users`** and alias to \"avg_users\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"520e809a-a1fd-48ce-b66c-c3da4baaf6fc"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import date_format, avg\n\nactive_dow_df = (active_users_df\n                 .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n                 .groupBy(\"day\")\n                 .agg(avg(col(\"active_users\")).alias(\"avg_users\"))\n                )\ndisplay(active_dow_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2ba815f-3652-4764-b563-5e6d8fd2114d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98836592-6ed7-4508-acc5-5f4eb6239472"}}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n\nexpected3a = StructType([StructField(\"day\", StringType(), True),\n                         StructField(\"avg_users\", DoubleType(), True)])\n\nresult3a = active_dow_df.schema\n\nassert expected3a == result3a, \"active_dow_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dafa822-1b3e-4b98-a86c-606732729b2d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected3b = [(\"Fri\", 247180.66666666666), (\"Mon\", 238195.5), (\"Sat\", 278482.0), (\"Sun\", 282905.5), (\"Thu\", 264620.0), (\"Tue\", 260942.5), (\"Wed\", 227214.0)]\n\nresult3b = [(row.day, row.avg_users) for row in active_dow_df.sort(\"day\").collect()]\n\nassert expected3b == result3b, \"active_dow_df does not have the expected values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c9fa36d-b7ba-4ad2-b029-ba0fb98454b8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Sort Day Lab\n\n##### Tasks\n1. Define a UDF to label the day of week\n1. Apply the UDF to label and sort by day of week\n1. Plot active users by day of week as a bar graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fabe5d01-cdd2-44ff-a5f4-ae75b0944e40"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38d1605c-9981-4fac-a058-8332d0546379"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Start with a DataFrame of the average number of active users by day of week.\n\nThis was the resulting **`df`** in a previous lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3d10b2d-2854-4d07-8631-2ac490236b17"}}},{"cell_type":"code","source":["from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n\ndf = (spark\n      .read\n      .format(\"delta\")\n      .load(DA.paths.events)\n      .withColumn(\"ts\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n      .withColumn(\"date\", to_date(\"ts\"))\n      .groupBy(\"date\").agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n      .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n      .groupBy(\"day\").agg(avg(col(\"active_users\")).alias(\"avg_users\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c8ba2df-800c-4625-9510-2122e45751e6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Define UDF to label day of week\n\nUse the **`label_day_of_week`** function provided below to create the UDF **`label_dow_udf`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"017731dc-16b3-4046-8623-bc725dec841c"}}},{"cell_type":"code","source":["def label_day_of_week(day: str) -> str:\n    dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\",\n           \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n    return dow.get(day) + \"-\" + day"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d280511-856a-4ff1-b931-5606104b66ab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nlabel_dow_udf = spark.udf.register(\"label_dow\", label_day_of_week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6352596-7851-43c6-b7d0-ab5d59f569d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Apply UDF to label and sort by day of week\n- Update the **`day`** column by applying the UDF and replacing this column\n- Sort by **`day`**\n- Plot as a bar graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"167be12b-6932-4289-b799-f57e3cbf45df"}}},{"cell_type":"code","source":["# ANSWER\nfinal_df = (df\n            .withColumn(\"day\", label_dow_udf(col(\"day\")))\n            .sort(\"day\")\n           )\ndisplay(final_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8071d623-d72b-4d1d-8a62-c82e61bc0c77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Revenue by Traffic Lab\nGet the 3 traffic sources generating the highest total revenue.\n1. Aggregate revenue by traffic source\n2. Get top 3 traffic sources by total revenue\n3. Clean revenue columns to have two decimal places\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`groupBy`**, **`sort`**, **`limit`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html\" target=\"_blank\">Column</a>: **`alias`**, **`desc`**, **`cast`**, **`operators`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\" target=\"_blank\">Built-in Functions</a>: **`avg`**, **`sum`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bdfe738-82b2-4ec1-babf-eac2feef49cb"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1f9733b-6d81-4827-9f72-f1b2e9d2f1a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame **`df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a6bc779-25e1-4b3b-bedd-3bcf47cd27f9"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\n# Purchase events logged on the BedBricks website\ndf = (spark.read.format(\"delta\").load(DA.paths.events)\n      .withColumn(\"revenue\", col(\"ecommerce.purchase_revenue_in_usd\"))\n      .filter(col(\"revenue\").isNotNull())\n      .drop(\"event_name\")\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3861f49a-3d3a-4392-95f8-6a706629141a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Aggregate revenue by traffic source\n- Group by **`traffic_source`**\n- Get sum of **`revenue`** as **`total_rev`**. Round this to the tens decimal place (e.g. `nnnnn.n`). \n- Get average of **`revenue`** as **`avg_rev`**\n\nRemember to import any necessary built-in functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ea33a48-a4a8-405e-88e9-c889e2582a01"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import avg, col, sum\n\ntraffic_df = (df\n              .groupBy(\"traffic_source\")\n              .agg(sum(col(\"revenue\")).alias(\"total_rev\"),\n                   avg(col(\"revenue\")).alias(\"avg_rev\"))\n             )\n\ndisplay(traffic_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"365e1d45-4eed-49d1-9942-5fbc7eb85346"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b208610b-885c-4940-8bd0-6506b130e70b"}}},{"cell_type":"code","source":["from pyspark.sql.functions import round\n\nexpected1 = [(12704560.0, 1083.175), (78800000.3, 983.2915), (24797837.0, 1076.6221), (47218429.0, 1086.8303), (16177893.0, 1083.4378), (8044326.0, 1087.218)]\ntest_df = traffic_df.sort(\"traffic_source\").select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\nresult1 = [(row.total_rev, row.avg_rev) for row in test_df.collect()]\n\nassert(expected1 == result1)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65b6d0b7-e116-456c-a9e6-a2ada032f91b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get top three traffic sources by total revenue\n- Sort by **`total_rev`** in descending order\n- Limit to first three rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ecb8768-dd98-487e-99bc-e60549e9dd7f"}}},{"cell_type":"code","source":["# ANSWER\ntop_traffic_df = traffic_df.sort(col(\"total_rev\").desc()).limit(3)\ndisplay(top_traffic_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48c0676e-3d98-40b1-80c4-bd84ec1a5bd8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3475e202-c650-49ac-8c9d-2166c7919b95"}}},{"cell_type":"code","source":["expected2 = [(78800000.3, 983.2915), (47218429.0, 1086.8303), (24797837.0, 1076.6221)]\ntest_df = top_traffic_df.select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\nresult2 = [(row.total_rev, row.avg_rev) for row in test_df.collect()]\n\nassert(expected2 == result2)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eac9d65-fd21-4726-a163-5be2e1db9498"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Limit revenue columns to two decimal places\n- Modify columns **`avg_rev`** and **`total_rev`** to contain numbers with two decimal places\n  - Use **`withColumn()`** with the same names to replace these columns\n  - To limit to two decimal places, multiply each column by 100, cast to long, and then divide by 100"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9e16417-3266-464c-9e14-43d3c746fce9"}}},{"cell_type":"code","source":["# ANSWER\nfinal_df = (top_traffic_df\n            .withColumn(\"avg_rev\", (col(\"avg_rev\") * 100).cast(\"long\") / 100)\n            .withColumn(\"total_rev\", (col(\"total_rev\") * 100).cast(\"long\") / 100)\n           )\n\ndisplay(final_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea0ec67b-5132-46d5-b5dc-8809f95b5897"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b70cd70e-fd7a-4d86-a50e-99f92e59d916"}}},{"cell_type":"code","source":["expected3 = [(78800000.29, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult3 = [(row.total_rev, row.avg_rev) for row in final_df.collect()]\n\nassert(expected3 == result3)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1b4de1b-35fe-4b2a-b5ef-53036f171b98"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Bonus: Rewrite using a built-in math function\nFind a built-in math function that rounds to a specified number of decimal places"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e67fe314-f67e-49a0-83da-36abd4b2c341"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import round\n\nbonus_df = (top_traffic_df\n            .withColumn(\"avg_rev\", round(\"avg_rev\", 2))\n            .withColumn(\"total_rev\", round(\"total_rev\", 2))\n           )\n\ndisplay(bonus_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bc30524-afc5-4d08-9413-0d67a749178c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79820407-0f1c-4afd-a8aa-4b32ea70eaa8"}}},{"cell_type":"code","source":["expected4 = [(78800000.3, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult4 = [(row.total_rev, row.avg_rev) for row in bonus_df.collect()]\n\nassert(expected4 == result4)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ab5214-9e2a-4f61-98dd-c6b8226e28a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. Chain all the steps above"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfeb15d1-288f-4fa6-ab9b-e5a8beb14db5"}}},{"cell_type":"code","source":["# ANSWER\n# Solution #1 using round\n\nchain_df = (df\n            .groupBy(\"traffic_source\")\n            .agg(sum(col(\"revenue\")).alias(\"total_rev\"),\n                 avg(col(\"revenue\")).alias(\"avg_rev\"))\n            .sort(col(\"total_rev\").desc())\n            .limit(3)\n            .withColumn(\"avg_rev\", round(\"avg_rev\", 2))\n            .withColumn(\"total_rev\", round(\"total_rev\", 2))\n           )\n\ndisplay(chain_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08349077-cd37-4e33-9dc0-f92ca33336f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n# Solution #2 using *100, cast, /100\n# chain_df = (df\n#             .groupBy(\"traffic_source\")\n#             .agg(sum(col(\"revenue\")).alias(\"total_rev\"),\n#                  avg(col(\"revenue\")).alias(\"avg_rev\"))\n#             .sort(col(\"total_rev\").desc())\n#             .limit(3)\n#             .withColumn(\"avg_rev\", (col(\"avg_rev\") * 100).cast(\"long\") / 100)\n#             .withColumn(\"total_rev\", (col(\"total_rev\") * 100).cast(\"long\") / 100)\n#            )\n\n# display(chain_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0d4fe95-4dd5-4594-95e4-293860f2e6d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**5.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"097e35ab-9128-44fc-ac0c-d5e2ce2f9ab6"}}},{"cell_type":"code","source":["method_a = [(78800000.3,  983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nmethod_b = [(78800000.29, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult5 = [(row.total_rev, row.avg_rev) for row in chain_df.collect()]\n\nassert result5 == method_a or result5 == method_b\nprint(\"All test pass\")\n\n%md # Query Optimization\n\nWe'll explore query plans and optimizations for several examples including logical optimizations and exanples with and without predicate pushdown.\n\n##### Objectives\n1. Logical optimizations\n1. Predicate pushdown\n1. No predicate pushdown\n\n##### Methods \n- <a href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.explain.html#pyspark.sql.DataFrame.explain\" target=\"_blank\">DataFrame</a>: **`explain`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c924c802-7465-4031-8a68-886b1085577a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let’s run our set up cell, and get our initial DataFrame stored in the variable **`df`**. Displaying this DataFrame shows us events data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70d78638-f58c-43ce-a2d7-4229a0e683bb"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1bc584f-8aff-4769-aa8e-48f9ce838ec8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(DA.paths.events)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df42353c-f1c8-4028-b923-d2b7168279c3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Logical Optimization\n\n**`explain(..)`** prints the query plans, optionally formatted by a given explain mode. Compare the following logical plan & physical plan, noting how Catalyst handled the multiple **`filter`** transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"480052c8-d0df-4e19-b6b4-b68b7c45eef1"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nlimit_events_df = (df\n                   .filter(col(\"event_name\") != \"reviews\")\n                   .filter(col(\"event_name\") != \"checkout\")\n                   .filter(col(\"event_name\") != \"register\")\n                   .filter(col(\"event_name\") != \"email_coupon\")\n                   .filter(col(\"event_name\") != \"cc_info\")\n                   .filter(col(\"event_name\") != \"delivery\")\n                   .filter(col(\"event_name\") != \"shipping_info\")\n                   .filter(col(\"event_name\") != \"press\")\n                  )\n\nlimit_events_df.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4762195-8992-4991-942d-f3af8f9a75f9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Of course, we could have written the query originally using a single **`filter`** condition ourselves. Compare the previous and following query plans."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"510cd3e5-0061-4dc3-8e4a-e88a875a265c"}}},{"cell_type":"code","source":["better_df = (df\n             .filter((col(\"event_name\").isNotNull()) &\n                     (col(\"event_name\") != \"reviews\") &\n                     (col(\"event_name\") != \"checkout\") &\n                     (col(\"event_name\") != \"register\") &\n                     (col(\"event_name\") != \"email_coupon\") &\n                     (col(\"event_name\") != \"cc_info\") &\n                     (col(\"event_name\") != \"delivery\") &\n                     (col(\"event_name\") != \"shipping_info\") &\n                     (col(\"event_name\") != \"press\"))\n            )\n\nbetter_df.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57fbca4f-6132-4601-83a6-51386db9d49e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Of course, we wouldn't write the following code intentionally, but in a long, complex query you might not notice the duplicate filter conditions. Let's see what Catalyst does with this query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"020f5c9b-8319-4076-92f6-c4b24e785374"}}},{"cell_type":"code","source":["stupid_df = (df\n             .filter(col(\"event_name\") != \"finalize\")\n             .filter(col(\"event_name\") != \"finalize\")\n             .filter(col(\"event_name\") != \"finalize\")\n             .filter(col(\"event_name\") != \"finalize\")\n             .filter(col(\"event_name\") != \"finalize\")\n            )\n\nstupid_df.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7761ab75-3f99-49bf-9430-211f3788a9ef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Caching\n\nBy default the data of a DataFrame is present on a Spark cluster only while it is being processed during a query -- it is not automatically persisted on the cluster afterwards. (Spark is a data processing engine, not a data storage system.) You can explicity request Spark to persist a DataFrame on the cluster by invoking its **`cache`** method.\n\nIf you do cache a DataFrame, you should always explictly evict it from cache by invoking **`unpersist`** when you no longer need it.\n\n<img src=\"https://files.training.databricks.com/images/icon_best_32.png\" alt=\"Best Practice\"> Caching a DataFrame can be appropriate if you are certain that you will use the same DataFrame multiple times, as in:\n\n- Exploratory data analysis\n- Machine learning model training\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Aside from those use cases, you should **not** cache DataFrames because it is likely that you'll *degrade* the performance of your application.\n\n- Caching consumes cluster resources that could otherwise be used for task execution\n- Caching can prevent Spark from performing query optimizations, as shown in the next example"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cd3ab03-8003-4b31-aaac-58540d45aa39"}}},{"cell_type":"markdown","source":["### Predicate Pushdown\n\nHere is example reading from a JDBC source, where Catalyst determines that *predicate pushdown* can take place."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d70e6ed-feaf-4261-a7d3-f08a991281a6"}}},{"cell_type":"code","source":["%scala\n// Ensure that the driver class is loaded\nClass.forName(\"org.postgresql.Driver\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6e3ec79-8ea4-4724-b738-5e2cda9e10f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["jdbc_url = \"jdbc:postgresql://54.213.33.240/training\"\n\n# Username and Password w/read-only rights\nconn_properties = {\n    \"user\" : \"training\",\n    \"password\" : \"training\"\n}\n\npp_df = (spark\n         .read\n         .jdbc(url=jdbc_url,                 # the JDBC URL\n               table=\"training.people_1m\",   # the name of the table\n               column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n               lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n               upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n               numPartitions=8,              # the number of partitions/connections\n               properties=conn_properties    # the connection properties\n              )\n         .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n        )\n\npp_df.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14a8e977-ccf2-49f3-8fd1-b75e790fd81e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note the lack of a **Filter** and the presence of a **PushedFilters** in the **Scan**. The filter operation is pushed to the database and only the matching records are sent to Spark. This can greatly reduce the amount of data that Spark needs to ingest."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e60f9ae-cbce-47a4-a154-8f852ee4b14c"}}},{"cell_type":"markdown","source":["### No Predicate Pushdown\n\nIn comparison, caching the data before filtering eliminates the possibility for the predicate push down."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d884127-a5b1-4a93-8198-a249ab564375"}}},{"cell_type":"code","source":["cached_df = (spark\n            .read\n            .jdbc(url=jdbc_url,\n                  table=\"training.people_1m\",\n                  column=\"id\",\n                  lowerBound=1,\n                  upperBound=1000000,\n                  numPartitions=8,\n                  properties=conn_properties\n                 )\n            )\n\ncached_df.cache()\nfiltered_df = cached_df.filter(col(\"gender\") == \"M\")\n\nfiltered_df.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6491f36e-e5b6-4b15-ac50-5a295fd41ed9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In addition to the **Scan** (the JDBC read) we saw in the previous example, here we also see the **InMemoryTableScan** followed by a **Filter** in the explain plan.\n\nThis means Spark had to read ALL the data from the database and cache it, and then scan it in cache to find the records matching the filter condition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d90c589-4beb-4b79-ba96-ab3f601cc5d0"}}},{"cell_type":"markdown","source":["Remember to clean up after ourselves!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1da859ca-3f59-4366-86f8-554a6957a1d9"}}},{"cell_type":"code","source":["cached_df.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a89f658-8620-4c3b-a692-f71f9a757f6e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Partitioning\n##### Objectives\n1. Get partitions and cores\n1. Repartition DataFrames\n1. Configure default shuffle partitions\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`repartition`**, **`coalesce`**, **`rdd.getNumPartitions`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html\" target=\"_blank\">SparkConf</a>: **`get`**, **`set`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html\" target=\"_blank\">SparkSession</a>: **`spark.sparkContext.defaultParallelism`**\n\n##### SparkConf Parameters\n- **`spark.sql.shuffle.partitions`**, **`spark.sql.adaptive.enabled`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d081af9-ad48-41bb-8008-0b0e34bd3cca"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df215e60-d096-49f3-9254-ba7ac161a17d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Get partitions and cores\n\nUse the **`rdd`** method **`getNumPartitions`** to get the number of DataFrame partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3de07705-5a5d-4a0f-b70c-63067a842f3b"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(DA.paths.events)\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e354c05-666f-4bf3-b10f-88d53c31fa4f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Access **`SparkContext`** through **`SparkSession`** to get the number of cores or slots.\n\nUse the **`defaultParallelism`** attribute to get the number of cores in a cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"775a78d6-09d1-44cc-9045-c39e4b687583"}}},{"cell_type":"code","source":["print(spark.sparkContext.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f679e1a-c4ee-4b77-8e1e-a7e383f548e1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**`SparkContext`** is also provided in Databricks notebooks as the variable **`sc`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa85f460-66a6-4b13-9fbd-1e51de9058e9"}}},{"cell_type":"code","source":["print(sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77230192-5e14-46d0-b665-34e1fbf7fbff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Repartition DataFrame\n\nThere are two methods available to repartition a DataFrame: **`repartition`** and **`coalesce`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29c0c19a-386f-4921-aa5c-74624dd6f7ef"}}},{"cell_type":"markdown","source":["#### **`repartition`**\nReturns a new DataFrame that has exactly **`n`** partitions.\n\n- Wide transformation\n- Pro: Evenly balances partition sizes  \n- Con: Requires shuffling all data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67ccfc33-6f64-42c4-a3b8-eb5ff3c458b6"}}},{"cell_type":"code","source":["repartitioned_df = df.repartition(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f507b6e-6210-42ee-a1fb-39f5a2f0dd34"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["repartitioned_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b2bdb36-5290-40fb-8602-fce2531d1da6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### **`coalesce`**\nReturns a new DataFrame that has exactly **`n`** partitions, when fewer partitions are requested.\n\nIf a larger number of partitions is requested, it will stay at the current number of partitions.\n\n- Narrow transformation, some partitions are effectively concatenated\n- Pro: Requires no shuffling\n- Cons:\n  - Is not able to increase # partitions\n  - Can result in uneven partition sizes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ff035ca-dd03-4ef6-bfc5-9975219332aa"}}},{"cell_type":"code","source":["coalesce_df = df.coalesce(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31669760-1422-4228-aa3e-59fbbe01bdd3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["coalesce_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47066c33-af37-4313-9ccc-3a45bbf70018"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Configure default shuffle partitions\n\nUse the SparkSession's **`conf`** attribute to get and set dynamic Spark configuration properties. The **`spark.sql.shuffle.partitions`** property determines the number of partitions that result from a shuffle. Let's check its default value:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e756f8fd-800b-4db7-a987-6a475b7c97c7"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b188f7da-b08d-46a7-b26a-52d3903db94d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09f58c87-7091-4a57-9391-3e6493943e18"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7c17212-c261-458b-a1c8-08855323c555"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Partitioning Guidelines\n- Make the number of partitions a multiple of the number of cores\n- Target a partition size of ~200MB\n- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1914f5b-30ac-4073-ac10-00e941b5f5e5"}}},{"cell_type":"markdown","source":["### Adaptive Query Execution\n\n<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n\nIn Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set **`spark.sql.shuffle.partitions`** based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n\nThe **`spark.sql.adaptive.enabled`** configuration option controls whether AQE is turned on/off."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3c60c7f-44d2-49af-bdc3-d1ab2d378430"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.adaptive.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b44b8ccf-bdc2-419f-aa57-a09563fa033c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# De-Duping Data Lab\n\nIn this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n\n* first, middle and last names\n* gender\n* birth date\n* Social Security number\n* salary\n\nBut, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n\n* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n\nIf all of the name fields match -- if you disregard character case -- then the birth dates and salaries are guaranteed to match as well,\nand the Social Security Numbers *would* match if they were somehow put in the same format.\n\nYour job is to remove the duplicate records. The specific requirements of your job are:\n\n* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n* Preserve the data format of the columns. For example, if you write the first name column in all lowercase, you haven't met this requirement.\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> The initial dataset contains 103,000 records.\nThe de-duplicated result has 100,000 records.\n\nNext, write the results in **Delta** format as a **single data file** to the directory given by the variable **delta_dest_dir**.\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> Remember the relationship between the number of partitions in a DataFrame and the number of files written.\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html\" target=\"_blank\">DataFrameReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\" target=\"_blank\">Built-In Functions</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html\" target=\"_blank\">DataFrameWriter</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18072b81-c6f3-4949-a393-426be4761723"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1eaa669-43cb-4502-83d7-c3b9d9e18427"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It's helpful to look at the file first, so you can check the format with **`dbutils.fs.head()`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cb3e8a8-1218-4a29-82bf-c30dfd3ec447"}}},{"cell_type":"code","source":["dbutils.fs.head(f\"{DA.paths.datasets}/people/people-with-dups.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b200e4b-2778-48ad-8d43-50d55166f29a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TODO\n\nsource_file = f\"{DA.paths.datasets}/people/people-with-dups.txt\"\ndelta_dest_dir = f\"{DA.paths.working_dir}/people\"\n\n# In case it already exists\ndbutils.fs.rm(delta_dest_dir, True)\n\n# Complete your work here...\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e8ad591-b802-4737-8b86-e812ac2f967f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d408e09-766d-44cc-b82b-54608e53bd70"}}},{"cell_type":"code","source":["verify_files = dbutils.fs.ls(delta_dest_dir)\nverify_delta_format = False\nverify_num_data_files = 0\nfor f in verify_files:\n    if f.name == \"_delta_log/\":\n        verify_delta_format = True\n    elif f.name.endswith(\".parquet\"):\n        verify_num_data_files += 1\n\nassert verify_delta_format, \"Data not written in Delta format\"\nassert verify_num_data_files == 1, \"Expected 1 data file written\"\n\nverify_record_count = spark.read.format(\"delta\").load(delta_dest_dir).count()\nassert verify_record_count == 100000, \"Expected 100000 records in final result\"\n\ndel verify_files, verify_delta_format, verify_num_data_files, verify_record_count\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae7cb028-991c-4f2c-856f-cd6b16951fd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cef60ff-4bb4-402d-9dd6-3fda28732b91"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_syntax","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":484899027341154}},"nbformat":4,"nbformat_minor":0}
