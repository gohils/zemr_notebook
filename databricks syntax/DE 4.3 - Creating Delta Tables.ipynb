{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac124394-eb45-4c0e-adbb-170a99bc0f7f"}}},{"cell_type":"markdown","source":["# Creating Delta Tables\n\nAfter extracting data from external data sources, load data into the Lakehouse to ensure that all of the benefits of the Databricks platform can be fully leveraged.\n\nWhile different organizations may have varying policies for how data is initially loaded into Databricks, we typically recommend that early tables represent a mostly raw version of the data, and that validation and enrichment occur in later stages. This pattern ensures that even if data doesn't match expectations with regards to data types or column names, no data will be dropped, meaning that programmatic or manual intervention can still salvage data in a partially corrupted or invalid state.\n\nThis lesson will focus primarily on the pattern used to create most tables, **`CREATE TABLE _ AS SELECT`** (CTAS) statements.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Use CTAS statements to create Delta Lake tables\n- Create new tables from existing views or tables\n- Enrich loaded data with additional metadata\n- Declare table schema with generated columns and descriptive comments\n- Set advanced options to control data location, quality enforcement, and partitioning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b201c06e-a466-4046-a5be-573e7f369b45"}}},{"cell_type":"markdown","source":["## Run Setup\n\nThe setup script will create the data and declare necessary values for the rest of this notebook to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"855f4300-6ad3-4d4b-9a71-d07ce2b9f225"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-04.3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b61f4aa-b67d-49d1-836e-4025342671c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create Table as Select (CTAS)\n\n**`CREATE TABLE AS SELECT`** statements create and populate Delta tables using data retrieved from an input query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbfbc2d-8a9a-4dc0-80e4-8e8bab0c3ed2"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE sales AS\nSELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`;\n\nDESCRIBE EXTENDED sales;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8581c58c-158b-45fc-b33d-6bc2a770bc8f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["CTAS statements automatically infer schema information from query results and do **not** support manual schema declaration. \n\nThis means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.\n\nCTAS statements also do not support specifying additional file options.\n\nWe can see how this would present significant limitations when trying to ingest data from CSV files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7147788-5f9a-440a-91ff-48f11db12f9e"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE sales_unparsed AS\nSELECT * FROM csv.`${da.paths.datasets}/ecommerce/raw/sales-csv`;\n\nSELECT * FROM sales_unparsed;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcad023e-3b02-46e8-a553-81f50ba9982a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To correctly ingest this data to a Delta Lake table, we'll need to use a reference to the files that allows us to specify options.\n\nIn the previous lesson, we showed doing this by registering an external table. Here, we'll slightly evolve this syntax to specify the options to a temporary view, and then use this temp view as the source for a CTAS statement to successfully register the Delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f375ca77-026e-4421-837c-f728a6b0df6b"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW sales_tmp_vw\n  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\nUSING CSV\nOPTIONS (\n  path = \"${da.paths.datasets}/ecommerce/raw/sales-csv\",\n  header = \"true\",\n  delimiter = \"|\"\n);\n\nCREATE TABLE sales_delta AS\n  SELECT * FROM sales_tmp_vw;\n  \nSELECT * FROM sales_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f19ae88-f7ba-4a8f-8850-b4419819ff1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Filtering and Renaming Columns from Existing Tables\n\nSimple transformations like changing column names or omitting columns from target tables can be easily accomplished during table creation.\n\nThe following statement creates a new table containing a subset of columns from the **`sales`** table. \n\nHere, we'll presume that we're intentionally leaving out information that potentially identifies the user or that provides itemized purchase details. We'll also rename our fields with the assumption that a downstream system has different naming conventions than our source data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"055cb9b4-c245-4494-8b2e-72371bb3f923"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE purchases AS\nSELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\nFROM sales;\n\nSELECT * FROM purchases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0000c88b-2380-49c0-99fd-8b94de40ef20"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we could have accomplished this same goal with a view, as shown below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f942cf62-2af7-4d0a-bd3a-4fc6120fbc27"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE VIEW purchases_vw AS\nSELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\nFROM sales;\n\nSELECT * FROM purchases_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cec84f7-1ed3-4374-8541-784be5f801bb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Declare Schema with Generated Columns\n\nAs noted previously, CTAS statements do not support schema declaration. We note above that the timestamp column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.\n\nGenerated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table (introduced in DBR 8.3).\n\nThe code below demonstrates creating a new table while:\n1. Specifying column names and types\n1. Adding a <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns\" target=\"_blank\">generated column</a> to calculate the date\n1. Providing a descriptive column comment for the generated column"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1f0de7e-4eea-436e-8e72-111351b75e75"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE purchase_dates (\n  id STRING, \n  transaction_timestamp STRING, \n  price STRING,\n  date DATE GENERATED ALWAYS AS (\n    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n    COMMENT \"generated based on `transactions_timestamp` column\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c782d59-3d07-4a2f-a8b6-740b084e521c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because **`date`** is a generated column, if we write to **`purchase_dates`** without providing values for the **`date`** column, Delta Lake automatically computes them.\n\n**NOTE**: The cell below configures a setting to allow for generating columns when using a Delta Lake **`MERGE`** statement. We'll see more on this syntax later in the course."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b12c8933-cd7f-46b5-a429-64292e9aa948"}}},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.schema.autoMerge.enabled=true; \n\nMERGE INTO purchase_dates a\nUSING purchases b\nON a.id = b.id\nWHEN NOT MATCHED THEN\n  INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98775576-5f1d-47bb-a83f-50ff3c969d32"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can see below that all dates were computed correctly as data was inserted, although neither our source data or insert query specified the values in this field.\n\nAs with any Delta Lake source, the query automatically reads the most recent snapshot of the table for any query; you never need to run **`REFRESH TABLE`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e830ecc9-e3fd-4a27-b18e-70a02a896268"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM purchase_dates"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22371fb1-b9ce-475a-890c-1d88b9a17507"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It's important to note that if a field that would otherwise be generated is included in an insert to a table, this insert will fail if the value provided does not exactly match the value that would be derived by the logic used to define the generated column.\n\nWe can see this error by uncommenting and running the cell below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e881d15-dbaa-4485-b146-9417ec1e30a7"}}},{"cell_type":"code","source":["%sql\n-- INSERT INTO purchase_dates VALUES\n-- (1, 600000000, 42.0, \"2020-06-18\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b77ba672-43f5-4c8d-bc2e-81babe59b5e8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Add a Table Constraint\n\nThe error message above refers to a **`CHECK constraint`**. Generated columns are a special implementation of check constraints.\n\nBecause Delta Lake enforces schema on write, Databricks can support standard SQL constraint management clauses to ensure the quality and integrity of data added to a table.\n\nDatabricks currently support two types of constraints:\n* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#not-null-constraint\" target=\"_blank\">**`NOT NULL`** constraints</a>\n* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#check-constraint\" target=\"_blank\">**`CHECK`** constraints</a>\n\nIn both cases, you must ensure that no data violating the constraint is already in the table prior to defining the constraint. Once a constraint has been added to a table, data violating the constraint will result in write failure.\n\nBelow, we'll add a **`CHECK`** constraint to the **`date`** column of our table. Note that **`CHECK`** constraints look like standard **`WHERE`** clauses you might use to filter a dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88172a29-e3e7-4cc4-ad93-bcbc7c575cf4"}}},{"cell_type":"code","source":["%sql\nALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18409066-1003-4423-909d-b788c59ee16f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Table constraints are shown in the **`TBLPROPERTIES`** field."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"811b58d8-51f2-4b81-99c2-e0060180cf44"}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED purchase_dates"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"551e14c4-9c35-49ae-8eff-327b2c8162aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Enrich Tables with Additional Options and Metadata\n\nSo far we've only scratched the surface as far as the options for enriching Delta Lake tables.\n\nBelow, we show evolving a CTAS statement to include a number of additional configurations and metadata.\n\nOur **`SELECT`** clause leverages two built-in Spark SQL commands useful for file ingestion:\n* **`current_timestamp()`** records the timestamp when the logic is executed\n* **`input_file_name()`** records the source data file for each record in the table\n\nWe also include logic to create a new date column derived from timestamp data in the source.\n\nThe **`CREATE TABLE`** clause contains several options:\n* A **`COMMENT`** is added to allow for easier discovery of table contents\n* A **`LOCATION`** is specified, which will result in an external (rather than managed) table\n* The table is **`PARTITIONED BY`** a date column; this means that the data from each data will exist within its own directory in the target storage location\n\n**NOTE**: Partitioning is shown here primarily to demonstrate syntax and impact. Most Delta Lake tables (especially small-to-medium sized data) will not benefit from partitioning. Because partitioning physically separates data files, this approach can result in a small files problem and prevent file compaction and efficient data skipping. The benefits observed in Hive or HDFS do not translate to Delta Lake, and you should consult with an experienced Delta Lake architect before partitioning tables.\n\n**As a best practice, you should default to non-partitioned tables for most use cases when working with Delta Lake.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48b1c533-8bca-4333-b43e-0952703f98b3"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE users_pii\nCOMMENT \"Contains PII\"\nLOCATION \"${da.paths.working_dir}/tmp/users_pii\"\nPARTITIONED BY (first_touch_date)\nAS\n  SELECT *, \n    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n    current_timestamp() updated,\n    input_file_name() source_file\n  FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-historical/`;\n  \nSELECT * FROM users_pii;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a747885-dd18-45e8-89f5-4e9459f22d45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The metadata fields added to the table provide useful information to understand when records were inserted and from where. This can be especially helpful if troubleshooting problems in the source data becomes necessary.\n\nAll of the comments and properties for a given table can be reviewed using **`DESCRIBE TABLE EXTENDED`**.\n\n**NOTE**: Delta Lake automatically adds several table properties on table creation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f5e0495-c97a-4d6a-a284-d8c2d558b354"}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED users_pii"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5d9a889-24ba-46a9-a6c8-827a6fc1b544"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Listing the location used for the table reveals that the unique values in the partition column **`first_touch_date`** are used to create data directories."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29285a64-d31d-421e-9102-0272d06595ba"}}},{"cell_type":"code","source":["%python \nfiles = dbutils.fs.ls(f\"{DA.paths.working_dir}/tmp/users_pii\")\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2012f9cb-eef2-4a38-a7f0-d51ff6cd761d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Cloning Delta Lake Tables\nDelta Lake has two options for efficiently copying Delta Lake tables.\n\n**`DEEP CLONE`** fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccc65bc3-646f-438b-bf97-d556b68faf22"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE purchases_clone\nDEEP CLONE purchases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20678b3d-b14e-440c-9772-a610ba9e635f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because all the data files must be copied over, this can take quite a while for large datasets.\n\nIf you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, **`SHALLOW CLONE`** can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26f895c0-5c73-4c3c-8341-8c288630dc8c"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE purchases_shallow_clone\nSHALLOW CLONE purchases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"476412e3-7ff5-42b0-8064-2fe05a094c40"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In either case, data modifications applied to the cloned version of the table will be tracked and stored separately from the source. Cloning is a great way to set up tables for testing SQL code while still in development."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cda26054-d034-444c-9ce1-6b665695df08"}}},{"cell_type":"markdown","source":["## Summary\n\nIn this notebook, we focused primarily on DDL and syntax for creating Delta Lake tables. In the next notebook, we'll explore options for writing updates to tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba06ae94-e3f0-466b-ac70-afc515e6d548"}}},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15352ec0-212a-40e1-8b37-e543a0e019cf"}}},{"cell_type":"code","source":["%python \nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1732e3ef-151d-4357-a752-c191a0ff8999"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"307be217-ffa0-4566-90dd-8b0d60da08de"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 4.3 - Creating Delta Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731306731}},"nbformat":4,"nbformat_minor":0}
