{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03ddcded-13ab-40e4-a18e-2ad27ae6f535"}}},{"cell_type":"markdown","source":["# Storing PII Securely\n\nAdding a pseudonymized key to incremental workloads is as simple as adding a transformation.\n\nIn this notebook, we'll examine design patterns for ensuring PII is stored securely and updated accurately. We'll also demonstrate an approach for processing delete requests to make sure these are captured appropriately.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_users.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this notebook, students will be able to:\n- Apply incremental transformations to store data with pseudonymized keys\n- Use windowed ranking to identify the most-recent records in a CDC feed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c36d4fc5-2df4-493c-b4ba-9141d8ed5874"}}},{"cell_type":"markdown","source":["Begin by running the following cell to set up relevant databases and paths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a6d5e33-213b-4b2e-8a91-b09f6887b4f2"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-6.2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de390c33-37ba-4d56-a74a-7525baae00f6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Execute the following cell to create the **`users`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da658e3b-269a-407e-a07a-8ee3bf8bb88b"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE users\n(alt_id STRING, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, city STRING, state STRING, zip INT, updated TIMESTAMP)\nUSING DELTA\nLOCATION '${da.paths.working_dir}/users'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b1f6a8b-031b-4140-95da-c2f8c0eafe3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ELT with Pseudonymization\nThe data in the **`user_info`** topic contains complete row outputs from a Change Data Capture feed.\n\nThere are three values for **`update_type`** present in the data: **`new`**, **`update`**, and **`delete`**.\n\nThe **`users`** table will be implemented as a Type 1 table, so only the most recent value matters\n\nRun the cell below to visually confirm that both **`new`** and **`update`** records contain all the fields we need for our **`users`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4622a9c4-d5cd-4c64-bedd-d6e512671fe1"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nschema = \"\"\"\n    user_id LONG, \n    update_type STRING, \n    timestamp FLOAT, \n    dob STRING, \n    sex STRING, \n    gender STRING, \n    first_name STRING, \n    last_name STRING, \n    address STRUCT<\n        street_address: STRING, \n        city: STRING, \n        state: STRING, \n        zip: INT>\"\"\"\n\nusers_df = (spark.table(\"bronze\")\n                 .filter(\"topic = 'user_info'\")\n                 .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n                 .filter(F.col(\"update_type\").isin([\"new\", \"update\"])))\n\ndisplay(users_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a31f2820-c307-4b39-8ec1-1bb78f0a8867"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deduplication with Windowed Ranking\n\nWe've previously explored some ways to remove duplicate records:\n- Using Delta Lake's **`MERGE`** syntax, we can update or insert records based on keys, matching new records with previously loaded data\n- **`dropDuplicates`** will remove exact duplicates within a table or incremental microbatch\n\nNow we have multiple records for a given primary key BUT these records are not identical. **`dropDuplicates`** will not work to remove these records, and we'll get an error from our merge statement if we have the same key present multiple times.\n\nBelow, a third approach for removing duplicates is shown below using the <a href=\"http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.html?highlight=window#pyspark.sql.Window\" target=\"_blank\">PySpark Window class</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0673f789-d007-4053-99ac-aba76321cbf9"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"user_id\").orderBy(F.col(\"timestamp\").desc())\n\nranked_df = (users_df.withColumn(\"rank\", F.rank().over(window))\n                     .filter(\"rank == 1\")\n                     .drop(\"rank\"))\ndisplay(ranked_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6364eae2-fbc0-4188-a0e9-0c50a716c256"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As desired, we get only the newest (**`rank == 1`**) entry for each unique **`user_id`**.\n\nUnfortunately, if we try to apply this to a streaming read of our data, we'll learn that\n> Non-time-based windows are not supported on streaming DataFrames\n\nUncomment and run the following cell to see this error in action:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e0c0646-291b-4741-9647-9177f88a3f6c"}}},{"cell_type":"code","source":["# ranked_df = (spark.readStream\n#                   .table(\"bronze\")\n#                   .filter(\"topic = 'user_info'\")\n#                   .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n#                   .select(\"v.*\")\n#                   .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n#                   .withColumn(\"rank\", F.rank().over(window))\n#                   .filter(\"rank == 1\").drop(\"rank\"))\n\n# display(ranked_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ea3277b-26fa-4dee-b336-813ccdeb8696"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Luckily we have a workaround to avoid this restriction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41c1d033-e1c9-4a3c-b08c-e1b381a34fc3"}}},{"cell_type":"markdown","source":["## Implementing Streaming Ranked De-duplication\n\nAs we saw previously, when apply **`MERGE`** logic with a Structured Streaming job, we need to use **`foreachBatch`** logic.\n\nRecall that while we're inside a streaming microbatch, we interact with our data using batch syntax.\n\nThis means that if we can apply our ranked **`Window`** logic within our **`foreachBatch`** function, we can avoid the restriction throwing our error.\n\nThe code below sets up all the incremental logic needed to load in the data in the correct schema from the bronze table. This includes:\n- Filter for the **`user_info`** topic\n- Dropping identical records within the batch\n- Unpack all of the JSON fields from the **`value`** column into the correct schema\n- Update field names and types to match the **`users`** table schema\n- Use the salted hash function to cast the **`user_id`** to **`alt_id`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa9b61b9-2f09-4bd1-8103-2e127c995a3d"}}},{"cell_type":"code","source":["salt = \"BEANS\"\n\nunpacked_df = (spark.readStream\n                    .table(\"bronze\")\n                    .filter(\"topic = 'user_info'\")\n                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n                    .select(\"v.*\")\n                    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n                            F.col(\"timestamp\").cast(\"timestamp\").alias(\"updated\"),\n                            F.to_date(\"dob\", \"MM/dd/yyyy\").alias(\"dob\"), \"sex\", \"gender\", \"first_name\", \"last_name\", \"address.*\", \"update_type\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e9db74f-2a65-4631-892e-4bd9c0aa723c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The updated Window logic is provided below. Note that this is being applied to each **`micro_batch_df`** to result in a local **`ranked_df`** that will be used for merging.\n \nFor our **`MERGE`** statement, we need to:\n- Match entries on our **`alt_id`**\n- Update all when matched **if** the new record has is newer than the previous entry\n- When not matched, insert all\n\nAs before, use **`foreachBatch`** to apply merge operations in Structured Streaming."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f648575e-2854-4f55-9544-64817f7960eb"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    \n    (microBatchDF.filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n                 .withColumn(\"rank\", F.rank().over(window))\n                 .filter(\"rank == 1\")\n                 .drop(\"rank\")\n                 .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n            WHEN MATCHED AND u.updated < r.updated\n              THEN UPDATE SET *\n            WHEN NOT MATCHED\n              THEN INSERT *\n    \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dbc649e-4549-4214-b781-d071b6346364"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we can apply this function to our data. \n\nHere, we'll run a trigger-available-now batch to process all records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3720959c-a942-4b0f-9b9e-5ddc46f30c65"}}},{"cell_type":"code","source":["query = (unpacked_df.writeStream\n                    .foreachBatch(batch_rank_upsert)\n                    .outputMode(\"update\")\n                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/batch_rank_upsert\")\n                    .trigger(availableNow=True)\n                    .start())\n\nquery.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eca6297c-f72d-4608-b161-205a5ed1f173"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The **`users`** table should only have 1 record for each unique ID."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f5cc1c3-e7e7-41a4-9932-c3603f48f0fe"}}},{"cell_type":"code","source":["count_a = spark.table(\"users\").count()\ncount_b = spark.table(\"users\").select(\"alt_id\").distinct().count()\nassert count_a == count_b\nprint(\"All tests passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5f63b91-3ed0-4b5a-bf17-948c523abf34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7001a214-d36a-4732-be9e-e19a2367fff5"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04dcfd88-487a-4b3b-add3-e6fe5c7736ff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bf214d8-a593-49ee-83f1-deb7392f97d3"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADE 5.2 -De-duplication Storing PII Securely","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731308859}},"nbformat":4,"nbformat_minor":0}
