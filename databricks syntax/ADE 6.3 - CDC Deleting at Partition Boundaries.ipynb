{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"338e5c33-96d3-45f7-a4e8-5045864f864d"}}},{"cell_type":"markdown","source":["# Deleting at Partition Boundaries\n\nWhile we've deleted our PII from our silver tables, we haven't dealt with the fact that this data still exists in our **`bronze`** table.\n\nNote that because of stream composability and the design choice to use a multiplex bronze pattern, enabling Delta Change Data Feed (CDF) to propagate delete information would require redesigning each of our pipelines to take advantage of this output. Without using CDF, modification of data in a table will break downstream composability.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />\n\nIn this notebook, you'll learn how to delete partitions of data from Delta Tables and how to configure incremental reads to allow for these deletes.\n\nThis functionality is not only useful for permanently deleting PII, but this same pattern can be applied in companies that just want to expunge data older than a certain age from a given table. Similarly, data could be backed up to a cheaper storage tier, and then safely deleted from \"active\" or \"hot\" Delta tables to drive savings on cloud storage.\n\n## Learning Objectives\nBy the end of this notebook, students will be able to:\n- Delete data using partition boundaries\n- Configure downstream incremental reads to safely ignore these deletions\n- Use **`VACUUM`** to review files to be deleted and commit deletes\n- Union archived data with production tables to recreate a full historic dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"564cf94f-f465-4ad0-b18c-0069dfe63e92"}}},{"cell_type":"markdown","source":["Begin by running our setup script."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8e4f6e8-49d3-4fed-a2d9-15ce0d0f2935"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-7.3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69351998-f455-4b14-9764-1172bfb06fbe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our Delta table is partitioned by two fields. \n\nOur top level partition is the **`topic`** column. \n\nRun the cell to note the 3 partition directories (and the Delta Log directory) that collectively comprise our **`bronze`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8c92e0c-1e41-418a-9e6f-f1c2b3d54d07"}}},{"cell_type":"code","source":["files = dbutils.fs.ls(f\"{DA.paths.user_db}/bronze\")\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3980f58f-eb11-43b0-87a7-663da4098790"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our 2nd level partition was on our **`week_part`** column, which we derived as the year and week of year. There are around 20 directories currently present at this level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e990a9ff-ca4e-49ad-8fb1-1c08f3cfc3b8"}}},{"cell_type":"code","source":["files = dbutils.fs.ls(f\"{DA.paths.user_db}/bronze/topic=user_info\")\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"133ca7a2-877a-44af-8f3f-df4eb05a7481"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that in our current dataset, we're tracking only a small number of total users in these files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77372dc-2e8f-4d90-b516-be7cd7b6f716"}}},{"cell_type":"code","source":["total = (spark.table(\"bronze\")\n              .filter(\"topic='user_info'\")\n              .filter(\"week_part<='2019-48'\")\n              .count())\n         \nprint(f\"Total: {total}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9ceb4e5-d748-49bc-b739-0db9d99b0c49"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Archiving Data\nIf a company wishes to maintain an archive of historic records (but only maintain recent records in production tables), cloud-native settings for auto-archiving data can be configured to move data files automatically to lower-cost storage locations.\n\nThe cell below simulates this process (here using copy instead of move). \n\nNote that because only the data files and partition directories are being relocated, the resultant table will be Parquet by default.\n\n**NOTE**: For best performance, directories should have **`OPTIMIZE`** run to condense small files. Because valid and stale data files are stored side-by-side in Delta Lake files, partitions should also have **`VACUUM`** executed prior to moving any Delta Lake data to a pure Parquet table to ensure only valid files are copied."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3add041c-c208-4be6-b449-c732b116c094"}}},{"cell_type":"code","source":["archive_path = f\"{DA.paths.working_dir}/pii_archive\"\nsource_path = f\"{DA.paths.user_db}/bronze/topic=user_info\"\n\nfiles = dbutils.fs.ls(source_path)\n[dbutils.fs.cp(f[0], f\"{archive_path}/{f[1]}\", True) for f in files if f[1][-8:-1] <= '2019-48'];\n\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS user_info_archived\nUSING parquet\nLOCATION '{archive_path}'\n\"\"\")\n\nspark.sql(\"MSCK REPAIR TABLE user_info_archived\")\n\ndisplay(spark.sql(\"SELECT COUNT(*) FROM user_info_archived\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33ace9d9-ffc9-4e81-be53-fad56aeee884"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the directory structure was maintained as files were copied."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fefb4be2-b04f-4241-a6b2-e079723b76ad"}}},{"cell_type":"code","source":["files = dbutils.fs.ls(archive_path)\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8459077-5bde-4c38-ba4d-097922496595"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deleting at a Partition Boundary\nHere we'll model deleting all **`user_info`** that was received before week 49 of 2019.\n\nNote that we are deleting cleanly along partition boundaries. All the data contained in the specified **`week_part`** directories will be removed from our table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2e3122b-879e-4923-9506-2e769153a719"}}},{"cell_type":"code","source":["%sql\nDELETE FROM bronze \nWHERE topic = 'user_info'\nAND week_part <= '2019-48'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b742761b-05bc-426c-b640-1eb8b5470eea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can confirm this delete processed successfully by looking at the history. The **`operationMetrics`** column will indicate the number of removed files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3607c01e-a461-4c48-b026-09290a728e75"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fcbe52f-9678-42d1-8074-00e07cb0532c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When deleting along partition boundaries, we don't write out new data files; recording the files as removed in the Delta log is sufficient. \n\nHowever, file deletion will not actually occur until we **`VACUUM`** our table. \n\nNote that all of our week partitions still exist in our **`user_info`** directory and that data files still exist in each week directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99c0f7b2-0604-4773-81a2-9349f11d42c7"}}},{"cell_type":"code","source":["files = dbutils.fs.ls(f\"{source_path}/week_part=2019-48\")\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b64c4390-8eb3-45d6-b07f-8e56ab23f536"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Reviewing and Committing Deletes\n\nBy default, the Delta engine will prevent **`VACUUM`** operations with less than 7 days of retention. The cell below overrides this check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"960d7e51-1f12-401b-841e-873075cb8a22"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb26a7d3-080f-4e81-93a6-00d2741427b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Adding the **`DRY RUN`** keyword to the end of our **`VACUUM`** statement allows us to preview files to be deleted before they are permanently removed. \n\nNote that at this moment we could still recover our deleted data by running:\n\n<strong><code>\nRESTORE bronze<br/>\nTO VERSION AS OF {version}\n</code></strong>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c58bb1c-47f5-4c2a-8e5c-379c4fdec8a8"}}},{"cell_type":"code","source":["%sql\nVACUUM bronze RETAIN 0 HOURS DRY RUN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8448a4e6-3a78-416f-9e41-d95e341c3a93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Executing the **`VACUUM`** command below permanently deletes these files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dc6b7e7-d7e4-4650-a12f-c98cd70b3029"}}},{"cell_type":"code","source":["%sql\nVACUUM bronze RETAIN 0 HOURS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a507ee70-7f91-4532-87f9-020608f85f37"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For safety, it's best to always re-enable our **`retentionDurationCheck`**. In production, you should avoid overriding this check whenever possible (if other operations are acting against files not yet committed to a Delta table and written before the retention threshold, **`VACUUM`** can result in data corruption)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7549c69-4019-41ef-8314-f52cadf850a5"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"299aa3c3-dcb0-4b7a-b3e6-537418cd4d05"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that empty directories will eventually be cleaned up with **`VACUUM`**, but may not always be deleted as they are emptied of data files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f96b506b-c158-47f8-802d-061754aa09a4"}}},{"cell_type":"code","source":["try:\n    files = dbutils.fs.ls(source_path)\n    print(\"The files NOT YET deleted.\")\nexcept Exception as e:\n    print(\"The files WERE deleted.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e584e5dd-4760-43ba-af41-34a123d6caba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As such, querying the **`bronze`** table with the same filters used in our delete statement should yield 0 records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"899bf372-e002-4385-9726-25360d68953f"}}},{"cell_type":"code","source":["%sql\nSELECT * \nFROM bronze \nWHERE topic='user_info' AND \n      week_part <= '2019-48'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c50d4bfd-c9cc-4d28-a6dc-542f82036bcd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Recreating Full Table History\n\nNote that because Parquet using directory partitions as columns in the resulting dataset, the data that was backed up no longer has a **`topic`** field in its schema.\n\nThe logic below addresses this while calling **`UNION`** on the archived and production datasets to recreate the full history of the **`user_info`** topic."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1756381e-0800-49a3-937b-b477ed2d4045"}}},{"cell_type":"code","source":["%sql\nWITH full_bronze_user_info AS (\n\n  SELECT key, value, partition, offset, timestamp, date, week_part \n  FROM bronze \n  WHERE topic='user_info'\n  \n  UNION SELECT * FROM user_info_archived) \n  \nSELECT COUNT(*) FROM full_bronze_user_info"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6680694f-2d58-4ff7-b9d1-92c2b124114e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Updating Streaming Reads to Ignore Changes\n\nThe cell below condenses all the code used to perform streaming updates to our **`users`** table.\n\nIf you try to execute this code right now, you'll raise an exception\n> Detected deleted data from streaming source\n\nLine 22 of the cell below adds the **`.option(\"ignoreDeletes\", True)`** to the DataStreamReader. This option is all that is necessary to enable streaming processing from Delta tables with partition deletes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7177e8d-7893-49d1-a00d-5a234f106107"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nschema = \"\"\"\n    user_id LONG, \n    update_type STRING, \n    timestamp FLOAT, \n    dob STRING, \n    sex STRING, \n    gender STRING, \n    first_name STRING, \n    last_name STRING, \n    address STRUCT<\n        street_address: STRING, \n        city: STRING, \n        state: STRING, \n        zip: INT\n    >\"\"\"\n\nsalt = \"BEANS\"\n\nunpacked_df = (spark.readStream\n                    .option(\"ignoreDeletes\", True)     # This is new!\n                    .table(\"bronze\")\n                    .filter(\"topic = 'user_info'\")\n                    .dropDuplicates()\n                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n                    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n                            F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n                            F.to_date('dob','MM/dd/yyyy').alias('dob'),\n                            'sex', 'gender','first_name','last_name','address.*', \"update_type\"))\n\n\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    from pyspark.sql.window import Window\n    \n    window = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n    \n    (microBatchDF\n        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n        .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n        WHEN MATCHED AND u.updated < r.updated\n          THEN UPDATE SET *\n        WHEN NOT MATCHED\n          THEN INSERT *\n    \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab061b5-6945-4bfd-a2fa-75644325b7f0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["query = (unpacked_df.writeStream\n                    .foreachBatch(batch_rank_upsert)\n                    .outputMode(\"update\")\n                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/batch_rank_upsert\")\n                    .trigger(once=True)\n                    .start())    \n\nquery.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f341f1e1-dbc5-488e-b28a-96b0f0e657ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we may see the table version increment as this code completes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"996e82aa-2ab8-44ee-89fa-a544f56369aa"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7a648da-311a-4077-b78f-cfa9837e2e35"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["However, by examining the Delta log file this version, we'll note that the file written out is just indicating the data change, but that no new records were added or modified."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b07c1c0c-28ee-4955-b8ba-c3f1fc105826"}}},{"cell_type":"code","source":["users_log_path = f\"{DA.paths.user_db}/users/_delta_log\"\nfiles = dbutils.fs.ls(users_log_path)\n\nmax_version = max([file.name for file in files if file.name.endswith(\".json\")])\ndisplay(spark.read.json(f\"{users_log_path}/{max_version}\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de585159-efe3-43db-a382-fefec1392ebb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next Steps\nWhile we did not modify data in our **`workout`** or **`bpm`** partitions, because these read from the same **`bronze`** table we'll need to also update their DataStreamReader logic to ignore changes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ae1278b-67c4-4a7d-9269-eb7d6d92559b"}}},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92a1d71b-94bb-44bb-a008-987e698afad9"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7daa256-1fdc-47f1-8377-d5dcdbbf7d38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3338d6bd-1ea2-4a3b-878e-b371a5d29d99"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADE 6.3 - CDC Deleting at Partition Boundaries","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731308666}},"nbformat":4,"nbformat_minor":0}
