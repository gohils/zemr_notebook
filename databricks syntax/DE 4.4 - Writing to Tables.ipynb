{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97e182c6-4023-4132-98bd-ca25fa395590"}}},{"cell_type":"markdown","source":["# Writing to Delta Tables\nDelta Lake tables provide ACID compliant updates to tables backed by data files in cloud object storage.\n\nIn this notebook, we'll explore SQL syntax to process updates with Delta Lake. While many operations are standard SQL, slight variations exist to accommodate Spark and Delta Lake execution.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Overwrite data tables using **`INSERT OVERWRITE`**\n- Append to a table using **`INSERT INTO`**\n- Append, update, and delete from a table using **`MERGE INTO`**\n- Ingest data incrementally into tables using **`COPY INTO`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb7048d0-0f8d-4445-bde8-f703d3f955df"}}},{"cell_type":"markdown","source":["## Run Setup\n\nThe setup script will create the data and declare necessary values for the rest of this notebook to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00305107-78aa-4e39-a5d9-76f67f0283d0"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-04.4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8177a2e7-f499-4a5e-878b-596a576e0fe7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Complete Overwrites\n\nWe can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n- Overwriting a table is much faster because it doesn’t need to list the directory recursively or delete any files.\n- The old version of the table still exists; can easily retrieve the old data using Time Travel.\n- It’s an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n- Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n\nSpark SQL provides two easy methods to accomplish complete overwrites.\n\nSome students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n\n**`CREATE OR REPLACE TABLE`** (CRAS) statements fully replace the contents of a table each time they execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"594132aa-db1e-4b88-8252-63b7398263c4"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE events AS\nSELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30e0bca3-7569-4829-a724-e85d3d3efb75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Reviewing the table history shows a previous version of this table was replaced."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29fea004-0da9-4229-9903-a4c560095e5d"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY events"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5badaa7-154c-4eb9-9f4d-58593126f6e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**`INSERT OVERWRITE`** provides a nearly identical outcome as above: data in the target table will be replaced by data from the query. \n\n**`INSERT OVERWRITE`**:\n\n- Can only overwrite an existing table, not create a new one like our CRAS statement\n- Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n- Can overwrite individual partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4eb9f20-2074-4eab-b619-514072490d0d"}}},{"cell_type":"code","source":["%sql\nINSERT OVERWRITE sales\nSELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical/`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da8675f1-6fc1-44b1-b95d-9823befb13bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that different metrics are displayed than a CRAS statement; the table history also records the operation differently."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66a607df-0440-4a4e-8c88-74d919bade09"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d21e7d00-088f-4708-9523-1f52a0e16882"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["A primary difference here has to do with how Delta Lake enforces schema on write.\n\nWhereas a CRAS statement will allow us to completely redefine the contents of our target table, **`INSERT OVERWRITE`** will fail if we try to change our schema (unless we provide optional settings). \n\nUncomment and run the cell below to generate an expected error message."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6b2f4da-bb32-47c0-8db9-473a351fbd50"}}},{"cell_type":"code","source":["%sql\n-- INSERT OVERWRITE sales\n-- SELECT *, current_timestamp() FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40679e4e-68c8-441b-9e9e-0bbdd6f35c34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Append Rows\n\nWe can use **`INSERT INTO`** to atomically append new rows to an existing Delta table. This allows for incremental updates to existing tables, which is much more efficient than overwriting each time.\n\nAppend new sale records to the **`sales`** table using **`INSERT INTO`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ca4415-d7af-4f7e-b7ad-f0bc4d9695f8"}}},{"cell_type":"code","source":["%sql\nINSERT INTO sales\nSELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-30m`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c95d09c2-2915-465e-a3aa-5ea21402acad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that **`INSERT INTO`** does not have any built-in guarantees to prevent inserting the same records multiple times. Re-executing the above cell would write the same records to the target table, resulting in duplicate records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7adbabc-de87-4efd-8f8b-1798a4617dc3"}}},{"cell_type":"markdown","source":["## Merge Updates\n\nYou can upsert data from a source table, view, or DataFrame into a target Delta table using the **`MERGE`** SQL operation. Delta Lake supports inserts, updates and deletes in **`MERGE`**, and supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n\n<strong><code>\nMERGE INTO target a<br/>\nUSING source b<br/>\nON {merge_condition}<br/>\nWHEN MATCHED THEN {matched_action}<br/>\nWHEN NOT MATCHED THEN {not_matched_action}<br/>\n</code></strong>\n\nWe will use the **`MERGE`** operation to update historic users data with updated emails and new users."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a374d01-31d4-46bb-a3d5-e985056cdf8d"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW users_update AS \nSELECT *, current_timestamp() AS updated \nFROM parquet.`${da.paths.datasets}/ecommerce/raw/users-30m`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"badc53e8-70c2-4b3b-b7ae-eaece3d4305f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The main benefits of **`MERGE`**:\n* updates, inserts, and deletes are completed as a single transaction\n* multiple conditionals can be added in addition to matching fields\n* provides extensive options for implementing custom logic\n\nBelow, we'll only update records if the current row has a **`NULL`** email and the new row does not. \n\nAll unmatched records from the new batch will be inserted."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e70a8c75-ee99-4db1-aca1-b5a9cddee198"}}},{"cell_type":"code","source":["%sql\nMERGE INTO users a\nUSING users_update b\nON a.user_id = b.user_id\nWHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n  UPDATE SET email = b.email, updated = b.updated\nWHEN NOT MATCHED THEN INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ad4ada1-a650-48bc-992e-a69392cb8676"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that we explicitly specify the behavior of this function for both the **`MATCHED`** and **`NOT MATCHED`** conditions; the example demonstrated here is just an example of logic that can be applied, rather than indicative of all **`MERGE`** behavior."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc2b4dd2-e4f1-4458-b9a9-3569d2039a36"}}},{"cell_type":"markdown","source":["## Insert-Only Merge for Deduplication\n\nA common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations. \n\nMany source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.\n\nThis optimized command uses the same **`MERGE`** syntax but only provided a **`WHEN NOT MATCHED`** clause.\n\nBelow, we use this to confirm that records with the same **`user_id`** and **`event_timestamp`** aren't already in the **`events`** table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dc48575-e800-40c3-b227-902903b8e1b5"}}},{"cell_type":"code","source":["%sql\nMERGE INTO events a\nUSING events_update b\nON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\nWHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n  INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20b09de9-a172-483d-9577-4c50eb158d8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Load Incrementally\n\n**`COPY INTO`** provides SQL engineers an idempotent option to incrementally ingest data from external systems.\n\nNote that this operation does have some expectations:\n- Data schema should be consistent\n- Duplicate records should try to be excluded or handled downstream\n\nThis operation is potentially much cheaper than full table scans for data that grows predictably.\n\nWhile here we'll show simple execution on a static directory, the real value is in multiple executions over time picking up new files in the source automatically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d16119d7-6a8d-4da2-9ad5-a3dfa09fbf4e"}}},{"cell_type":"code","source":["%sql\nCOPY INTO sales\nFROM \"${da.paths.datasets}/ecommerce/raw/sales-30m\"\nFILEFORMAT = PARQUET"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f48a84bd-c150-48e4-ba31-feebb3400f0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f711fef3-773e-4f45-99af-47ebb7b181ca"}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cda15a05-6d75-4644-bdf0-1950fe28e073"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbd8d36e-0be6-4148-9bac-247708f7e205"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 4.4 - Writing to Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2264272731306964}},"nbformat":4,"nbformat_minor":0}
