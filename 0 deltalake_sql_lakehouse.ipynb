{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edd7048-7d6c-4cc8-847a-adcb97cdf54e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Ensure proper schema registration for all layers\n",
    "spark.sql(\"DROP SCHEMA  IF EXISTS bronze CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS silver CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS gold CASCADE\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DimensionalModeling\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), -1)  # The date of the load\n",
    "\n",
    "# Example data for dimension tables\n",
    "raw_customer_data = [\n",
    "    (1, \"John Doe\", \"New York\", \"NY\", \"USA\", \"Group A\"),\n",
    "    (2, \"Jane Smith\", \"Los Angeles\", \"CA\", \"USA\", \"Group B\"),\n",
    "    (3, \"Michael Johnson\", \"San Francisco\", \"CA\", \"USA\", \"Group A\"),\n",
    "    (4, \"Emily Brown\", \"Chicago\", \"IL\", \"USA\", \"Group C\")\n",
    "]\n",
    "\n",
    "raw_customer_group_data = [\n",
    "    (\"Group A\", \"High Value Customers\"),\n",
    "    (\"Group B\", \"Medium Value Customers\"),\n",
    "    (\"Group C\", \"Low Value Customers\")\n",
    "]\n",
    "\n",
    "raw_product_data = [\n",
    "    (1, \"Product A\", \"Category X\", \"Group 1\"),\n",
    "    (2, \"Product B\", \"Category Y\", \"Group 2\"),\n",
    "    (3, \"Product C\", \"Category Z\", \"Group 3\")\n",
    "]\n",
    "\n",
    "raw_product_group_data = [\n",
    "    (\"Group 1\", \"Electronics\"),\n",
    "    (\"Group 2\", \"Clothing\"),\n",
    "    (\"Group 3\", \"Home & Kitchen\")\n",
    "]\n",
    "\n",
    "raw_date_data = [\n",
    "    (\"2024-03-01\", \"2024-03-01\", 2024, 3),\n",
    "    (\"2024-03-02\", \"2024-03-02\", 2024, 3),\n",
    "    (\"2024-03-03\", \"2024-03-03\", 2024, 3),\n",
    "    (\"2024-03-04\", \"2024-03-04\", 2024, 3)\n",
    "]\n",
    "\n",
    "# Example data for fact table\n",
    "raw_sales_data = [\n",
    "    (101, 1, 1, \"2024-03-01\", 2, 100),\n",
    "    (102, 2, 2, \"2024-03-02\", 1, 50),\n",
    "    (103, 3, 3, \"2024-03-02\", 3, 200),\n",
    "    (104, 1, 1, \"2024-03-03\", 1, 50),\n",
    "    (105, 2, 2, \"2024-03-04\", 2, 150)\n",
    "]\n",
    "\n",
    "# Ensure proper schema registration for all layers\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "# Create DataFrames for separate tables in the bronze layer with ingestion_date\n",
    "raw_customer_df = spark.createDataFrame(raw_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_customer_group_df = spark.createDataFrame(raw_customer_group_data, [\"customer_group\", \"group_description\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_product_df = spark.createDataFrame(raw_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_product_group_df = spark.createDataFrame(raw_product_group_data, [\"product_group\", \"group_description\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_date_df = spark.createDataFrame(raw_date_data, [\"order_date\", \"full_date\", \"year\", \"month\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Write each table separately to Delta Lake without joining\n",
    "raw_customer_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer USING DELTA LOCATION '/tmp/raw_customer'\")\n",
    "\n",
    "raw_customer_group_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_customer_group\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_customer_group USING DELTA LOCATION '/tmp/raw_customer_group'\")\n",
    "\n",
    "raw_product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_product USING DELTA LOCATION '/tmp/raw_product'\")\n",
    "\n",
    "raw_product_group_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/raw_product_group\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_product_group USING DELTA LOCATION '/tmp/raw_product_group'\")\n",
    "\n",
    "raw_date_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\", \"month\").option(\"zorder\", \"order_date\").save(\"/tmp/raw_date\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_date USING DELTA LOCATION '/tmp/raw_date'\")\n",
    "\n",
    "raw_sales_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_date\").option(\"zorder\", \"customer_id\").save(\"/tmp/raw_sales\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.raw_sales USING DELTA LOCATION '/tmp/raw_sales'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5828a1-200a-4fca-bad2-130eecd84c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TestSparkSQL\").getOrCreate()\n",
    "\n",
    "# Calculate ingestion_date_from and ingestion_date_to using Python's datetime module\n",
    "ingestion_date_from = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')  # 7 days ago\n",
    "ingestion_date_to = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')    # 1 day ago\n",
    "\n",
    "# Print the ingestion_date_from and ingestion_date_to\n",
    "print(f\"ingestion_date_from = {ingestion_date_from}\")\n",
    "print(f\"ingestion_date_to = {ingestion_date_to}\")\n",
    "\n",
    "# Define the SQL query with named parameter markers\n",
    "query = \"\"\"\n",
    "-- Customer Dimension\n",
    "SELECT\n",
    "    cust.customer_id,\n",
    "    cust.customer_name,\n",
    "    cust.city,\n",
    "    cust.state,\n",
    "    cust.country,\n",
    "    cust.customer_group,\n",
    "    grp.group_description AS customer_group_description,\n",
    "    cust.ingestion_date\n",
    "FROM bronze.raw_customer AS cust\n",
    "LEFT JOIN bronze.raw_customer_group AS grp\n",
    "    ON cust.customer_group = grp.customer_group\n",
    "WHERE cust.ingestion_date BETWEEN :ingestion_date_from AND :ingestion_date_to;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query with named parameters\n",
    "# Pass parameters using a dictionary\n",
    "params = {\n",
    "    \"ingestion_date_from\": ingestion_date_from,\n",
    "    \"ingestion_date_to\": ingestion_date_to\n",
    "}\n",
    "result_df = spark.sql(query, params)\n",
    "\n",
    "# Show the results\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65f6bdf-594d-4615-adcd-5f0f73589caa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataPipelineSQL:\n",
    "    def __init__(self, config_dict):\n",
    "        self.config_dict = config_dict\n",
    "        self.spark = SparkSession.builder.getOrCreate()  # Use the existing Spark session\n",
    "\n",
    "    def execute_sql_file(self, sql_file_path):\n",
    "        print(f\"Executing SQL file: {sql_file_path}\")\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            sql_queries = file.read().split(';')  # Split the file content by semicolon\n",
    "\n",
    "        for sql_query in sql_queries:\n",
    "            sql_query = sql_query.strip()\n",
    "            if sql_query:  # Execute only non-empty queries\n",
    "                self.spark.sql(sql_query)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        for task in self.config_dict[\"transformation_rules\"]:\n",
    "            self.execute_sql_file(task[\"sql_file_path\"])\n",
    "\n",
    "        print(\"End of ETL pipeline workflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784f0fd5-75b6-4e26-88df-75addcf04bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "class DataPipelineSQL:\n",
    "    def __init__(self, config_dict):\n",
    "        self.config_dict = config_dict\n",
    "        self.spark = SparkSession.builder.getOrCreate()  # Use the existing Spark session\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def execute_sql_file(self, sql_file_path, params=None):\n",
    "        self.logger.info(f\"Executing SQL file: {sql_file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(sql_file_path, 'r') as file:\n",
    "                sql_queries = file.read().split(';')  # Split the file content by semicolon\n",
    "\n",
    "            if params is None:\n",
    "                params = {}\n",
    "\n",
    "            for sql_query in sql_queries:\n",
    "                sql_query = sql_query.strip()\n",
    "                if sql_query:  # Execute only non-empty queries\n",
    "                    # Substitute named parameters\n",
    "                    for param_name, param_value in params.items():\n",
    "                        sql_query = sql_query.replace(f\":{param_name}\", str(param_value))\n",
    "                    self.logger.debug(f\"Executing query: {sql_query}\")\n",
    "                    self.spark.sql(sql_query)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error executing SQL file {sql_file_path}: {e}\", exc_info=True)\n",
    "\n",
    "    def run_pipeline(self, params):\n",
    "        self.logger.info(\"Starting ETL pipeline workflow\")\n",
    "\n",
    "        try:\n",
    "            for task in self.config_dict.get(\"transformation_rules\", []):\n",
    "                sql_file_path = task.get(\"sql_file_path\")\n",
    "                if sql_file_path:\n",
    "                    self.execute_sql_file(sql_file_path, params)\n",
    "\n",
    "            self.logger.info(\"ETL pipeline workflow completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in ETL pipeline: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb682a81-1816-47a7-b0d2-ab1b8b2276b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /dbfs/tmp/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beaf2637-4f59-40ab-8177-6bad81df9490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cat <<EOF > /dbfs/tmp/config/config_sql1.json\n",
    "{\n",
    "    \"transformation_rules\": [\n",
    "        {\n",
    "            \"sql_file_path\": \"/dbfs/tmp/config/silver_layer_transformations.sql\"\n",
    "        },\n",
    "        {\n",
    "            \"sql_file_path\": \"/dbfs/tmp/config/gold_layer_transformations.sql\"\n",
    "        }    ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d22320-5388-4e26-b057-c48b344ba1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cat <<EOF > /dbfs/tmp/config/silver_layer_transformations.sql\n",
    "-- SQL Script to update Silver Layer with daily delta records\n",
    "\n",
    "-- Step 1: Create Silver Layer tables if they don't exist\n",
    "-- Customer Dimension\n",
    "CREATE TABLE IF NOT EXISTS silver.tab_dim_customer (\n",
    "    customer_id STRING,\n",
    "    customer_name STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    country STRING,\n",
    "    customer_group STRING,\n",
    "    customer_group_description STRING,\n",
    "    ingestion_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/path/to/delta/silver/tab_dim_customer';\n",
    "\n",
    "-- Product Dimension\n",
    "CREATE TABLE IF NOT EXISTS silver.tab_dim_product (\n",
    "    product_id STRING,\n",
    "    product_name STRING,\n",
    "    product_category STRING,\n",
    "    product_group_description STRING,\n",
    "    ingestion_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/path/to/delta/silver/tab_dim_product';\n",
    "\n",
    "-- Date Dimension\n",
    "CREATE TABLE IF NOT EXISTS silver.tab_dim_date (\n",
    "    order_date DATE,\n",
    "    full_date STRING,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    ingestion_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/path/to/delta/silver/tab_dim_date';\n",
    "\n",
    "-- Sales Fact Table\n",
    "CREATE TABLE IF NOT EXISTS silver.tab_fact_sales (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    order_date DATE,\n",
    "    quantity INT,\n",
    "    amount DOUBLE,\n",
    "    ingestion_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/path/to/delta/silver/tab_fact_sales';\n",
    "\n",
    "-- Step 2: Insert data into Silver Layer tables\n",
    "\n",
    "-- Customer Dimension\n",
    "INSERT INTO silver.tab_dim_customer\n",
    "SELECT\n",
    "    cust.customer_id,\n",
    "    cust.customer_name,\n",
    "    cust.city,\n",
    "    cust.state,\n",
    "    cust.country,\n",
    "    cust.customer_group,\n",
    "    grp.group_description AS customer_group_description,\n",
    "    cust.ingestion_date\n",
    "FROM bronze.raw_customer AS cust\n",
    "LEFT JOIN bronze.raw_customer_group AS grp\n",
    "    ON cust.customer_group = grp.customer_group\n",
    "WHERE cust.ingestion_date = :ingestion_date;\n",
    "\n",
    "-- Product Dimension\n",
    "INSERT INTO silver.tab_dim_product\n",
    "SELECT\n",
    "    prod.product_id,\n",
    "    prod.product_name,\n",
    "    prod.product_category,\n",
    "    grp.group_description AS product_group_description,\n",
    "    prod.ingestion_date\n",
    "FROM bronze.raw_product AS prod\n",
    "LEFT JOIN bronze.raw_product_group AS grp\n",
    "    ON prod.product_group = grp.product_group\n",
    "WHERE prod.ingestion_date = :ingestion_date;\n",
    "\n",
    "-- Date Dimension\n",
    "INSERT INTO silver.tab_dim_date\n",
    "SELECT\n",
    "    order_date,\n",
    "    full_date,\n",
    "    year,\n",
    "    month,\n",
    "    ingestion_date\n",
    "FROM bronze.raw_date\n",
    "WHERE ingestion_date = :ingestion_date;\n",
    "\n",
    "-- Sales Fact Table\n",
    "INSERT INTO silver.tab_fact_sales\n",
    "SELECT\n",
    "    sales.order_id,\n",
    "    sales.customer_id,\n",
    "    sales.product_id,\n",
    "    sales.order_date,\n",
    "    sales.quantity,\n",
    "    sales.amount,\n",
    "    sales.ingestion_date\n",
    "FROM bronze.raw_sales AS sales\n",
    "WHERE sales.ingestion_date = :ingestion_date;\n",
    "\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237904ac-0ef1-45de-b819-ee8268341e44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cat <<EOF > /dbfs/tmp/config/gold_layer_transformations.sql\n",
    "-- SQL Script to update Gold Layer with daily delta records\n",
    "\n",
    "-- Step 1: Create the Gold Layer table if it doesn't exist\n",
    "CREATE TABLE IF NOT EXISTS gold.fact_sales_summary (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    order_date DATE,\n",
    "    quantity INT,\n",
    "    amount DOUBLE,\n",
    "    customer_group_description STRING,\n",
    "    product_group_description STRING,\n",
    "    product_category STRING,\n",
    "    state STRING,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    ingestion_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/path/to/delta/gold/fact_sales_summary';\n",
    "\n",
    "-- Step 2: Append to Gold Layer - Aggregate and enrich data\n",
    "INSERT INTO gold.fact_sales_summary\n",
    "SELECT\n",
    "    sales.order_id,\n",
    "    sales.customer_id,\n",
    "    sales.product_id,\n",
    "    sales.order_date,  -- Explicit reference to fact_sales order_date\n",
    "    sales.quantity,\n",
    "    sales.amount,\n",
    "    cust.customer_group_description,\n",
    "    prod.product_group_description,\n",
    "    prod.product_category,\n",
    "    cust.state,\n",
    "    dt.year,\n",
    "    dt.month,\n",
    "    sales.ingestion_date  -- Explicit reference to fact_sales ingestion_date\n",
    "FROM\n",
    "    silver.tab_fact_sales AS sales\n",
    "LEFT JOIN\n",
    "    silver.tab_dim_customer AS cust ON sales.customer_id = cust.customer_id\n",
    "LEFT JOIN\n",
    "    silver.tab_dim_product AS prod ON sales.product_id = prod.product_id\n",
    "LEFT JOIN\n",
    "    silver.tab_dim_date AS dt ON sales.order_date = dt.order_date\n",
    "WHERE\n",
    "    sales.ingestion_date = :ingestion_date;\n",
    "\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d63cb6-a7ed-4218-ad5a-dd6bc894acda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP SCHEMA  IF  EXISTS silver CASCADE\")\n",
    "spark.sql(\"DROP SCHEMA  IF  EXISTS gold CASCADE\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea2bdca-7e71-47c0-bb71-7e81900daeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class DataPipelineSQL:\n",
    "    def __init__(self, config_dict, params):\n",
    "        self.config_dict = config_dict\n",
    "        self.params = params\n",
    "        self.spark = SparkSession.builder.getOrCreate()  # Use the existing Spark session\n",
    "\n",
    "    def replace_placeholders(self, sql_script):\n",
    "        for key, value in self.params.items():\n",
    "            sql_script = sql_script.replace(f\":{key}\", f\"'{value}'\")  # Ensure values are properly quoted\n",
    "        return sql_script\n",
    "\n",
    "    def execute_sql_file(self, sql_file_path):\n",
    "        print(f\"Executing SQL file: {sql_file_path}\")\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            sql_script = file.read()\n",
    "        sql_script = self.replace_placeholders(sql_script)\n",
    "        # Execute the SQL script directly\n",
    "        for statement in sql_script.split(';'):\n",
    "            statement = statement.strip()\n",
    "            if statement:\n",
    "                try:\n",
    "                    self.spark.sql(statement)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing SQL statement: {e}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        for task in self.config_dict[\"transformation_rules\"]:\n",
    "            self.execute_sql_file(task[\"sql_file_path\"])\n",
    "        print(\"End of ETL pipeline workflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5422a222-0a02-4fc0-883f-420218b68e56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Calculate ingestion_date using Python's datetime module\n",
    "ingestion_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(\"ingestion_date ----\", ingestion_date)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters as a dictionary\n",
    "    params = {\"ingestion_date\": ingestion_date}\n",
    "\n",
    "    # Load the SQL script configuration\n",
    "    config_path = \"/dbfs/tmp/config/config_sql1.json\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config_dict = json.load(file)\n",
    "\n",
    "    # Initialize the DataPipelineSQL class with the configuration and parameters\n",
    "    pipeline = DataPipelineSQL(config_dict, params)\n",
    "\n",
    "    # Run the pipeline with parameters\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b00c448-79c7-4d58-bc9c-ba1e2bf4c1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_path = \"/dbfs/tmp/config/config_sql1.json\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config_dict = json.load(file)\n",
    "\n",
    "    pipeline = DataPipelineSQL(config_dict)\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1092623-ff9d-4f16-bbf1-db1fa995fab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT :x * :y * :z AS volume\", args = { \"x\" : 3, \"y\" : 4, \"z\"  : 5 }).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739e3ace-f8ae-4e3b-a602-f9d63670e34a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5a1faf-2cb8-41f6-82fa-59096f7b12f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate ingestion_date using Python's datetime module\n",
    "ingestion_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(\"ingestion_date ----\",ingestion_date)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set ingestion_date dynamically\n",
    "    # Set the parameter in Spark SQL context\n",
    "    spark.sql(f\"SET ingestion_date = {ingestion_date}\")\n",
    "    print(f\"SET ingestion_date = {ingestion_date}\")\n",
    "    config_path = \"/dbfs/tmp/config/config_sql1.json\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config_dict = json.load(file)\n",
    "\n",
    "    pipeline = DataPipelineSQL(config_dict)\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73674852-e5a1-43ce-9ae6-7e203f8692c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Calculate ingestion_date using Python's datetime module\n",
    "ingestion_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(\"ingestion_date ----\", ingestion_date)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set ingestion_date dynamically\n",
    "    # Set the parameter in Spark SQL context, ensuring it's quoted correctly\n",
    "    spark.sql(f\"SET ingestion_date = '{ingestion_date}'\")\n",
    "    print(f\"SET ingestion_date = '{ingestion_date}'\")\n",
    "\n",
    "    # Assuming config_path and pipeline execution as before\n",
    "    config_path = \"/dbfs/tmp/config/config_sql1.json\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config_dict = json.load(file)\n",
    "\n",
    "    pipeline = DataPipelineSQL(config_dict)\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4300a48a-e090-4ef7-b772-6aaad03fe7e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET ingestion_date = '2024-08-10';\n",
    "SELECT\n",
    "    cust.customer_id,\n",
    "    cust.customer_name,\n",
    "    cust.city,\n",
    "    cust.state,\n",
    "    cust.country,\n",
    "    cust.customer_group,\n",
    "    grp.group_description AS customer_group_description,\n",
    "    cust.ingestion_date\n",
    "FROM bronze.raw_customer AS cust\n",
    "LEFT JOIN bronze.raw_customer_group AS grp\n",
    "    ON cust.customer_group = grp.customer_group\n",
    "WHERE cust.ingestion_date = '${ingestion_date}';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c70986-5389-49c4-9c98-516c08e25e09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TestSparkSQL\").getOrCreate()\n",
    "\n",
    "# Calculate ingestion_date_from and ingestion_date_to using Python's datetime module\n",
    "ingestion_date_from = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')  # 7 days ago\n",
    "ingestion_date_to = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')    # 1 day ago\n",
    "\n",
    "# Print the ingestion_date_from and ingestion_date_to\n",
    "print(f\"ingestion_date_from = {ingestion_date_from}\")\n",
    "print(f\"ingestion_date_to = {ingestion_date_to}\")\n",
    "\n",
    "# Define the SQL query with named parameter markers\n",
    "query = \"\"\"\n",
    "-- Customer Dimension\n",
    "SELECT\n",
    "    cust.customer_id,\n",
    "    cust.customer_name,\n",
    "    cust.city,\n",
    "    cust.state,\n",
    "    cust.country,\n",
    "    cust.customer_group,\n",
    "    grp.group_description AS customer_group_description,\n",
    "    cust.ingestion_date\n",
    "FROM bronze.raw_customer AS cust\n",
    "LEFT JOIN bronze.raw_customer_group AS grp\n",
    "    ON cust.customer_group = grp.customer_group\n",
    "WHERE cust.ingestion_date BETWEEN :ingestion_date_from AND :ingestion_date_to;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query with named parameters\n",
    "# Pass parameters using a dictionary\n",
    "params = {\n",
    "    \"ingestion_date_from\": ingestion_date_from,\n",
    "    \"ingestion_date_to\": ingestion_date_to\n",
    "}\n",
    "result_df = spark.sql(query, params)\n",
    "\n",
    "# Show the results\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a342a11-c920-4cfb-bb60-410e77d96577",
     "showTitle": true,
     "title": "Next Day Script for Daily Delta Data Load Appending"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, date_add\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DailyDeltaLoad\").getOrCreate()\n",
    "\n",
    "# Define ingestion date\n",
    "ingestion_date = date_add(current_date(), 0)  # The date of the load\n",
    "\n",
    "# New data for the next day\n",
    "next_day_customer_data = [\n",
    "    (5, \"Alice Williams\", \"Houston\", \"TX\", \"USA\", \"Group B\"),\n",
    "    (6, \"Bob Davis\", \"Seattle\", \"WA\", \"USA\", \"Group A\")\n",
    "]\n",
    "\n",
    "next_day_product_data = [\n",
    "    (4, \"Product D\", \"Category W\", \"Group 2\")\n",
    "]\n",
    "\n",
    "next_day_sales_data = [\n",
    "    (106, 5, 4, \"2024-03-05\", 1, 75),\n",
    "    (107, 6, 4, \"2024-03-05\", 2, 100)\n",
    "]\n",
    "\n",
    "# Create DataFrames for next day's data\n",
    "next_day_customer_df = spark.createDataFrame(next_day_customer_data, [\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\", \"customer_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_product_df = spark.createDataFrame(next_day_product_data, [\"product_id\", \"product_name\", \"product_category\", \"product_group\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "next_day_sales_df = spark.createDataFrame(next_day_sales_data, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\", \"amount\"]) \\\n",
    "    .withColumn(\"ingestion_date\", ingestion_date)\n",
    "\n",
    "# Bronze Layer: Append new data to existing Delta Lake tables\n",
    "next_day_customer_df.write.format(\"delta\").mode(\"append\").partitionBy(\"state\").save(\"/tmp/raw_customer\")\n",
    "next_day_product_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/raw_product\")\n",
    "next_day_sales_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").save(\"/tmp/raw_sales\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4213377204887985,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "0lakehouse_sql",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
